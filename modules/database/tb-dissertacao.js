
var dissertacao = [
 {numero: 767, ano: 2021, dia: '24/03', autor: 'Jones de Oliveira Avelino', orientador: [8], linha: 3, arquivo: '',
  titulo: 'INTEGRA: Uma Abordagem Baseada em Grafo para Integrar Dados em Diferentes Níveis de Abstração',
  resumo_pt: 'Ainda indisponível',
  resumo_en: 'Yet unavailable'},

 {numero: 766, ano: 2021, dia: '09/02', autor: 'Rafael Neves da Silveira', orientador: [8], linha: 3, arquivo: '2021-RafaelSilveira.pdf',
  titulo: 'Método para Rotular Ligações Semânticas na Web de Dados',
  resumo_pt: 'A Web Semântica, com suas linguagens e padrões, fornece uma estrutura comum que permite que os dados sejam compartilhados e reutilizados. Uma forma de aumentar o conhecimento sobre esses dados é realizando novas interligações entre datasets. No entanto, a maioria das abordagens de interligação apresentam ligações do tipo "Same As" ou "Related To". Este último tipo deixa vago o significado da relação encontrada. Este trabalho apresenta um método para rotular esse tipo de relação entre datasets, por meio da combinação do uso de ontologias e vocabulários controlados com técnicas de Relation Extraction. Além do método, apresenta também a aplicação WEB denominada PLAIN que o implementa, e estudos de caso demonstrando a funcionalidade, a viabilidade e o impacto da abordagem proposta.',
  resumo_en: 'The Semantic Web, with its languages and standards, provides a common framework that allows data to be shared and reused. One way to increase knowledge about this data is by making new interconnection between datasets. However, most of the interconnection approaches have connections like "Same As" or "Related To". The latter type leaves vague the meaning of the relationship found. This paper presents a method for labeling this type of relationship between datasets, by combining the use of ontologies and controlled vocabularies with Relation Extraction techniques. Besides the method, it also presents the WEB application called PLAIN that implements it, and case studies demonstrating the functionality, feasibility and impact of the proposed approach.'},

 {numero: 765, ano: 2021, dia: '09/02', autor: 'Marcelo Pereira de Souza', orientador: [13], linha: 3, arquivo: '',
  titulo: 'Detecção de Fake News no Idioma Português: um Método Baseado em Análise de Sentimentos e Características Linguísticas',
  resumo_pt: 'Ainda indisponível',
  resumo_en: 'Yet unavailable'},

 {numero: 764, ano: 2020, dia: '21/12', autor: 'Flavio Ferreira da Silva', orientador: [6], linha: 3, arquivo: '2020-FlavioFerreira.pdf',
  titulo: 'Metodologia para Extração Automatizada de Estatísticas Relacionadas a Eventos de Segurança de Microtextos das Redes Sociais',
  resumo_pt: 'Nos dias atuais é muito difícil encontrar algum indivíduo que não esteja cadastrado em alguma rede social. Muitas dessas pessoas as utilizam por meio de seus computadores e dispositivos móveis, tornando-as parte de nossas vidas. Outro destaque é a troca de informação (colaboração) frenética entre os participantes dessas redes. A facilidade com que essas informações são geradas, juntamente com o seu crescimento exponencial, é impressionante ao ponto de não conseguirmos absorvê-las com a mesma velocidade com que são criadas e com uma eficiência desejada. Isso tem sido um problema, pois inevitavelmente algumas informações, importantes ou não, passam despercebidas. Diante deste fato, este trabalho tem como objetivo apresentar uma metodologia de mineração de dados que permita conhecer e identificar padrões de comportamento nesses grandes volumes de dados, por meio do desenvolvimento de uma ferramenta que utiliza técnicas de processamento de linguagem natural e aprendizado de máquina, de publicações de eventos de segurança em redes sociais.',
  resumo_en: 'Nowadays it is very difficult to find any individual who is not registered in any social network. Many of these people use them through their computers and mobile devices, making them part of our lives. Another highlight is the frantic exchange of information (collaboration) between the participants of these networks. The ease with which this information is generated, along with its exponential growth, is impressive to the point that we are unable to absorb it with the same speed with which it is created and with a desired efficiency. This has been a problem, as inevitably some information, important or not, goes unnoticed. Given this fact, this work aims to present a data mining methodology that allows obtaining knowledge and identifying behavior patterns in these large volumes of data, through the development of a tool that uses natural language processing techniques and machine learning. , publications of security events on social networks.'},

 {numero: 763, ano: 2020, dia: '16/12', autor: 'Antonio Luiz Carlucio Doneda', orientador: [4], linha: 2, arquivo: '2020-Doneda.pdf',
  titulo: 'Simulação de Orientação de Aeronave: Integrando Realidade Virtual e Aprendizado de Máquina em uma Solução de Baixo Custo para Otimização do Treinamento da Marinha do Brasil',
  resumo_pt: 'O militar que fica no heliponto de um navio em movimento, encarregado de auxiliar o piloto da aeronave por meio de sinais visuais durante o pouso e a decolagem, garantindo as condições gerais de segurança, é o orientador. Esta função requer autoconfiança, conhecimento, habilidades, coordenação de equipe e capacidade de reação adequada, que somente são alcançados com treinamento intensivo. Depois de serem apresentados aos conceitos teóricos, os orientadores em formação da Marinha do Brasil vão direto para o estágio prático. Esta fase de treinamento com a aeronave logo após a sala de aula revela uma série de limitações no desempenho desses alunos. Para melhor preparar o orientador em sua formação, reduzir custos de treinamento e proporcionar um ambiente seguro para simular diversos cenários, faz parte do escopo deste trabalho o desenvolvimento de um simulador de Realidade Virtual leve, de fácil transporte pelas Organizações Militares do Brasil, e de baixo custo, com tecnologias de prateleira disponíveis comercialmente, com o objetivo de incrementar o treinamento, denominado Simulador de Orientação de Aeronave (SOA). Devido às suas especificidades, um método de reconhecimento de gestos de Aprendizado de Máquina foi integrado ao ambiente virtual e dois conjuntos de dados foram criados. Esta dissertação detalha o processo para atingir os objetivos propostos e o desenvolvimento do protótipo do simulador, além de apresentar a análise dos testes realizados com 15 instrutores experientes. Os resultados indicam que o SOA atende a todos os requisitos para fornecer uma solução de treinamento confiável e de baixo custo para a Marinha do Brasil.',
  resumo_en: 'Landing Signalman (LS) is the military on the helipad of a moving ship in charge of assisting the pilot, employing visual signs during landing and takeoff and ensuring general safety conditions of the flight deck area. This task requires self-confidence, knowledge, skills, team coordination, and the ability to react appropriately, which can be only achieved with intensive training. After being introduced to theoretical concepts, the Brazilian Navy LS trainees go directly to the practical stage. This hands-on training phase straight after the classroom reveals a series of limitations in LS trainee’s performance. In order to better prepare the LS trainee, reduce training costs and provide a safe environment to perform several training situations, a low-cost and easy to transport Virtual Reality (VR) simulator was developed to increase training, denominated Helicopter Visual Signal Simulator. Due to its specificities, a Machine Learning gesture recognition method was integrated into the virtual environment, and two data sets were created. This dissertation details the process to achieve the proposed objectives and the development of the simulator prototype, in addition to presenting the analysis of the tests carried out with 15 experienced instructors. The results indicate that this simulator meets all requirements to provide a reliable and low-cost training solution for the Brazilian Navy.'},

 {numero: 762, ano: 2020, dia: '16/12', autor: 'Samir de Oliveira Cunha Ramos', orientador: [13, 1], linha: 3, arquivo: '2020-Samir.pdf',
  titulo: 'Detecção de Bots Sociais: um Método baseado em Léxico de Sentimento Aprendido de Mensagens',
  resumo_pt: 'O uso de bots em redes sociais para fins maliciosos cresceu significativamente nos últimos anos. Entre as técnicas de última geração usadas na detecção automática de bots sociais, estão aquelas que levam em conta o sentimento existente nas mensagens propagadas na rede. Essas informações são calculadas com base em léxicos de sentimento com conteúdo anotado manualmente por um conjunto restrito de pessoas. Como todo processo que envolve interpretação humana, tal anotação está sujeita a subjetividade. Além disso, no processo de anotação manual, as palavras são analisadas isoladamente, sem levar em conta o texto em que estão inseridas, o que pode não ser suficiente para expressar de forma precisa o sentimento existente no contexto. Diante destas limitações, este trabalho levanta a hipótese de que a detecção automática de bots sociais que considera características de sentimento das palavras das mensagens pode ser melhorada caso tais características tenham sido previamente aprendidas por máquinas a partir dos dados, ao invés de utilizar léxicos anotados manualmente por humanos. Assim sendo, este estudo propõe um método de detecção de bots sociais que permite calcular características de sentimento presentes em mensagens a partir do sentiment-specific word embedding (SSWE), um léxico de sentimento aprendido por uma rede neural recorrente homônima, treinada em um grande volume histórico de mensagens. Isto posto, o objetivo deste trabalho é obter evidências experimentais que apontem para a validade da hipótese levantada. O método proposto combina as informações de sentimento extraídas das palavras das mensagens com outros atributos comportamentais associados às contas e aplica algoritmos de aprendizado de máquina para classificar contas em bot e não bot. Experimentos realizados em uma base de dados popular em estudos de detecção de bots sociais geraram evidências que confirmam a hipótese levantada, entre elas a melhoria da acurácia dos modelos de detecção em mais de 5 pontos percentuais.',
  resumo_en: 'The use of bots on social networks for malicious purposes has grown significantly in recent years. Among the last generation techniques used in the automatic detection of social bots, are those that take into account the sentiment existing in the messages propagated on the network. This information is calculated based on lexicons of sentiment with content manually annotated by a restricted set of people. Like any process that involves human interpretation, such an annotation is subject to subjectivity. In addition, in the manual annotation process, words are analyzed in isolation, without taking into account the text in which they are inserted, which may not be sufficient to accurately express the sentiment existing in the context. In view of these limitations, this work raises the hypothesis that the automatic detection of social bots that considers the sentiment characteristics of the words of the messages can be improved if these characteristics were previously learned by machines from the data, instead of using manually annotated lexicons by humans. Therefore, this study proposes a method of detecting social bots that allows the calculation of sentiment characteristics present in messages from the sentiment-specific word embedding (SSWE), a lexicon of sentiment learned by a homonymous recurrent neural network, trained in a large historical volume of messages. That said, the objective of this work is to obtain experimental evidence that points to the validity of the raised hypothesis. The proposed method combines the sentiment information extracted from the words of the messages with other behavioral attributes associated with the accounts and applies machine learning algorithms to classify accounts into bot and non-bot. Experiments carried out in a popular database in social bot detection studies have generated evidence that confirms the raised hypothesis, including the improvement in the accuracy of detection models by more than 5 percentage points.'},

 {numero: 761, ano: 2020, dia: '09/12', autor: 'Apolo Takeshi Arai Batista', orientador: [13, 255], linha: 3, arquivo: '2020-Apolo.pdf',
  titulo: 'Detecção de Fraude em Social Commerce: uma Abordagem Baseada na Combinação de Informações Estruturadas e Imagens',
  resumo_pt: 'As necessidades de mercado demandaram mudanças nos e-commerce e nas redes sociais, resultando no surgimento do social commerce. Somado a isto, é observado o contínuo crescimento das transações comerciais em Social Commerce. Este ambiente é atraente para usuários bem e mal intencionados, os últimos causam danos financeiros e psicológicos as suas vítimas por meio das fraudes online. Como a volumetria de casos é alta e a ocorrência de fraude é baixa, o processo de detecção manual não é escalável e é ineficiente, muito recurso para pouca detecção. Por outro lado, as soluções existentes para a detecção automática de fraude em Social Commerce baseiam-se em informações estruturadas extraídas dos anúncios (i.e.: preço, se é novo ou usado, qual a cor, etc.). Entretanto, tais soluções ignoram potenciais indícios de fraude nas imagens que complementam as informações sobre o objeto comercializado. Assim, o presente trabalho propõe uma investigação da utilização de Deep Learning para combinação de diferentes algoritmos unindo base de dados estruturadas e imagens. Desse modo, foi feita uma proposta de um método chamado de DFSC (Detecção de Fraude em Social Commerce), que permitiu a análise de anúncios considerando suas respectivas imagens e informações estruturadas. Para tais combinações foram estudados a utilização de modelos de dados estruturados isoladamente, de imagens isoladamente, e duas formas de combinação dos modelos seja considerando a saída dos modelos de imagem como um dado enriquecido para um modelo de dados estruturado, seja por meio da composição de modelos de dados estruturados e de imagens sujeitos a um novo classificador para avalia-los. Tais experimentações se mostraram promissoras, apresentando resultados que evidenciam que a consideração da imagem impacta de modo significativo a qualidade de detecção de fraude em Social Commerce, com ganhos de até 20% em F-Score.',
  resumo_en: 'The Social commerce has risen due to changes in either e-commerce or social networks. On top of that, the number of online ads and transactions in Social Commerce has grown. This environment is attractive to either good users and bad users. The bad users cause harm to their victim by making them lose money or psychological damage. Since the volume of transactions is high and the fraud occurrence is low, the manual detection is not scalable and highly inefficient, too much resource wasted for low detection. The existing solutions for automatic fraud detection in Social Commerce, e-commerce with social networks, are based on structured information available in ads such as price, product type, brand, new/used, among others. However, such solutions ignore possible fraud signs from the ads’ images that exhibit the product sold. Therefore, this article aims to evaluate if combining structured information and images available in the ads provides more effective models than the ones that consider only structured information. The proposed method uses deep learning to evaluate the images. It has been developed a method called Fraud Detection at Social Commerce (FDSC) which has enabled the combined evaluation of ads’ images and structured information. Experimental evidence shows an incremental opportunity of 20% in F-score by the adoption of this method.'},

 {numero: 760, ano: 2020, dia: '04/12', autor: 'Flávio Roberto Matias da Silva', orientador: [13], linha: 3, arquivo: '2020-Matias.pdf',
  titulo: 'FakeNewsSetGen: um Processo para Construção de Datasets que Viabilizem a Comparação entre Métodos de Detecção de Fake News Baseados em Diferentes Demandas de Informação',
  resumo_pt: 'Devido ao fácil acesso e ao baixo custo, o consumo de notícias on-line em redes sociais aumentou significativamente na última década. Apesar de seus benefícios, algumas redes sociais permitem que qualquer pessoa divulgue notícias com intenso poder de difusão, o que amplia um problema antigo: a disseminação do fake news (i.e., notícias falsas veiculadas de forma intencional). A proliferação de fake news, geralmente, afeta não apenas a integridade jornalística, mas também perturba as áreas social, política, econômica, cultural, assim como da saúde e segurança. Diante desse cenário, foram propostos vários métodos baseados em aprendizado de máquina para detectar automaticamente fake news (machine learningbased methods to automatically detect fake news- MLFN). Esses métodos necessitam de datasets para treinar e avaliar seus modelos de detecção. Embora os MLFN recentes tenham sido projetados para considerar dados sobre a propagação de notícias em redes sociais, poucos dos datasets disponíveis contêm esses dados. Assim, a comparação de desempenho entre MLFN está restrita à utilização de um número limitado de datasets. Além disso, os datasets existentes com dados de propagação não contêm notícias em português, o que prejudica a avaliação do MLFN nesse idioma. Portanto, este trabalho propõe o FakeNewsSetGen, um processo de construção de datasets para o estudo de fake news que contenham dados de propagação de notícias e viabilizem a comparação entre MLFN. O processo de engenharia de software do FakeNewsSetGen foi orientado para incluir todos os tipos de dados exigidos pelos MLFN existentes. Para ilustrar a viabilidade e adequação do FakeNewsSetGen, foi realizado um estudo de caso que abrange a implementação de um protótipo do FakeNewsSetGen e a aplicação desse protótipo para criar uma instância de dataset denominada FakeNewsSet, composta de notícias em português. Dez MLFN com diferentes tipos de requisitos de dados (sete deles exigindo dados de propagação de notícias) foram aplicados ao FakeNewsSet e comparados, demonstrando o potencial de utilização do processo proposto e do dataset criado.',
  resumo_en: 'Due to easy access and low cost, social media online news consumption has increased significantly for the last decade. Despite their benefits, some social media allow anyone to post news with intense spreading power, which amplifies an old problem: the dissemination of fake news (ie., false information that is spread deliberately to deceive). The proliferation of fake news generally affects not only journalistic integrity, but also disrupts social, political, economic, cultural, as well as health and safety. In the face of this scenario, several machine learning-based methods to automatically detect fake news (MLFN) have been proposed. All of them require fake news datasets to train and evaluate their detection models. Although recent MLFN were designed to consider data regarding the news propagation on social media, most of the few available fake news datasets do not contain this kind of data. Hence, comparing the performances amid those recent MLFN and the others is restricted to a very limited number of datasets. Moreover, all existing datasets with propagation data do not contain news in Portuguese, which impairs the evaluation of the MLFN in this language. Thus, this work proposes FakeNewsSetGen, a process that builds fake news datasets that contain news propagation data and support comparison amid the state-of-the-art MLFN. FakeNewsSetGen’s software engineering process was guided to include all kind of data required by the existing MLFN. In order to illustrate FakeNewsSetGen’s viability and adequacy, a case study was carried out. It encompassed the implementation of a FakeNewsSetGen prototype and the application of this prototype to create a dataset called FakeNewsSet, with news in Portuguese. Ten MLFN with different kind of data requirements (seven of them demanding news propagation data) were applied to FakeNewsSet and compared, demonstrating the potential use of both the proposed process and the created dataset.'},

 {numero: 759, ano: 2020, dia: '23/11', autor: 'Miguel Ângelo Lellis Moreira', orientador: [17, 254], linha: 3, arquivo: '2020-Miguel.pdf',
  titulo: 'Formulação de uma Nova Modelagem do Método Promethee',
  resumo_pt: 'O método PROMETHEE é um modelo de apoio multicritério à decisão baseado em problemáticas de sobreclassificação, ordenando ações ou alternativas das mais favoráveis as menos favoráveis como formas de solução de um problema. O estudo tem por propósito apresentar um novo método de apoio à decisão, denominado PROMETHEE-SAPEVO-M1. Mediante uma revisão da literatura baseada nos métodos da família PROMETHEE, foram identificadas vinte e três variantes, apresentando diferentes modelos de aplicação. Neste contexto, o novo modelo proposto aborda uma integração de dois métodos multicritérios, o PROMETHEE e SAPEVO-M, integrando avaliações quantitativas e qualitativas mediante entradas cardinais e ordinais respectivamente, onde as entradas ordinais são também utilizada como formato de indicação de preferência entre critérios em prol de obter-se uma importância global e consecutivamente seu respectivo peso. A modelagem provê três modelos de análise de preferência, como análise de preferência parcial, total e por intervalos, além de uma análise intra-critério por limiar de veto, permitindo analisar o desempenho de uma alternativa em um específico critério. Como forma de suporte a aplicação do método de forma eficaz, uma plataforma computacional web é apresentada, viabilizando uma implementação trivial e análise robusta, mediante resultados numéricos e gráficos. Como forma de demonstração da aplicabilidade do método, um estudo de caso real é realizado, abordando uma análise estratégica da Marinha do Brasil quanto aquisição de drones como forma de apoio as operações de guerra naval. A problemática em avaliação foi estruturado com base na abordagem Valued-Focused Thinking, identificando alternativas e critérios que viabilizassem o alcance dos objetivos estratégicos definido durante a análise da problemática. O método PROMETHEE-SAPEVO-M1 mostrou-se favorável ao dado caso, proporcionando avaliar as alternativas em abordagens quantitativas e qualitativas e considerando a subjetividade do agente decisor, possibilitando a obtenção das relações de sobreclassificação e identificação das alternativas mais favoráveis ao dado caso. A análise dos resultados é apresentada de modo detalhado dentro das quatro formas de análises considerando avaliações inter-critério e intra-critério, juntamente com um procedimento de rank reversal, onde removeu-se duas alternativas para identificação de relações de dependências ou influências na análise. Uma análise de comparação de resultados em cenários considerando apenas critérios quantitativos ou qualitativos também é apresentada em busca de evidenciar a relativa importância em integrar-se ambas naturezas.',
  resumo_en: 'The PROMETHEE method is a model of multicriteria decision support based on outranking problems, ranking actions or alternatives from the most favorable to the least favorable as ways of solving a problem. The study aims to present a new decision-support method, named PROMETHEE-SAPEVO-M1. Through a literature review based on the PROMETHEE family methods, twenty-three variants have been identified, presenting different models of application. In this context, the new model approaches an integration of two multicriteria methods, PROMETHEE and SAPEVO-M, integrating quantitative and qualitative assessments through cardinal and ordinal inputs respectively, where the ordinal input is also used as a preference indicator between criteria, in search to obtain its global importance and consecutively their respective weight. The modeling provides three models of preference analysis, such as partial, total, and interval preference analysis, in addition to an intra-criterion analysis by veto threshold, allowing to analyze the performance of an alternative in a specific criterion. As a way to support the application of the method effectively, a web platform is presented, enabling a trivial implementation and robust analysis, through numerical and graphical results. As a way of demonstrating the applicability of the method, a real case study is approached, addressing a strategic analysis of the Brazilian Navy regarding the acquisition of drones as a way to support naval warfare operations. The problematic in evaluation was structured based on the Valued-Focused Thinking approach, identifying alternatives and criteria that would enable the achievement of the strategic objectives defined during the analysis of the problem. The PROMETHEESAPEVO- M1 method proved to be favorable to the given case, providing an assessment of the alternatives in quantitative and qualitative approaches, considering the subjectivity of the decision-maker, making it possible to obtain the outranking relations and identify the most favorable alternatives to the given case. The result analysis is presented in detail by four types of evaluation, considering inter-criterion and intra-criterion approaches, along with a rank reversal procedure, where two alternatives were removed to identify dependency relationships or influences in the analysis. A comparative analysis of results in scenarios considering only quantitative or qualitative criteria is also presented in searcht to highlight the relative importance of integrating both natures.'},

 {numero: 758, ano: 2020, dia: '15/09', autor: 'Jéssica de Almeida Berlezi', orientador: [11], linha: 3, arquivo: '2020-Jessica.pdf',
  titulo: 'Um Estudo da Evolução de Clusters de Code Smell em Relação ao Uso de Técnicas de Refatoração',
  resumo_pt: 'Este trabalho tratou de analisar clusters de classes com codes smells relacionadas entre si. Para isto versões de projeto diferentes foram analisadas na busca da existência de versões do código em que no mínimo 2 clusters fossem encontrados. Os resultados preliminares deste estudo no projeto Lucene-Solr demonstraram que uma refatoração que buscava otimizar classes o desempenho de testes de determinada classe, resultou na separação deste trecho em um novo code smell. Analisando melhor o caso foi possível perceber que a classe central deste novo cluster estava na borda tanto dos clusters de Big Method quanto de Big Class (Os 2 únicos que a classe pertencia dos 4 analisados). Repassando todas as classes por cluster e analisando seus relacionamentos, foi possível notar que sempre que uma classe estava pouco relacionado ao restante do cluster, este comportamento se repetia nos demais clusters de code smells que esta classe pertencia.',
  resumo_en: 'This work tried to analyze clusters of classes with codes related to each other. To do so, different project versions were analyzed in search of versions of the code in which at least 2 clusters were found. The preliminary results of this study in the Lucene-Solr project demonstrated that, in a refactoring that sought to optimize classes, the performance of tests of a certain class resulted in the separation of this section in a new code smell. Analyzing better, it was possible to notice that the central class of this new cluster was on the edge of both the Big Method and Big Class clusters (The only 2 that the class belonged of the 4 analyzed). Going through all the classes per cluster and analyzing their relationships, it was possible to notice that whenever a class was little related to the rest of the cluster, this behavior was repeated in the other code smells clusters that this class belonged to.'},

 {numero: 757, ano: 2020, dia: '03/09', autor: 'Airan Poelking Camargo', orientador: [6], linha: 3, arquivo: '2020-AiranCamargo.pdf',
  titulo: 'Aprendizado de Máquina no Apoio ao Diagnóstico Médico utilizando a Classificação Internacional de Doenças',
  resumo_pt: 'A hipótese diagnóstica é um processo preliminar de tomada de decisão do médico gerado em função dos dados clínicos e laboratoriais disponíveis ao final da consulta. Atualmente, o grande número de atendimentos no sistema de emergência prejudica o tempo de espera do paciente e aumenta o risco de erros médicos. O erro de diagnóstico de um médico conduz a um tratamento incorreto ou tardio, e até, em muitos casos, a nenhum tratamento, onde a condição do paciente pode piorar significativamente. De forma a auxiliar o processo de tomada de decisão de um médico, esse estudo propõe a utilização do cadastro nacional de sinais/sintomas e doenças (CID-9) e do prontuário de pacientes como entrada para técnicas de aprendizado de máquina com o objetivo de gerar hipóteses diagnósticas, fornecendo uma segunda opinião para o profissional. O nosso modelo de predição da hipótese de diagnóstico médico utiliza a combinação de dados estruturados e não-estruturados do conjunto de dados do prontuário eletrônico do paciente da UPA de Senador Camará. O modelo de dados desse sistema utiliza o próprio CID-9 para registrar antecedentes, queixa principal, traumas e causas externas, exame físico e protocolos na anamnese do paciente, assim como registrar a hipótese de diagnóstico do médico. Ao final, apresentamos uma avaliação qualitativa com resultados gerados por diferentes algoritmos aplicados em diferentes modelos propostos e diferentes visões do mesmo conjunto de dados. Propomos um modelo composto de predição que apresentou os melhores resultados em todos os cenários avaliados. O classificador gerador a partir de um Comitê de Classificação para identificar o capítulo CID de doença obteve uma taxa de acurácia de 46,01% e o classificador gerado a partir da técnica de Árvore de Decisão para identificar a categoria CID de doença obteve uma taxa de acurácia média de 57,59%.',
  resumo_en: 'The diagnostic hypothesis is a preliminary decision-making process of the physician that is generated based on the clinical and laboratory data available at the end of the consultation. Currently, the large number of emergency room visits impairs patient waiting time and increases the risk of medical errors. The diagnosis error of a physician leads to an incorrect or late treatment, and even, in many cases, to no treatment, where the patient’s condition can get worse significantly. In order to assist the physician’s decision-making process, this study proposes the use of the national registry of signs, symptoms and diseases (ICD-9) and patient records by applying machine learning techniques with the objective of generating hypotheses while providing a second opinion for the professional. Our medical diagnosis prediction model uses a combination of structured and unstructured data from the dataset of the patient’s electronic medical record at the Senador Camará UPA. The data model of this system uses the ICD-9 itself to record antecedents, main complaints, traumas, external causes, physical examination and protocols in the patient’s anamnesis, while registering the hypothesis of medical diagnosis. We, also, present a qualitative evaluation with results generated by different algorithms applied in different models proposed and different views of the same dataset. We propose a composite model of prediction that presented the best results in all evaluated scenarios. The Ensemble Classifiers algorithm to identify the ICD chapter of disease obtained an accuracy rate of 46.01% and the Decision Tree algorithm to identify the ICD category of disease obtained an average accuracy rate of 57.59%.'},

 {numero: 756, ano: 2020, dia: '31/08', autor: 'Thiago João Miranda Baldivieso', orientador: [9, 253], linha: 2, arquivo: '2020-ThiagoBaldivieso.pdf',
  titulo: 'Um Estudo do Uso de VANTS para a Reconstrução de Cenas 3D',
  resumo_pt: 'Estudos relacionados à visão computacional envolvendo reconstrução tridimensional vem aumentando atualmente e demonstra a importância da área que é um subdomínio da fotogrametria. Aplicações com VANTs (Veículos Aéreos Não Tripulados) para mapeamento de terrenos utilizando a fotogrametria para criar ambientes tridimensionais facilitam análises estruturais e viabilizam a obtenção de informações topográficas de superfície terrestre capturada, utilizando metodologias modernas aplicáveis tanto na área civil como militar. O assunto possui relevância, pois devido às características da fotogrametria com VANTs proporcionam facilidade de acesso, precisão e economia de tempo de missão e em equipamentos. Esta dissertação tem como objetivo o desenvolvimento de reconstrução tridimensional utilizando imagens aéreas em ambientes distintos. Durante o estudo foram realizados experimentos com aeronaves em ambientes externos e internos, após a aquisição das imagens aéreas foi realizada a reconstrução utilizando três softwares específicos de fotogrametria, dois deles comerciais e um software livre, em seguida com avaliação qualitativa dos resultados. Finalizando com indicações de melhorias e trabalhos futuros para pesquisas relacionadas com técnicas de inteligência artificial.',
  resumo_en: 'Studies related to computer vision involving three-dimensional reconstruction are currently increasing and demonstrate the importance of the area that is a subdomain of photogrammetry. Applications with UAVs (Unmanned Aerial Vehicles) for terrain mapping using photogrammetry to create three-dimensional environments facilitate structural analyzes and make it possible to obtain topographic information from captured land surfaces, using modern methodologies applicable in both civil and military areas. The subject has relevance, because due to the characteristics of photogrammetry with UAVs, they provide easy access, precision and savings in mission time and in equipment. This dissertation aims to develop three-dimensional reconstruction using aerial images in different environments. During the study, experiments were carried out with aircraft in external and internal environments, after the acquisition of aerial images, reconstruction was performed using three specific photogrammetry software, two of them commercial and one free software, followed by a qualitative evaluation of the results. Concluded with indications of improvements and future work for research related to artificial intelligence techniques.'},

 {numero: 755, ano: 2020, dia: '27/08', autor: 'Magno Alves Cavalcante', orientador: [5, 2], linha: 2, arquivo: '2020-Magno.pdf',
  titulo: 'Um Sistema Híbrido de Criptografia para Garantir a Confidencialidade e o Não Repúdio sobre Documentos Eletrônicos Armazenados em Blockchain, no Contexto da Defesa Cibernética',
  resumo_pt: 'A tecnologia blockchain é o resultado da acumulação de conhecimento técnico-científico na área de tecnologia da informação, principalmente nos campos da criptografia, sistemas distribuídos, redes ponto a ponto, bancos de dados e linguagens formais. Blockchain partiu de uma tecnologia de aplicação específica que suporta uma criptomoeda, com uma estrutura de dados resistente às fraudes, para uma tecnologia de aplicação multipropósito que pode suportar o processamento de retaguarda de diversos tipos de aplicações que necessitam de recursos de segurança. Em uma transação entre duas partes, a rede blockchain tem a capacidade de fornecer um grau de confiança equivalente a uma terceira parte confiável, porque as informações armazenadas servem para comprovar a integridade e a rastreabilidade da transação ocorrida. As plataformas de blockchain proveem rastreabilidade, integridade, imutabilidade e disponibilidade por construção nativa, fornecendo meios para os auditores atestarem a rastreabilidade temporal dos dados armazenados nos blocos. Porém as plataformas de blockchain não proveem confidencialidade e não repúdio nativamente. Neste trabalho foi definido um sistema híbrido de criptografia para agregar as características de confidencialidade e não repúdio a um sistema de notarização de documentos eletrônicos, o qual é executado sobre uma rede blockchain permissionada, gerando-se confiança suficiente para o uso seguro posterior dos documentos que estão armazenados. As contribuições deste trabalho são: melhoria na segurança dos dados em redes blockchain ao agregar as características de confidencialidade e não repúdio ao conjunto de dados armazenado; definição de um sistema híbrido de criptografia para blockchain, agregando as características de confidencialidade e não repúdio aos dados armazenados na estrutura da blockchain, que possibilita armazenar informações confidenciais em redes blockchain, sobre as quais o administrador da organização não tem controle total; definição de um processo de notarização de documentos eletrônicos em blockchain, estruturado sobre uma rede blockchain permissionada; definição de um modelo de dados para a composição de conteúdo dentro um novo bloco encadeado na blockchain; definição de um modelo para estabelecer autointegridade dentro do próprio conjunto de dados que é armazenado na blockchain; definição da centralização das regras de negócio compartilhadas no processo de notarização a partir um contrato inteligente; definição de um modelo de arquitetura de software para sistemas com blockchain; e a especialização de um sistema de blockchain permissionada para agregar as características de confidencialidade e não repúdio no conjunto de dados que é armazenado em um bloco encadeado dentro da blockchain.',
  resumo_en: 'Blockchain technology is the result of accumulation of technical and scientific knowledge in the area of information technology, mainly in the fields of cryptography, distributed systems, point-to-point networks, databases and formal languages. Blockchain began from application-specific technology that supports a cryptocurrency, with a tamper-resistant data structure, to a multipurpose application technology that can support the backward processing of various types of applications that require security features. In a transaction between two parties, the blockchain network has the ability to provide a degree of trust equivalent to a trusted third party, because the stored information serves to prove the integrity and traceability of the transaction that occurred. Blockchain platforms provide traceability, integrity, immutability and availability by native construction, providing resources for auditors to attest the temporal traceability of data stored in the blocks. However, blockchain platforms do not provide confidentiality and non repudiation natively. In this work, a hybrid cryptography system was defined to add the characteristics of confidentiality and non-repudiation to an electronic document notarization system, which is executed over a permissioned blockchain network, generating sufficient confidence for the subsequent secure use of documents that are stored. The contributions of this work are: improvement of data security in blockchain networks by adding the characteristics of confidentiality and non-repudiation to the data set stored; definition of a hybrid cryptography system for blockchain, adding the characteristics of confidentiality and non-repudiation to the data stored in the blockchain structure, which makes it possible to store confidential information in blockchain networks, over which the organization’s administrator does not have full control; definition of a notarization process of electronic documents in blockchain, structured on a permissioned blockchain network; definition of a data model for composing content within a new block chained in the blockchain; definition of a model to establish self-integrity within the data set that is stored in the blockchain; defining the centralization of shared business rules in the notarization process as from a smart contract; definition of a software architecture model for systems with blockchain; and the specialization of a permissioned blockchain system to aggregate the characteristics of confidentiality and non-repudiation in the data set that is stored in a chained block within the blockchain.'},

 {numero: 754, ano: 2020, dia: '18/06', autor: 'Fabricio Maione Tenório', orientador: [17, 254], linha: 3, arquivo: '2020-Fabricio.pdf',
  titulo: 'Modelagem Multicritério: uma Evolução do Método THOR',
  resumo_pt: 'O método multicritério de apoio à decisão THOR busca por intermédio de um processo decisório, ordenar alternativas discretas, eliminando critérios redundantes e verificando, por exemplo, a existência de duplicidade de informações, elevação da imprecisão do processo de decisão ou quantificação desta imprecisão. O presente estudo tem como objetivo propor uma evolução axiomática do método, denominado THOR 2, a partir da análise do algoritmo original, contemplando a evolução do conhecimento a ser incorporada no algoritmo. Diante da análise do algoritmo original foi proposta a distinção na atribuição dos pesos no somatório de pontuações para as situações de indiferença e preferência fraca em S1, S2 e S3. Além disso, convencionou-se que nas situações em que ocorrem preferência estrita, preferência fraca e indiferença, o valor do peso do critério seja multiplicado pelo índice nebuloso-aproximativo, deteriorando, desta forma, a comparação em função do grau de segurança do dado. Esta funcionalidade permite que na falta de dados para o preenchimento da classificação das alternativas e dos pesos na matriz de decisão, seja possível estimar o dado e atribuir um baixo valor de pertinência para atribuição daquele dado, evitando-se, desta forma, a eliminação da alternativa ou do critério devido à ausência do dado. Esta dissertação, também, apresenta um estudo de caso de uma estratégia para compra de oportunidade de uma fragata para a Marinha do Brasil, objetivando auxiliar no processo de tomada de decisão de um navio de referência. No estudo de caso, foi feita uma análise comparativa entre os resultados dos métodos THOR e THOR 2, onde apontou-se o navio LCF como melhor alternativa em ambos os métodos. No estudo avaliou-se, adicionalmente, a função de pertinência do THOR 2, no qual o método mostrou-se um eficiente instrumento em situações que ocorrem incerteza ou falta de dados.',
  resumo_en: 'The multicriteria decision support method THOR seeks, through a decision-making process, to order discrete alternatives, eliminating redundant criteria and verifying, for example, the existence of duplicate information, increasing the imprecision of the decision process or quantifying this imprecision. The present study aims to propose an axiomatic evolution of the method, called THOR 2, from the analysis of the original algorithm, contemplating the evolution of knowledge to be incorporated into the algorithm. In view of the analysis of the original algorithm, a distinction in the allocation of weights in the sum of scores was proposed for situations of indifference and weak preference in S1, S2 and S3. In addition, it was agreed that in situations where strict preference, weak preference and indifference occur, the weight value of the criterion is multiplied by the fuzzy-approximate index, thus deteriorating the comparison according to the degree of security of the data. This functionality allows that, in the absence of data to fill in the classification of alternatives and weights in the decision matrix, it is possible to estimate the data and assign a low pertinence value for attributing that data, thus avoiding the elimination of alternative or criterion due to the absence of the data. This dissertation also presents a case study of a strategy for buying opportunity from a frigate for the Brazilian Navy, aiming to assist in the decision-making process of a reference ship. In the case study, a comparative analysis was made between the results of the THOR and THOR 2 methods, where the LCF was pointed out as the best alternative in both methods. In the study, the THOR 2 pertinence function was further evaluated, in which the method proved to be an efficient instrument in situations where uncertainty or lack of data occurs.'},

 {numero: 753, ano: 2020, dia: '04/06', autor: 'Silas Luiz Cardoso Furtado Cruz de Lima', orientador: [4], linha: 2, arquivo: '2020-Silas.pdf',
  titulo: 'Uso de Visão Computacional e Redes Neurais Convolucionais no Reconhecimento de Libras',
  resumo_pt: 'Nos últimos anos é possível notar diversos esforços no sentido de aumentar a inclusão de pessoas portadoras de algum tipo de deficiência. Dentre essas deficiências, podemos destacar a dificuldade em se comunicar verbalmente que atinge uma parcela considerável de nossa população. Com isso, o estudo em escala global de linguagem de sinais tem se tornado uma importante área de pesquisa, no objetivo de atenuar os obstáculos impostos no dia a dia das pessoas surdas e/ou com alguma deficiência auditiva e aumentar a integração destas pessoas na sociedade majoritariamente ouvinte em que vivemos. Baseado nisso, esta dissertação de mestrado propõe, apoiando-se nos fundamentos de visão computacional, o desenvolvimento de um sistema de informação para o reconhecimento automático de Língua Brasileira de Sinais (LIBRAS), que tem como objetivo simplificar a comunicação entre deficiente auditivos conversando em LIBRAS e ouvintes que não conheçam esta língua de sinais. O reconhecimento é feito através de processamento de frames em sequência de imagens digitais (vídeos) por meio do processamento em sequências de imagens digitais (vídeos) com pessoas realizando sinais em LIBRAS, sem o uso de hardwares de apoio, como luvas coloridas e/ou sensores ou até mesmo o uso de gravações em altas resoluções feitas em laboratórios com ambientes controlados, focando, para este trabalho, em sinais que utilizam apenas as mãos. Dada a grande dificuldade de criação de um sistema com este propósito, foi utilizada uma abordagem para o seu desenvolvimento por meio da divisão em etapas. Por se tratar em numa tarefa complexa considera-se que todas as etapas do trabalho proposto servem de abertura para estudo em trabalhos futuros da área de reconhecimento de sinais, além de poderem contribuir para outros tipos de trabalhos que envolvam processamento de imagens, segmentação de imagem em diversos tons de pele humana, rastreamento de objetos, entre outros. Para atingir o objetivo proposto pelo trabalho, além de utilizar os conceitos de visão computacional, para realizar a identificação dos sinais dinamicamente, foram utilizados os conceitos e ferramentas de redes neurais para extrair as características de interesse e classificá-los adequadamente. Para isso, foi construído um dataset composto de três mil imagens dos sinais referentes ao alfabeto em LIBRAS, sem a utilização de luvas coloridas, auxílio de qualquer outro hardware, ou laboratório com ambiente controlado e após isso foi construído uma ferramenta para identificar, com o auxílio de uma webcam, o sinal executado por um usuário e transcrevê-lo em tela . Após o treinamento da rede neural e submetido o classificador a um conjunto de teste, quanto à acurácia para o reconhecimento dos sinais referentes ao alfabeto, o sistema proposto atingiu aproximadamente 98% de acerto para reconhecer um dataset de amostras de imagens de teste construído atingindo assim, o objetivo proposto neste trabalho com êxito.',
  resumo_en: 'In recent years, it is possible to note several efforts to increase the inclusion of people with some type of disability. Among these deficiencies, we can highlight the difficulty in communicating verbally, which affects a considerable portion of our population. With this, the study on a global scale of sign language has become an important research area, in order to mitigate the obstacles imposed in the daily life of deaf people and / or with some hearing impairment and increase the integration of these people in society mostly listener in which we live. Based on this, this study proposes, based on the fundamentals of computer vision, the development of an information system for the automatic recognition of Brazilian Sign Language (LIBRAS), which aims to simplify communication between hearing impaired people talking in POINTS and listeners who do not know this sign language. Recognition is done by processing frames in sequence of digital images (videos) by processing in sequences of digital images (videos) with people performing LIBRAS signals, without the use of supporting hardware, such as colored gloves and / or sensors or even the use of high resolution recordings made in laboratories with controlled environments, focusing, for this work, on signals that use only the hands. Given the great difficulty of creating a system for this purpose, an approach to its development through division into stages was used. Because it is a complex task, it is considered that all stages of the proposed work serve as an opening for study in future works in the area of signal recognition, in addition to being able to contribute to other types of work that involve image processing, image segmentation in different tones of human skin, object tracking, among others. To achieve the objective proposed by the work, in addition to using the concepts of computer vision, to perform the identification of the signals dynamically, the concepts and tools of neural networks were used to extract the characteristics of interest and classify them accordingly. For this, a dataset was built, composed of three thousand images of the signs referring to the alphabet in LIBRAS, without the use of colored gloves, aid of any other hardware, or laboratory with controlled environment. After that, a tool was built to identify, with the aid of a webcam, the signal executed by a user and transcribe it on screen. After training the neural network and submitting the classifier to a test set, as to the accuracy for the recognition of the signs referring to the alphabet, the proposed system reached approximately 98 % of correctness to recognize a dataset of test image samples built thus achieving the objective proposed in this work successfully.'},

 {numero: 752, ano: 2020, dia: '06/05', autor: 'Daniel de Almeida Rocha', orientador: [6], linha: 2, arquivo: '2020-Rocha.pdf',
  titulo: 'Simulação do Comportamento Humano em Jogos Utilizando Aprendizado de Máquina',
  resumo_pt: 'O estudo da Inteligência Artificial e, principalmente, do aprendizado de máquina em jogos é de grande interesse para a indústria de jogos, devido à sua ampla aplicação em vários cenários e às suas capacidades de simular o comportamento humano para personagens do jogo. Nos cenários de jogos online multiplayer, uma das maiores preocupações são as desconexões dos jogadores e como substituí-los por robô que mantenha o equilíbrio do jogo. Neste trabalho, é proposta uma metodologia baseada em aprendizado de máquina que permite simular o comportamento de jogadores por meio de um aprendizado na sua base histórica de jogos que utiliza redes neurais Perceptron de múltiplas camadas avaliadas em 3 jogos clássicos, sendo eles Blackjack, Hearts e Xadrez. Os resultados obtidos nos experimentos indicam que o método proposto apresenta bom desempenho para jogos finitos e de informações completas, uma vez que os modelos gerados conseguiram aproximar quantidades semelhantes de vitórias ao comparar com os comportamentos com os quais foram treinados. Pode se concluir que o aprendizado por comportamento pode ser melhorado ainda mais em jogos que possuem grande quantidade de combinações possíveis e sem variações durante a partida, como é o exemplo do Xadrez.',
  resumo_en: 'The study of Artificial Intelligence and, more specifically, Machine Learning in games is of great interest to the game industry, due to their wide application in several scenarios and ther capabilities of simulating human behavior by game characters. In online multiplayer game scenarios, one of the biggest concerns is disconnecting players and how to replace them with a bot that maintains the balance of the game. In this work, a methodology based on machine learning is proposed that allows to simulate the behavior of players that learns from their historical game base that uses Perceptron neural networks of multiple layers evaluated in 3 classic games, being Blackjack, Hearts and Chess. The results obtained in the experiments indicate that the proposed method performs well for finite games and complete information, since the models generated managed to approximate similar amounts of victories when comparing with the behaviors with which they were trained. It can be concluded that that behavioral learning can be further improved in games that have a large number of possible combinations and without variations during the game, as is the example of the Chess game.'},

 {numero: 751, ano: 2020, dia: '19/03', autor: 'Marta Rigaud Faria', orientador: [8, 15], linha: 3, arquivo: '2020-Marta.pdf',
  titulo: 'DEFESA: uma Metodologia para Análise de Incidentes de Segurança da Informação',
  resumo_pt: 'O aumento substancial da gravidade dos danos e prejuízos causados por Incidentes de Segurança da Informação levam à busca de novas estratégias de defesa cibernética. E, ao observar os relatórios estatísticos, percebe-se que a mesma técnica de ataque costuma ser replicada inúmeras vezes. Diante desses fatos, as organizações têm buscado reunir as informações de Incidentes de Segurança de Informação em um único ambiente para analisá-las e compará-las com o objetivo de oferecer um suporte à tomada de decisão que leve a uma redução significativa do número de ocorrências de incidentes. Porém, os conflitos conceituais inerentes do domínio e as diferentes formas de categorização tornam a tarefa de reunir informações bastante complexa. Apesar de haver várias ontologias de Incidente de Segurança da Informação, elas enfatizam a representação de parte do domínio, ou seja, não visam uma abordagem aberta e genérica para a interoperabilidade. Todas essas diferentes formas de representação e, até mesmo a falta de uma representação formal evidenciam a necessidade de haver um modelo amplamente aceito para ser utilizado como referência para comunicar, trocar informações e correlacionar as ontologias que representem parte do domínio. Em outros domínios os conflitos conceituais foram minimizados com o uso da chamada análise ontológica, que usa ontologias de fundamentação para nortear a concepção de um modelo de referência. Neste trabalho, escolheu-se utilizar a UFO, como ontologia de fundamentação, devido ao seu amplo uso em diversos domínios. Além disso, em domínios que requerem múltiplos níveis de classificação, a MLT tem sido aplicada em conjunto com a UFO. Outras abordagens, se beneficiam da expressividade semântica da ontologia para auxiliar no desenvolvimento de sistema de apoio à decisão. Visando obter todos esses benefícios no domínio de Incidente de Segurança da Informação, este trabalho propõe uma metodologia para Definir, Especificar e Formalizar os conceitos do domínio com Ênfase em oferecer Suporte à tomada de decisão e Aumentar a expressividade semântica (DEFESA). A metodologia DEFESA tem o propósito de ser abrangente com relação a descrição dos conceitos e específica em oferecer suporte à tomada de decisão formando uma arquitetura em camadas. Na camada fundacional fica a UFO-MLT para que os seus sistemas de categorias embasem a ontologia do domínio. Esta ontologia fica no centro da arquitetura, representando os conceitos, suas relações e seus tipos e alicerça o modelo dimensional com expressividade semântica para atender à demanda de construção do sistema de apoio à decisão. A aplicação da metodologia DEFESA no domínio de Incidente de Segurança da Informação resultou na modelagem de sCuDO, a ontologia de domínio de Incidente de Segurança da Informação, na elaboração de sCuD2O, o modelo dimensional de Incidentes de Segurança da Informação e no desenvolvimento de um ambiente de análise com os dados de ocorrências de incidentes de um Grupo de Resposta de Incidentes de Segurança em Computadores.',
  resumo_en: 'The substantial increase in the gravity of damages and losses caused by Information Security Incidents guide us on the search for new cyber defence strategies. And, observing statistical reports, it is understood that the same attack technical is usually applied numerous times. In face of these facts, organizations have been trying to congregate the Information Security Incident informations in a single environment so it is possible to analyze and compare them aiming to offer a support to decision making that will lead to a significant reduction in the number of incidents occurrences. However, conceptual conflicts inherent of this domain and the different ways of categorization make the task of information congregating quite complex. Despite existing several Information Security Incident ontologies, they emphasize the representation of part of the domain, meaning they do not aim at an open and generic approach to facilitate the interoperability. All these different forms of representation and even the lack of formal representation show the need for a widely accepted model to be used as a reference to communicate, exchange information and correlate ontologies that represent part of the domain. In other domains, conceptual conflicts were minimized with the use of the so-called ontological analysis, which uses ontological foundations to guide the design of a reference model. In this work, it was chosen to use UFO, as a foundational ontology, due to its wide use in several domains. And, in domains that require multiple levels of classification, MLT has been applied along with UFO. Other approaches benefit themselves from the ontology semantic expressiveness to assist in the development of a decision support system. Aiming to obtain all of these benefits in the Information Security Incident domain, this work proposes a methodology to define, specify and formalize the concepts of the domain with emphasis in offering support to the decision making and enhance the semantic expressiveness (DEFESA). The DEFESA methodology has the purpose of being comprehensive regarding the description of the concepts and being specific in offering support to decision making, forming a layered architecture. In the foundational layer there is UFO-MLT so that its metacategories system underlies the domain ontology. This ontology stays in the center of the architecture, representing the concepts, its relations and its types and underlies the dimensional model with semantic expressiveness to meet the demands of a decision making system. The application of DEFESA methodology in Information Security Incident domain resulted in the modeling of sCuDO, an ontology of Information Security Incident domain, in the elaboration of sCuD2O, the dimensional model of Information Security Incident and in the development of an environment of analysis with the data of incidents occurrences from a Group of Response to Security Incidents in Computers.'},

 {numero: 750, ano: 2020, dia: '03/02', autor: 'Álex Silva do Prado', orientador: [4], linha: 2, arquivo: '2020-Alex.pdf',
  titulo: 'Um jogo sério utilizando realidade virtual para o tratamento de Ambliopia',
  resumo_pt: 'Os jogos sérios, juntamente com a realidade virtual, estão ganhando cada vez mais aplicações em diversas áreas, com destaque nas áreas médicas, por seu potencial nas terapias e treinamentos. Neste trabalho, é apresentado um jogo sério que explora as características da visão humana juntamente com as características dos visores de realidade virtual para tratar a ambliopia através do ajuste de luminância de alguns elementos das imagens exibidas para cada olho, penalizando o olho bom enquanto estimula o uso do olho amblíope, além de estimular também a fusão binocular. Para isto, a calibração dinâmica de diferentes níveis é definida e aplicada, sendo a principal contribuição deste trabalho, apresentando uma forma diferente de aplicar um ajuste gráfico nos elementos das imagens exibidas para cada olho. O objetivo do jogo é fornecer uma ferramenta que auxilie os profissionais da área oftalmológica, enquanto torna o tratamento dos seus pacientes divertido. Desta forma, o jogo passou por experimentos, com o apoio de especialistas da área, avaliando sua usabilidade e a sensação de presença durante a experiência com jogo. Como resultado, o jogo foi muito bem aceito em relação a sua usabilidade e teve uma avaliação suficiente em relação a sensação de presença experienciada.',
  resumo_en: 'Serious games, along with virtual reality, are used in applications in many areas, especially in medical areas, because of their potencial in therapies and training. In this research, we present a serious game that explores the characteristics of human vision along with the characteristics of virtual reality viewers to treat amblyopia by adjusting the luminance of some objects of the images displayed for each eye, penalizing the good eye while stimulating the lazy eye, as well as stimulating binocular fusion. The multi levels dynamic calibration is defined and applied as the main contribution of this research, presenting a different way of applying a graphic adjustment to the objects of the images displayed for each eye. The goal of the game is to provide an tool that assists eye care professionals while making treating their patients fun. Thus, the game underwent through experiments, with the support of eye care professionals, evaluating its usability and the sense of presence during the game. As a result, the game was very well accepted regarding its usability and had a sufficient rating regarding the sense of presence experienced.'},

 {numero: 749, ano: 2020, dia: '03/02', autor: 'Tamirys Virgulino Ribeiro', orientador: [4], linha: 2, arquivo: '2020-Tamirys.pdf',
  titulo: 'SimObturation: Aplicação da Realidade Virtual para Simulação de Treinamento do Procedimento de Obturação utilizando Dispositivo Tátil com 6GDL',
  resumo_pt: 'A constante evolução das aplicações de realidade virtual na área da odontologia vem ganhando destaque. Ela contribui em diversas áreas de conhecimento que estão associadas à formação de profissionais para a realização de procedimentos odontológicos. Neste trabalho é apresentado um protótipo de simulador virtual para treinamento do procedimento de obturação utilizando um dispositivo tátil com 6 graus de liberdade(GDL) de forma que o dispositivo tátil represente o instrumento clínico manipulado em consultório, reproduzindo-o no cenário virtual. Desta forma, o dispositivo tátil simula a geração de força, sensação tátil e sensação cinestésica através de seu indicador tátil, inclusive quando ocorrer o processo de colisão com algum objeto 3D no cenário. Sendo assim, o protótipo desenvolvido aplica sensação tátil em toda a estrutura 3D criada no ambiente virtual, a fim de auxiliar o usuário a realizar o procedimento de obturação. O ambiente ainda retrata um cenário virtual semelhante ao cenário real, para facilitar o aprendizado da técnica simulada. No protótipo foram desenvolvidos dois cenários para simular o procedimento de obturação. Foi utilizada a plataforma Unity e o ambiente Visual Studio Community, assim como o dispositivo tátil Phantom Omni. Além disso, o ambiente virtual foi desenvolvido em linguagem C#, além de fazer integração com o plugin 3D Systems OpenHaptics Unity. O protótipo SimObturation foi avaliado por dois profissionais da área endodôntica a partir do questionário System Usability Scale (SUS), o que permitiu validar a usabilidade da aplicação. Assim, depois da análise do teste, mesmo com a amostra mostrando-se positiva em relação aos parâmetros de usabilidade, o tamanho da amostra analisada não é suficiente para avaliar se realmente o protótipo é viável para o treinamento dos estudantes no procedimento retratado neste trabalho. Desta forma, sugere-se um aperfeiçoamento no protótipo, afim de deixá-lo mais semelhante à realidade e facilitar cada vez mais a realização do procedimento, além de realizar os testes com estudantes da área odontológica.',
  resumo_en: 'The constant evolution of virtual reality applications in dentistry has been gaining prominence. It contributes in several areas of knowledge that are associated with the training of professionals to perform dental procedures. This work presents a prototype of a virtual simulator for training the filling procedure using a tactile device with 6 degrees of freedom (GDL) so that the tactile device represents the clinical instrument manipulated in the office, reproducing it in the virtual scenario. In this way, the tactile device simulates the generation of force, tactile sensation and kinesthetic sensation through its tactile indicator, even when the collision process with a 3D object in the scenario occurs. Thus, the developed prototype applies tactile sensation to the entire 3D structure created in the virtual environment, in order to assist the user to perform the filling procedure. The environment also portrays a virtual scenario similar to the real scenario, to facilitate the learning of the simulated technique. In the prototype, two scenarios were developed to simulate the filling procedure. The Unity platform and the Visual Studio Community environment were used, as well as the Phantom Omni tactile device. In addition, the virtual environment was developed in C# language, in addition to integrating with the plugin 3D Systems OpenHaptics Unity. The SimObturation prototype was evaluated by two professionals in the endodontic area using the System Usability Scale (SUS) questionnaire, which allowed to validate the usability of the application. Thus, after the analysis of the test, even with the sample showing positive in relation to the usability parameters, the size of the analyzed sample is not sufficient to assess whether the prototype is really feasible for training students in the procedure portrayed in this work. In this way, it is suggested an improvement in the prototype, in order to make it more similar to reality and to facilitate the procedure more and more, in addition to carrying out the tests with dental students.'},

 {numero: 748, ano: 2020, dia: '30/01', autor: 'Argus Antonio Barbosa Cavalcante', orientador: [13, 3], linha: 3, arquivo: '2020-Argus.pdf',
  titulo: 'Predição de Links em Redes Sociais: Resgatando Informações Contextuais e Topológicas no Histórico de Evolução do Grafo',
  resumo_pt: 'A predição de links em redes sociais virtuais é uma tarefa de mineração de grafos. O objetivo desta tarefa é identificar pares de nós não conectados que possuem alta probabilidade de se conectar no futuro. De maneira geral, o estado da arte dos métodos de predição de links considera apenas a estrutura mais completa e recente da rede, não levando em consideração as informações sobre o estado da rede quando novas arestas foram adicionadas à sua estrutura. Os poucos trabalhos que o fizeram, utilizaram exclusivamente informações topológicas sem considerar as importantes informações contextuais disponíveis na rede. Este trabalho levanta a hipótese de que resgatar tanto informações topológicas quanto contextuais pode contribuir na criação de modelos preditivos com maior qualidade do que os existentes no estado da arte, tendo em vista que esses dados combinados podem vir a enriquecer a descrição do problema com exemplos que representam justamente o tipo de evento que se pretende prever: o surgimento de novas ligações. A fim de cumprir esse objetivo, é proposto um método de predição de links baseado em informações topológicas e contextuais do histórico de estados da rede social analisada. Os resultados dos experimentos com redes sociais de coautoria, frequentemente empregadas na avaliação de métodos de predição de links apontam para a validade da hipótese levantada.',
  resumo_en: 'Link prediction in social networks is a graph mining task. This task’s objective is to identify pairs of non-connected vertices that have a high probability to be connected in the near future. In general, the state-of-the-art link prediction methods only consider data from the most complete and recent structure of the network and do not take into account the information about the network state when new edges were added to its structure. The few works that have considered this approach have exclusively used topological information without using the meaningful contextual information available at the network. This study raises the hypothesis that recovering both topological and contextual data may contribute for building more accurate predictive models than the available ones, since those data enrich the description of the network with examples that represent exactly the kind of event to be foreseen: the appearance of new connections. For this purpose, this work proposes a link prediction method based on topological and contextual information from the history of the social network. Results from experiments with co-authorship social networks, frequently employed in link prediction methods evaluation, reveal the validity of the proposed method and the confirmation of the raised hypothesis.'},

 {numero: 747, ano: 2020, dia: '24/01', autor: 'Fábio Luiz Junior', orientador: [9], linha: 2, arquivo: '2020-Fabio.pdf',
  titulo: 'Estratégia líder-seguidor utilizando VANTs para a navegação através do reconhecimento de imagens do terreno',
  resumo_pt: 'Os Veículos Aéreos Não Tripulados (VANTs) são aeronaves que não necessitam de um piloto humano a bordo, podendo ser controlados por um computador embarcado ou por um operador à distância. Nos últimos anos, os VANTs vêm ganhando cada vez mais importância no cenário mundial devido às suas inúmeras possibilidades de aplicação. Neste contexto, as pesquisas sobre VANTs têm buscado introduzir um nível cada vez maior de inteligência embarcada, possibilitando que a máquina tome algumas decisões automaticamente, como o controle do seu trajeto de voo e a resposta a estímulos externos. Uma das linhas de pesquisa que está sendo empregada na automatização de voos é a utilização da visão computacional, a qual está em grande ascensão recentemente.  Este trabalho de dissertação realiza um estudo e implementa um software de navegação de um VANT utilizando o reconhecimento de imagens do terreno. O conceito principal envolvido é navegar um VANT de maneira autônoma, capturando e utilizando as imagens do terreno como waypoints visuais de uma trajetória. Para realizar o pretendido, utilizam-se mecanismos de visão computacional e um VANT com uma câmera embarcada. Divide-se o funcionamento do software de navegação em duas etapas principais, as quais serão descritas a seguir. Na primeira etapa, é realizado um voo de reconhecimento e aquisição das imagens do terreno. Neste voo, um VANT líder fará uma trajetória qualquer, controlado manualmente por um operador. Durante este voo, realizam-se a aquisição das imagens do terreno, o seu processamento e o armazenamento das informações resultantes em um banco de dados de trajetórias. Tais informações irão compor o percurso que se deseja realizar no voo autônomo. Na segunda etapa, são realizados voos autônomos utilizando as imagens colhidas no primeiro voo. Os dados da trajetória são carregados no software de navegação. O controle da trajetória é realizado através do processamento em tempo real das informações obtidas pela câmera embarcada. A partir disso, são enviados os comandos necessários ao piloto automático da aeronave para reconstruir a trajetória original coletada na primeira etapa. Por fim, destaca-se, como contribuição obtida por este trabalho, o desenvolvimento de uma estação de solo capaz de realizar a navegação de VANTs conforme explicitado. Para tanto, foi utilizado o SURF (Speeded-Up Robust Features) como mecanismo de visão computacional para realizar o reconhecimento das imagens do terreno. Além da navegação, o sistema desenvolvido possui também a capacidade para fazer a aquisição, persistência e compartilhamento, entre os VANTs, da trajetória executada. Nos experimentos executados, foram obtidos resultados satisfatórios, tanto para as simulações, quanto para os voos com a plataforma robótica utilizada. Finalmente, frisa-se que o sistema implementado é independente dos mecanismos tradicionais de georreferenciamento por satélites.',
  resumo_en: 'Unmanned Aerial Vehicles (UAVs) are aircrafts that do not require a human pilot on board and can be controlled by an on-board computer or a remote operator. In recent years, UAVs have become increasingly important on the world stage due to their numerous application possibilities. In this context, UAV research seeks to introduce an ever-increasing level of embedded intelligence, enabling the machine to make certain decisions automatically, such as controlling its flight path and responding to external stimuli. One of the lines of research employed in flight automation is the use of computer vision. This work conducts a study and implements a UAV navigation software using terrain image recognition. The main concept involved is navigating a UAV autonomously, capturing and using images of the ground as visual waypoints of a trajectory. To accomplish the desired, computational vision mechanisms and a UAV with an embedded camera are used. The operation of the navigation software is divided into 2 main steps described below. In the first stage, a flight of recognition and acquisition of terrain images is performed. In this flight, a leader UAV makes any trajectory manually controlled by an operator. During this flight, the terrain images are acquired, processed and stored in a trajectory database. Thus, this information will compose the route you wish to take on the autonomous flight. In the second stage, autonomous flights are performed using the images taken on the first flight. The trajectory data is loaded into the navigation software. Trajectory control is performed through real-time processing of information obtained by the onboard camera. From this, the necessary commands are sent to the autopilot of the aircraft to remake the original trajectory collected in the first stage. Finally, I highlight, as a contribution obtained by this work, the development of a ground station capable of performing UAV navigation as explained. For that, the SURF (Speeded-Up Robust Features) was used as a computer vision mechanism to perform the recognition of the terrain images. In addition to navigation, the developed system also has the capacity to make the acquisition, persistence and sharing, between UAVs, of the executed trajectory. In the experiments performed, satisfactory results were obtained, both for simulations and for flights with the robotic platform used. Lastly, it is emphasized that the system implemented is independent of the traditional mechanisms of georeferencing by satellites.'},

 {numero: 746, ano: 2019, dia: '05/12', autor: 'Rômullo Girardi Moreira', orientador: [4], linha: 2, arquivo: '2019-Girardi.pdf', 
  titulo: 'Aplicação da Realidade Virtual no Adestramento do Observador de Artilharia e Generalização da Solução com o Framework IME_VR', 
  resumo_pt: 'A prática da observação de Artilharia tem sido dificultada pelo alto custo e pelo número restrito de áreas de instrução disponíveis. Diante deste cenário, o uso da Realidade Virtual (RV) torna-se uma alternativa viável para a execução desta importante atividade de adestramento. Neste sentido, foi desenvolvido um simulador de RV que permite que um instrutor adestre um observador em um ambiente virtual de emprego da Artilharia. Este simulador foi testado por uma amostra de 13 oficiais de Artilharia com vasta experiência prática em observação. Os resultados foram comparados com valores de referência obtidos a partir dos trabalhos relacionados. Os dados mostraram que o simulador atingiu os valores de referência para os dois parâmetros avaliados: sensação de presença e eficácia. Como a solução desenvolvida para o adestramento do observador de Artilharia foi satisfatória, ela foi generalizada para o framework de RV IMEV R com o intuito de fornecer a arquitetura do simulador para o reuso em outros simuladores de adestramento.', 
  resumo_en: 'The practice of observation in Army Artillery has been hampered by the high cost and the limited number of available instruction areas. Using Virtual Reality (VR) becomes a viable alternative for this important training activity. In this context, was developed a VR simulator in which an instructor can train an observer in a virtual environment of Artillery employment. This simulator was tested by a sample of 13 Artillery officers with extensive practical experience in observation. The results were compared with reference values obtained from related work. The data showed that the simulator reached the reference values for the two parameters evaluated: presence and effectiveness. As the solution obtained for Army Artillery observer training was satisfactory, it was generalized to the VR framework IMEV R in order to provide the system architecture for reuse in other training simulators. '},

 {numero: 745, ano: 2019, dia: '03/12', autor: 'Otavio Augusto Maciel Camargo', orientador: [2], linha: 2, arquivo: '2019-Camargo.pdf', 
  titulo: 'Impacto de Malwares Reais em Sistemas Industriais com Classificação Supervisionada Usando Aprendizado de Máquina', 
  resumo_pt: 'Os Sistemas de Controle Industrial (ICS) são responsáveis pelo controle de infraestruturas críticas que, por sua acuidade, são frequentemente alvos de ataques cibernéticos motivados por interesses políticos, militares ou financeiros. Os sistemas Supervisórios de Controle e Aquisição de Dados (SCADA) estão entre os principais integrantes dos ICS, e constituem sistemas altamente interconectados que empregam soluções comuns à sistemas computacionais convencionais. Os malwares estão entre as principais ameaças cibernéticas a esses sistemas. Existem dificuldades, bem como impossibilidades, para se testar a resiliência cibernética de um ICS real, sendo necessárias plataformas de testes, testbeds, que consistem em verificar o comportamento nocivo dessas ameaças sem colocar em risco o sistema original. Cumulativamente, uma vez que as ferramentas convencionais de detecção, particularmente as baseadas em assinatura, ficam cada vez mais ineficientes devido à evolução das técnicas evasivas, cresce a necessidade da pesquia e desenvolvimento de ferramentas automatizadas e dinâmicas para detecção de malwares. Este trabalho apresenta o testbed MalDomE , desenvolvido para a análise e detecção de malwares em sistemas industriais, com verificação de impacto e utilizando técnicas de aprendizado de máquina em dados textuais. O sistema implementado foi ofuscado para evitar técnicas anti-análise e comparado com outros sistemas não adaptados, comprovando ser 60% mais eficiente quanto à ocultação de atributos que o denunciem como um sistema de análise. Conjuntamente, foi realizado um experimento para análise do impacto de um ataque com malwares reais e constatado falhas na comunicação ModBus entre cliente e servidor, resultando em 2,6 vezes mais valores atípicos entre pacotes de requisição e resposta nas amostras de malwares comparadas a amostras de aplicações benignas. Por fim, foi aplicada uma técnica de mineração de textos com dados da memória volátil, submetidos a diversos algoritmos de classificação supervisionada para definição de amostras como malignas ou benignas, nos quais foi obtido como melhor resultado um F-1 de 96,9% com o classificador AdaBoost.', 
  resumo_en: 'Industrial Control Systems (ICS) are responsible for controlling critical infrastructures, which, by their importance, are often targets of cyber attacks motivated by political, military or financial interests. Supervisory Control and Data Acquisition Systems (SCADA) are among the main components of ICS, and constitute highly interconnected systems that employ common solutions to conventional computer systems. Malwares are among the top cyber threats to these systems. There are difficulties as well as impossibilities in testing the cyber resilience of a real ICS, and testing platforms are required to verify the harmful behavior of these threats without endangering the original system. Cumulatively, as conventional detection tools, particularly signature-based ones, become increasingly inefficient due to evasive evolving techniques, the need for research and development of automated and dynamic malware detection tools are growing. This work presents a testbed named MalDomE, developed for the analysis and detection of malwares in industrial systems with impact checking, using machine learning techniques in textual data. The implemented system was obfuscated to avoid anti-analysis techniques and compared with other non-adapted systems, proving to be 60 % more efficient in hiding attributes that denounce it as an analysis system. An experiment was conducted to analyze the impact of an actual malware attack and was found ModBus client-server communication failures, resulting in 2.6 times more atypical values between request and response packets in the malware compared to benig samples. Finally, a text mining technique using volatile memory data was applied and submitted to several supervised classification algorithms in order to define samples as malignant or benign, in wich the best F-Score obtained was 96.9% using AdaBoost classifier.'},

 {numero: 744, ano: 2019, dia: '18/10', autor: 'Vivian Gabriela Santos Monteiro', orientador: [8], linha: 3, arquivo: '2019-Gabriela.pdf', 
  titulo: 'ODIN: Dimensões Ontológicas para Apoio à Análise de um Corpus', 
  resumo_pt: 'A Web trouxe para os dias atuais a disponibilidade de informação de forma simples e rápida, criando um valioso repositório de dados, com grande potencial para abastecer Sistemas de Apoio a Decisão (SAD). A dificuldade no tratamento de dados não estruturados e sua utilização em SAD, é um grande desafio, e para superá-lo, pesquisas científicas têm surgido com o objetivo de construir soluções para este problema. Nesse cenário, encontramos abordagens para a construção de SADs que aplicam técnicas para a extração e tratamento de dados não estruturados, como Processamento Linguagem de Natural, Mineração de texto e Anotação semântica. A anotação semântica é uma técnica que aplica significado aos dados contidos nos documentos, tornando possível a leitura computacional desse conteúdo. Seu uso combinado com Ontologias, permite classificar os textos de acordo com sua semântica e criar uma camada de metadados, possibilitando o enriquecimento de informações sobre o conteúdo textual. As abordagens desenvolvidas para análises textuais, são em geral, muito trabalhosas e exigem muito tempo e esforço na construção dos SADs, devido a várias etapas que precisam ser executadas na fase de análise da necessidade e na construção do Data Mart (Sistemas de Apoio à Decisão para um determinado assunto). Nesse sentido, este trabalho propõe o método ODIN, que tem como objetivo apoiar à modelagem dimensional para SADs, utilizando a abordagem de anotação semântica. A ideia é identificar e popular as perspectivas de análises para o Data Mart, de forma sistematizada, baseada em Ontologias, focando na sua relevância no Corpus analisado. Os resultados da aplicação desse método sobre os estudos de caso, confirmam a eficiência no apoio à construção de SADs e do uso de Ontologias na modelagem dimensional. A sistematização trouxe redução do esforço do Analista nas etapas de análise e identificação das dimensões na construção de um Data Mart para apoio à decisão. A implementação do método ODIN, chamada ODINI, permitiu a sua aplicação em dois estudos de caso, cujos resultados confirmam a eficácia do método desenvolvido. Foram gerados com estes estudos dois Data Marts: TOR DM, um Data Mart para a Priorização de Alvos de Fármacos para o combate de doenças negligenciadas, e Rol DM, um Data Mart para auxílio em pesquisas de medicamentos, procedimentos e eventos em saúde.', 
  resumo_en: 'The World Wide Web, nowadays, has made available a vast array of information that can be accessed in a simple and fast way, creating an invaluable data repository, with a great potential to supply Decision Support systems (DSS). The difficulty in dealing with unstructured data and their use in DSS represents a great challenge, and in order to cope, scientific research has arisen in an attempt to devise solutions for this issue. In this scenario, an approach useful in the construction of DSSs, which apply techniques for the extraction and treatment of unstructured data such as Natural Language Processing, Text mining and Semantic annotation. Semantic annotation is a technique which adds meaning to the data containing in the documents, making the computational reading of the content possible. Its use, combined with Ontologies, enables the classification of the texts according to its semantics and also allows the creation of a layer of metadata, thus permitting the enrichment of the information about the textual content. The approaches devised for textual analysis are in general complex and the implementation of the DSSs take a lot of time and effort, due to the various steps that have to be followed for the construction of the Data Mart (Decision Support Systems for a specific subject). In this direction, the current work proposes the ODIN method, whose aim is to support the Dimensional Modelling for DSSs by using the Semantic Annotation approach. The idea is to identify and populate the Dimensions for the Data Mart in a systematised way based upon Ontologies focusing on the relevance to the Corpus analysed. The results obtained from the application of this method in the case studies confirm the efficiency of the use of semantic instruments in the support and construction of DSSs and of the use of Ontologies in Dimensional Modelling. Systematisation resulted in a reduction of the Analyst effort in the analysis and identification of dimensions steps in the construction of a Data Mart for decision support. The implementation of the ODIN method, was called ODINI, was applied in two scenarios, confirming the effectiveness of the developed method. Two Data Marts were created: TOR DM, a Data Mart for prioritization of drug targets for treatment of neglected diseases and Rol DM, a Data Mart devised for support to research into pharmaceutical drugs, procedure and health events.'},

 {numero: 743, ano: 2019, dia: '08/08', autor: 'Luis Vinicius Pinho Bueno de Carvalho', orientador: [11], linha: 3, arquivo: '2019-Luis.pdf', 
  titulo: 'Uma Plataforma de Serviços para o Apoio à Transparência de Informações em Processos de Compras e-Gov', 
  resumo_pt: 'Ferramentas de Governo Eletrônico (E-Gov) são desenvolvidas com o objetivo de serem importantes instrumentos de prestação de contas aos cidadãos e de fomentar transparência sobre as informações manuseadas pelos governos. No que tange a compras governamentais essas ferramentas tem aumentado a divulgação das informações geradas pelo processo de contratação. Entretanto, essas ferramentas têm a limitação de apresentar o conceito de transparência com o foco exclusivo na ampla publicidade das informações, o que gera o espalhamento destas mesmas informações pelas suas diversas funcionalidades. Este trabalho propõe uma arquitetura que contribui para a reunir informações relacionadas a compras governamentais com o objetivo de atender uma ou mais necessidades de transparência do usuário, principalmente sob a ótica do entendimento e usabilidade das informações. A utilização desta arquitetura é demonstrada através de uma prova de conceito baseada no sistema Compras Governamentais. Através desta prova de conceito mostra-se o funcionamento da solução com o objetivo de se construir e apresentar uma informação de fácil uso e entendimento pelo usuário da aplicação.', 
  resumo_en: 'Electronic Government tools (E-Gov) are developed with the aim of being important instruments of accountability to citizens and of providing transparency on the information handled by governments. With regard to government procurement these tools have increased the disclosure of information generated by the hiring process. However, these tools have the limitation of presenting the concept of transparency with the exclusive focus on the ample publicity of the information, which generates the spread of this same information by its diverse functionalities. This work proposes an architecture that contributes to gathering information related to government purchases with the purpose of meeting one or more needs of user transparency, mainly from the point of view of the understanding and usability of information. The use of this architecture is demonstrated through a proof of concept based on the Government Procurement system. Through this proof of concept shows the operation of the solution in order to build and present user-friendly information and understanding of the application.'},

 {numero: 742, ano: 2019, dia: '26/04', autor: 'Gabriel Resende Machado', orientador: [13], linha: 3, arquivo: '2019-Gabriel.pdf', 
  titulo: 'MultiMagNet: Uma Abordagem Baseada na Formação Não Determinística de Comitês para Detecção de Imagens Contraditórias', 
  resumo_pt: 'Os algoritmos de Aprendizado Profundo, em especial as Redes Neurais Convolucionais, são o estado da arte na solução de diversas tarefas envolvendo classificação e reconhecimento de imagens. Entretanto, trabalhos recentes mostraram que esses algoritmos estão suscetíveis a ataques com imagens contraditórias. Imagens contraditórias são aquelas que possuem perturbações ínfimas, geradas maliciosamente por um algoritmo de ataque, com o objetivo de produzir erros de classificação. Essas imagens maliciosas ameaçam seriamente a adoção de modelos de aprendizado em tarefas críticas de segurança em Visão Computacional, uma vez que ataques conduzidos nesses ambientes de produção poderiam acarretar em acidentes e prejuízos de larga escala. Este cenário problemático incentivou a comunidade científica a propor inúmeros métodos para defesa de modelos de aprendizado contra imagens contraditórias. Contudo, os principais métodos de defesa existentes têm falhado principalmente por permitirem que o atacante compreenda seus modi operandi. Assim sendo, esta dissertação propõe o MultiMagNet, um método de defesa que, ao apresentar um comportamento não determinístico, dificulta evasões por ataques contraditórios. O não determinismo do método proposto decorre de uma escolha aleatória, realizada em tempo de execução, de um comitê de múltiplos componentes que analisam individualmente as imagens a fim de, em conjunto, identificar exemplares contraditórios. Os resultados de um estudo comparativo conduzido nos datasets MNIST e CIFAR-10 mostraram que o MultiMagNet foi capaz de superar o MagNet, um método de detecção de imagens contraditórias baseado em comportamento não determinístico, na defesa contra imagens contraditórias geradas pelos principais algoritmos de ataque.', 
  resumo_en: 'Deep Learning algorithms, especially the Convolutional Neural Networks, are the state-of-the-art for solving several tasks involving image classification and recognition. However, recent works have shown that Convolutional Neural Networks are susceptible to attacks with adversarial images. Adversarial images have tiny perturbations, maliciously generated using an attack algorithm, in order to lead pretrained models to misclassification. The adversarial images seriously menace the use of deep learning models in security-critical tasks involving Computer Vision, since successful attacks conducted in these environments can result in large-scale losses and accidents. This alarming scenario though, has encouraged the scientific community to propose various methods to defend learning models against adversarial images. Nevertheless, the main existing defense methods have failed, especially because they facilitate the attacker to comprehend their modi operandi. Therefore, this dissertation proposes MultiMagNet, a defense method that, due to its non-deterministic behaviour, hinders evasions by adversarial attacks. The non-deterministic effect of the proposed defense method stems from an ensemble randomly formed at runtime, containing multiple defense components, which individually analyze the input images in order to identify adversarial samples. The results obtained from a comparative study conducted on the datasets MNIST and CIFAR-10 have shown that MultiMagNet was able to surpass MagNet, a detection method based on non-deterministic behaviour, when defending against adversarial images generated by the main attack algorithms.'},

 {numero: 741, ano: 2019, dia: '04/02', autor: 'Thiago Paiva Pimentel', orientador: [13], linha: 3, arquivo: '2019-Thiago.pdf', 
  titulo: 'Predição de churn baseada em detecção de padrões sequenciais e análise de sentimentos sobre as interações de clientes no crm', 
  resumo_pt: 'Devido ao crescimento da concorrência, a maior parte dos mercados está cada vez mais saturada. Como consequência, as empresas vêm percebendo que as estratégias de negócio devem priorizar a manutenção de seus clientes. Neste cenário, ganham relevância as iniciativas de gerenciamento do relacionamento com o cliente (em inglês, CRM - Client Relationship Managemenent) dentro das organizações. Um dos principais desafios enfrentados pelo CRM na atualidade é a identificação de clientes com propensão ao churn (i.e., cancelamento) de produtos e/ou serviços. Diversas iniciativas de pesquisa têm buscado a criação de modelos preditivos que tentam detectar antecipadamente a ocorrência de churn. Entre elas, destacam-se as que se baseiam na detecção de padrões sequenciais. Embora promissoras, tais iniciativas deixam de considerar informações muitas vezes presentes nas conversas registradas ao longo do tempo entre clientes e empresas e que podem fornecer indícios importantes para a prevenção de churn: os sentimentos manifestados pelos clientes durante essas interações. Diante do exposto, o presente trabalho levanta a hipótese de que utilizar informações sobre possíveis sentimentos presentes nas interações entre cliente e empresa na ordem em que elas ocorrem ao longo do tempo pode levar à identificação de modelos mais precisos do que os do estado da arte na detecção preventiva de churn. Assim, este trabalho tem como objetivo apresentar evidências experimentais que confirmem a hipótese levantada. Para tanto, propõe-se um método que combina técnicas de mineração de padrões sequenciais e de análise de sentimento, aplicando-as sobre informações acerca das interações ocorridas longitudinalmente entre clientes e empresas. Concebido para ser aplicado em qualquer contexto onde existam dados históricos sobre os contatos realizados junto aos clientes, o método proposto produziu, nos experimentos realizados em bases de dados da área de educação e de telecomunicações, resultados que apontam para a validade da hipótese formulada. ', 
  resumo_en: 'As a result of increased competition, most markets are becoming more saturated. As a consequence, companies are realizing that business strategies should prioritize the maintenance of their customers. In this scenario, initiatives of customer relationship  management within organizations are gaining relevance. One of the main challenges faced by CRM is the identification of customers with a tendency to churn of products and / or services. Several research initiatives have sought to create predictive models that try to detect the occurrence of churn in advance. Among them, those that are based on the  detection of sequential patterns stand out. Although promising, such initiatives fail to address information that is often present in the conversations recorded over time between clients and companies and that can provide important clues to churn prevention: the sentiments expressed by customers during these interactions. Considering the above, the present work raises the hypothesis that using information about possible feelings present in the interactions between client and company in the order in which they occur over time can lead to the identification of models more precise than those of the state of the art in Preventive churn detection. Thus, this paper aims to present experimental evidence to confirm the hypothesis raised. For that, a method is proposed that combines techniques of mining of sequential patterns and sentiment of analysis, applying them to information about the interactions occurred longitudinally between clients and companies. Designed to be applied in any context where there is historical data about contacts made with customers, the method proposed produced results in the data base of education and telecommunications, which point to the validity of the hypothesis formulated.'},

 {numero: 740, ano: 2019, dia: '01/02', autor: 'Marco Aurélio da Silva Cruz', orientador: [13], linha: 3, arquivo: '2019-Marco.pdf', 
  titulo: 'Aprendizagem Profunda Aplicada ao Reconhecimento de Usuários Baseado na Dinâmica da Digitação', 
  resumo_pt: 'Diversos estudos têm investigado como realizar o reconhecimento de usuários baseado na dinâmica da digitação por meio de algoritmos de Aprendizado de Máquina. No entanto, todos esses estudos demandam Engenharia de Atributos, um processo em que especialistas definem que atributos devem ser considerados no aprendizado. Tal processo está sujeito a falhas como seleção de atributos inadequados ou perda de informações originais relevantes. Diante disso, o presente trabalho tem por objetivo demonstrar a hipótese de que a Engenharia de Atributos pode ser dispensada ao utilizar um algoritmo devidamente projetado para operar diretamente sobre os dados brutos da dinâmica da digitação. Para tanto, o presente trabalho propôs uma rede neural profunda, chamada DRK, que foi projetada com múltiplas camadas que aprendem a representação de dados necessária para reconhecer usuários a partir dos dados brutos da dinâmica da digitação, dispensando, assim, a Engenharia de Atributos. Experimentos em 4 conjuntos de dados envolvendo 280 usuários compararam o desempenho da DRK com outras 4 redes profundas de trabalhos relacionados que demandam Engenharia de Atributos. A rede proposta superou as demais em todos os conjuntos avaliados, fornecendo evidências que apontam para a validade da hipótese levantada.  ', 
  resumo_en: 'Several studies have investigated how to use Machine Learning algorithms to recognize users based on keystroke dynamic. All those studies required Feature Engineering, i.e., a process in which specialists choose what attributes should be considered for learning. However, this process is susceptible to problems such as original information loss or inappropriate attribute choices. Thus, the objective of this work is to demonstrate the hypothesis that the Feature Engineering can be avoided by using an algorithm built specifically to learn from raw data. Therefore, this work proposes a deep neural network named DRK. The proposed network contains layers that learn adequate data representations to perform user recognition based on keystroke dynamics raw data, avoiding Feature Engineering. Experiments compared DRK with four other deep neural networks that use Feature Engineering in four datasets with 280 users. The proposed network achieved better results in all datasets, showing strong evidence that the stated hypothesis is, in fact, valid.'},

 {numero: 739, ano: 2018, dia: '05/12', autor: 'Camila Mesquita de Moraes', orientador: [13], linha: 3, arquivo: '2018-Camila.pdf', 
  titulo: 'Predição de ligação utilizando detecção de comunidades baseada em informação de contexto', 
  resumo_pt: 'Predição de Ligação (LP) é a tarefa de prever quais participantes em uma rede irão interagir no futuro, dadas as interações existentes entre esses participantes. Uma abordagem comum para resolver essa tarefa usa métricas para computar graus de compatibilidade entre pares de vértices não conectados na rede. Nesta abordagem, o modelo preditivo é gerado usando métricas de similaridade aplicadas da mesma forma para todos os pares de vértices desconectados, independente das posições ocupadas por eles na estrutura topológica da rede. Trabalhos mais recentes nessa área têm aplicado uma abordagem diferente: detectar comunidades na rede original e aplicar LP utilizando a informação de cada comunidade. No entanto, há uma limitação importante em relação a esses trabalhos: a detecção de comunidades utilizadas por eles considera apenas aspectos topológicos da rede. E não consideram, no agrupamento de vértices, características relacionadas ao contexto da aplicação, como perfis, interesses e preferências dos participantes, que podem ser fundamentais tanto para a identificação de comunidades de conteúdo mais coesas quanto para uma maior assertividade nas conexões de predição. Esta dissertação de mestrado propõe um método para LP que usa um processo de detecção de comunidade baseado em informações de contexto. A fase de predição de ligação é precedida pela detecção de comunidades que, por sua vez, leva em consideração características dos participantes da rede, a fim de separá-las em grupos cujo conteúdo é coeso em cada grupo. Experimentos computacionais demonstram a efetividade do método adotado, uma vez que superou outras abordagens existentes na literatura. ', 
  resumo_en: 'Link Prediction (LP) is the task of predicting which participants in a network will interact in the future, given the existing interactions between these participants. A common approach to solve this task uses metrics to compute degrees of compatibility between vertex pairs not connected in the network. In this approach, the predictive model is generated using some similarity metrics applied in the same way for all pairs of unconnected vertices, independent of the positions occupied by them in the network topological structure. More recent work on this area has been appllying a different approach: detecting communities in the original network and applying LP on each community. Nevertheless, there is an important limitation in relation to these works: the detection of communities used by them only considers topological aspects of the network. They fail to consider, at the of vertex grouping, characteristics related to the application context, such as participant profiles, interests and preferences, which may be fundamental both for the identification of more cohesive content communities and for a greater assertiveness in the prediction connections. This paper proposes a method for LP that uses a community detection process based on context information. The link prediction phase is preceded by the detection of communities which, in turn, takes into account characteristics of the network participants in order to separate them into groups whose content is cohesive in each group. Computational experiments demonstrate the effectiveness of the proposed method, since it overcame other approaches in the literature. '},

 {numero: 738, ano: 2018, dia: '21/08', autor: 'Alexsandro de Paula Miranda', orientador: [4], linha: 2, arquivo: '2018-Alexsandro.pdf', 
  titulo: 'Renderização de cena com ponto de observação livre', 
  resumo_pt: 'A área de visão computacional permite criar inúmeras aplicações no campo de pesquisa de Realidade Virtual e processamento de imagens, desde simulações de treinamentos à reconhecimento de objetos. A multivisão é um outro exemplo de pesquisa que vem sido explorada nos últimos anos. Este trabalho se propõe a investigar a criação de múltiplas imagens sintéticas a partir de imagens de entrada cuja posição em um plano tridimensional é árbitrária, e o ponto focal da câmera possui uma pequena distinção de perspectiva entre as imagens, criando um modelo de visão projetado de forma ilusória ao usuário, dando-lhe a sensação de que diversas imagens foram obtidas através de múltiplas capturas através de uma câmera real, quando na verdade as imagens são geradas sinteticamente. O sistema apresentado por este trabalho utiliza a linguagem de programação Python e a biblioteca de visão computacional OpenCV para gerar as imagens sintéticas. Foram efetuados experimentos utilizando imagens com geometria conhecida, e perspectiva arbitrária para gerar novas imagens. A dissertação descreve a utilização de várias técnicas consolidadas na área de visão computacional, tais como: retificação de imagens, triangulação de delauney, morphing, entre outras. A partir dos experimentos realizados, as imagens de saídas possuem diversas perspectivas inexistentes na cena de entrada, pois foram geradas através de diversas distorções nas malhas após serem trianguladas.', 
  resumo_en: 'The area of computer vision allows to create applications in the field of research of Virtual Reality and image processing, from training objects recognition simulations. Multivision is another example of research that has been explored in recent years. This work proposes to investigate the creation of multiple synthetic images from input images whose position in a three-dimensional plane is arbitrary, and the focal point of the camera has a small distinction of perspective between the images, creating a model of vision projected from illusory way to the user, giving him the sensation that several images were obtained through multiple captures through a real camera, when in fact the images are generated synthetically. The system presented by this work uses the programming language Python and the computational view library OpenCV to generate the synthetic images. Experiments were performed using images with known geometry, and an arbitrary perspective to generate new images. The dissertation describes the use of several techniques consolidated in the area of computational vision, such as: rectification of images, triangulation of delauney, morphing, among others. From the experiments performed, the output images have several perspectives that did not exist in the input scene because they were generated through several distortions in the meshes after being triangulated. '},

 {numero: 737, ano: 2018, dia: '20/08', autor: 'Dhiego Ramos Pinto', orientador: [6], linha: 3, arquivo: '2018-Dhiego.pdf', 
  titulo: 'Aprendizado Profundo Aplicado à Análise Estática de Malwares', 
  resumo_pt: 'Descobrir se um programa suspeito possui código malicioso e categorizá-lo em famílias vem se tornando uma tarefa cada vez mais complexa. Com o crescimento no número de novas gerações de malware, a exploração de vulnerabilidades, sejam de hardware ou software, tem sido cada vez maior, gerando prejuízos incontáveis anualmente. Diversas técnicas de aprendizado de máquina tem sido empregadas no contexto da análise de malware visando automatizar parte do trabalho do analista, comumente utilizando-se de características pertencentes ao domínio do problema. O Aprendizado Profundo, uma subárea do Aprendizado de Máquina, despontou na última década como sendo um método de aprendizado de representações, onde hierarquias de representações são criadas nas camadas internas de uma rede neural, necessitando de pouco ou nenhuma engenharia de características para que problemas complexos pudessem ser resolvidos com uma boa performance, e até mesmo, redefinindo o estado da arte em algumas áreas. Portando, o presente trabalho expõe e experimenta uma abordagem ao problema da classificação de malware, utilizando redes profundas com pré-treinamento não supervisionado, aprendendo uma hierarquia de representações, para posteriormente servir como base para classificadores profundos, sem o auxílio de qualquer característica advinda de conhecimento sobre as famílias de malware. Os resultados analisados confirmam que a abordagem é bem sucedida nesta tarefa.', 
  resumo_en: 'Deciding if a suspect program has malicious code and categorize it among malware families has become an increasingly complex task. With the growth in the numbers of new malware generations, the exploitation of vulnerabilities, hardware or software based, creates uncountable losses yearly. Several machine learning techniques have been employed in the context of malware analysis, seeking to automate part of an analyst’s job, commonly using features belonging to the problem domain. Deep learning, a subarea of machine learning, emerged in the last decade as a method of representation learning, where representational hierarchies are created in the hidden layers of a neural network, needing little or nothing of engineering feature to achieve a good performance and even more, redefining the state of the art in some areas. Therefore, the present work exposes and experiments an approach for malware classification, using deep networks with unsupervised pre-training, learning a hierarchy of representations, lately serving as a basis for deep classifiers, without the help of any features that come from the knowledge about the malware’s families. The analyzed results confirmed that the approach is well succeeded at this task.'},

 {numero: 736, ano: 2018, dia: '20/08', autor: 'Paulo Alvez Braz', orientador: [13], linha: 3, arquivo: '2018-Paulo.pdf', 
  titulo: 'Investigando a Utilização de Informações Textuais na Detecção de Bots Sociais', 
  resumo_pt: 'Atualmente, as redes sociais estão sujeitas a ações de bots sociais que executam atividades maliciosas como a disseminação de notícias falsas. Algumas pesquisas voltadas à detecção desse tipo de malware se baseiam em estatísticas extraídas a partir do conteúdo das mensagens postadas. Como a extração de estatísticas pode ocasionar perda de informação, este trabalho tem como objetivo apresentar evidências experimentais de que o uso de textos originais das mensagens pode melhorar a precisão de detecção. Para tanto, propõe-se um método que aplica uma rede neural convolucional para identificar mensagens suspeitas. Resultados utilizando dados do Twitter fornecem indícios de adequação do método proposto, de forma a aprimorar a acurácia dos classificadores de contas automatizadas.', 
  resumo_en: 'Social bots are responsible for malicious activities in social networks. State-of-the-art on social bot detection combines account data with textual content statistics extracted from messages posted in those environments. Nevertheless, statistical consolidation may lead to information loss. In such scenario, this work searches for experimental evidence that supports the hypothesis that the usage of original textual content from messages may improve detection accuracy. To help our search, we developed an approach that applies convolutional neural networks to identify suspicious messages based on their original textual content. Those network models are trained on samples selected at random. Experiments with our method on Twitter data confirm our hypothesis. '},

 {numero: 735, ano: 2018, dia: '15/08', autor: 'Thiago Bastos Leão', orientador: [8], linha: 3, arquivo: '2018-Thiago.pdf', 
  titulo: 'Heurística para modelagem de dados no Cassandra', 
  resumo_pt: 'Com o aumento da adesão aos SGBDs NOSQL para atender as necessidades de escalabilidade e distribuição de dados, diferentes tipos de SGBDs NOSQL começaram a surgir e a serem usados com diferentes propósitos e modelos, como os orientados a documentos, orientados a colunas, chave-valor, orientados a grafos, entre outros. Neste trabalho, foi feito um estudo sobre o Cassandra, um SGBD orientado a colunas. Apesar de existirem estudos sobre o desempenho de outros SGBDs orientados a colunas, não podemos assumir que as premissas de modelagem lógico/física são as mesmas para todos. Este trabalho propõe uma heurística de modelagem lógica para o Cassandra que se baseia nas restrições de chaves do Cassandra e utiliza visões materializadas. Esta heurística parte de um conjunto pré-definido de consultas e utiliza os seus filtros como base para definir como os atributos irão compor a chave primária de uma visão materializada. Um experimento usando o benchmark CNSSB, com 1 milhão de registros foi realizado, mostrando que a modelagem gerada pela heurística proposta tem bons resultados em termos de desempenho, podendo ser aplicada em outros contextos, auxiliando os desenvolvedores que têm o objetivo de usar o Cassandra como SGBD. ', 
  resumo_en: 'Due to the increase of NOSQL adoption to supply the needs of data scalability and distribution, different classifications of NOSQL DBMS have emerged and used for different purposes. We have classifications types of databases such as document oriented column oriented, key-value and many other types of NOSQL DBMS. We propose to study Cassandra, a column-oriented DBMS. Althought there are other studies about others column-oriented DBMS, we cannot assume that logical/physical modeling premises are the same for every DBMS. This study proposes a logical modeling heuristic for Cassandra that is based on Cassandra’s primary key restrictions and uses materialized visions. This heuristic has as input a set predefined queries and uses its filters to define the primary key attribute composition of a materialized view. An experiment using the CNSSB benchmark, with 1 milion registers, showing that the modeling generated by the proposed heuristic has good results in terms of performance, being able to be applied on many contexts, helping developers that has the objective to use Cassandra as a DBMS.'},

 {numero: 734, ano: 2018, dia: '21/05', autor: 'Dulcinéia Santos Sennejunker', orientador: [2], linha: 2, arquivo: '2018-Dulcineia.pdf', 
  titulo: 'T-SLOW: Algoritmo de Detecção de Ataques Slow DoS', 
  resumo_pt: 'Recentemente, como consequência dos esforços de mitigação aos ataques de negação de serviço tradicionais, os ataques Slow DoS surgem como ameaça à garantia da disponibilização de serviços na Web. Esses ataques são considerados preocupantes devido a furtividade do seu modo de operação, pois podem desabilitar a vítima lentamente e sem alarde. E, diferenciam-se dos ataques DDoS tradicionais, pois não causam mudanças abruptas no comportamento do tráfego de rede, como as que evidenciam um ataque volumétrico, tais como um aumento significativo de endereços IP em um dado instante no servidor alvo. A sofisticação dessa categoria de ataque motivou o presente trabalho, com a proposição de um algoritmo para a detecção de ataques Slow DoS à camada de aplicação, com foco na pronta detecção destes e com altas taxas de desempenho. Durante a pesquisa, foram realizados estudos observacionais e experimentos para melhor avaliação do funcionamento das métricas propostas, e que compõem parte da estrutura do algoritmo principal chamado T-Slow. As métricas desenvolvidas nesse trabalho não destinam-se ao fornecimento de thresholds para a detecção, como por exemplo, a limitação de conexões que um servidor pode manipular ou uma limitação de várias conexões simultâneas que um cliente pode ter, mas sim à verificação do verdadeiro status dos atributos de rede em um dado momento. Essas métricas procurarão detectar características ou padrões bem definidos nos traces maliciosos dos ataques lentos à camada de aplicação, mediante o aumento súbito no número de conexões, cabeçalhos das requisições incompletos ou leitura extremamente lenta das respostas da vítima. Dessa forma, as três funções do algoritmo T-Slow, cujos nomes referem-se às próprias métricas, chamadas Flags, Omnia e RTTR (F.O.R.), atuarão de forma paralela, porque cada uma das funções investiga uma característica do trace malicioso Slow DoS, que poderá evidenciar distorções nos valores históricos dos atributos do tráfego de rede. Dois estudos observacionais foram realizados. O primeiro, para determinação do tamanho de janela adequado para a detecção e o segundo, para determinar o melhor tamanho de amostra para aplicação das métricas F.O.R. Com relação ao tamanho da janela, foi utilizada a lógica fuzzy para alteração dinâmica do intervalo de tempo analisado. Os resultados desse experimento foram observados e comparados aos resultados obtidos com as janelas de tempo estáticas. Com relação à escolha do melhor tamanho de amostra, para aplicação do previsor de anomalias aos resultados das métricas, foi utilizada a técnica do menor erro quadrático médio por ser uma técnica adequada às previsões de curto prazo. Assim, cada métrica poderá utilizar o tamanho de amostra mais adequado ao seu potencial. Os resultados desse experimento também foram observados e comparados aos resultados obtidos com as amostras, cujo tamanho não era variável, ou seja, comum à todas às métricas. O principal diferencial deste trabalho, em relação a outros trabalhos sobre detecção de ataques Slow DoS, é a utilização da lógica fuzzy para a resolução do problema da estimação adequada das janelas de tempo, para a análise do tráfego de rede. Outra observação importante é que, a literatura sobre algoritmos de aprendizado sugere que, estes sofrem com o problema de overfitting e o underfitting de dados durante o processo de treinamento. O T-Slow não sofre com tais problemas, porque o algoritmo faz uso do histórico de dados mais recentes da própria rede, com intervalos de dados definidos pelo menor erro quadrático médio. O núcleo do mecanismo de detecção utiliza a desigualdade Tchebycheff. Dado que não se conhece a distribuição de probabilidades das variáveis de tráfego envolvidas nesse trabalho, emprega-se esta desigualdade, pois ela se aplica a qualquer distribuição de dados. Com ela, foi possível alcançar baixa taxa de erro na classe positiva e, dentro do escopo do trabalho, alta acurácia em detecção. O algoritmo foi aplicado a um dataset de tráfego de rede real, que mescla tráfego de fundo ao tráfego de ataques à camada de aplicação, com os ataques Slow Headers, Slowloris, Slow Body, R.U.D.Y. e Slow Read.', 
  resumo_en: 'Recently, as a result of mitigation efforts of traditional denial of service attacks, Slow DoS attacks appeared as a threat to the availability of web services. These attacks are considered to be worrisome due to their stealthily approach, disabling the victim slowly and without shew. These attacks are different from traditional DDoS attacks, as they do not cause abrupt changes in the behavior of network traffic such as volumetric attacks, like the significant increase of active IP addresses in a given instant. The sophistication of this category of attacks motivated the present work, with the proposition of an algorithm for detecting Slow DoS attacks on the application layer, focused on prompt detection and high performance. During the research, observational studies and experiments were carried out to evaluate the performance of the proposed metrics that comprise part of the main algorithm’s structure, called T-Slow. The metrics developed in this work do not intend to provide thresholds for the detection, for example, a limit of connections that a server can handle or a limitation of multiple simultaneous connections that a client may have at the same time, but rather verify the true status of the attributes of the network at any given time. The metrics will seek to detect characteristics and well-defined patterns of the malicious traces of the slow attacks on the application layer, like the sudden increase in the number of connections, headers of incomplete requisitions or extremely slow reading of the victim’s responses. In this way, the algorithm T-Slow counts on three functions, whose names refer to the metrics themselves, called Flags, Omnia and RTTR (F.O.R.). Since each of the functions investigates a specific characteristic of traces of Slow DoS attacks that may show distortions in the network traffic attributes, they work in parallel. Two important observational studies were carried out: The first one, to determine the appropriate analysis window size for the detection and the second, to determine the best sample size for application of the F.O.R. metrics. With respect to the determination of the appropriate analysis window size, Fuzzy Logic was applied to dynamically determine the time interval of analysis. The results of this experiment were compared to the results obtained with the static time windows. Regarding the choice of the best sample size, for application of the anomaly detection using the mentioned metrics, the method of the smallest average quadratic error was used, being suitable for short-term forecasts. This way, each metric uses the most suitable sample size. The results of this experiment were also compared to the results obtained with the samples whose size was not variable but common for all metrics. The main difference of this work with respect to other works on the detection of Slow DoS attacks is the use of fuzzy logic to solve the problem of adequate estimation of the time windows for the analysis of network traffic. Another important observation is that literature suggests that machine learning algorithms may suffer from data overfitting or underfitting during the training phase. The T-Slow algorithm does not face these issues since it uses the most recent data of the network, with data ranges defined by the smallest mean square error. The core of the detection mechanism uses the Tchebycheff inequality. The distribution of the traffic variables used in this work is not known, however the inequality of Tchebycheff can be applied to any distribution. This way it was possible to achieve low rates of error in the positive class and high rates if detection accuracy, within the scope of the work. The algorithm was applied to a dataset of real network traffic that contains background traffic merged with attack traffic to the application layer, in specific the attacks Slow Headers, Slowloris, Slow Body, R.U.D.Y. e Slow Read.'},

 {numero: 733, ano: 2018, dia: '17/05', autor: 'Marcelo Eduardo Machado Cota', orientador: [11], linha: 2, arquivo: '2018-Marcelo.pdf', 
  titulo: 'Avaliação Quantitativa da Deterioração Arquitetural na Evolução do Software: Uma Aboardagem Considerando Componentes Não Conectados Estruturalmente', 
  resumo_pt: 'Mudanças tendem a degradar a estrutura do software, causando deterioração arquitetural. Partes do software podem apresentar dependências ocultas, denominadas de acoplamento evolutivo, que violam seus princípios arquiteturais projetados. Revelar essas dependências ocultas é importante porque favorece a avaliação de qualidade do código e indica a necessidade de refatoração antes que o pagamento da dívida arquitetural seja impraticável. O principal objetivo deste estudo é definir uma métrica direta para medir a deterioração arquitetural de um software, usando informações sobre o acoplamento evolutivo. A ideia é determinar quanto cada elemento arquitetural avaliado está logicamente acoplado a outros elementos e comparar os resultados com instâncias reais de acoplamento evolutivo identificadas pela nova abordagem da verificação deslizante. Técnicas têm sido propostas para avaliar a dívida arquitetural usando dependências lógicas, tal como o History Coupling Probability (HCP). No entanto, detectar o acoplamento evolutivo é uma tarefa difícil, pois requer a confirmação se, de fato, as mudanças conjuntas representam uma dependência lógica adquirida. Uma solução para esse problema é usar dados obtidos de um sistema de controle de versão no qual todas as alterações de um componente podem ser visualizadas no nível do sistema. Assim, este trabalho propõe uma variação do método HCP, integrando informações obtidas do histórico de versões com informações gerenciais sobre tarefas para melhor identificar e qualificar as mudanças conjuntas. Depois de classificar o acoplamento evolutivo, é possível medir seu impacto usando a métrica proposta. A metodologia foi aplicada a um grande cenário de evolução de um Sistema Militar de Comando e Controle (SMC2), desenvolvido pela Marinha do Brasil (MB). Os resultados mostram que a nova métrica indica o nível de decaimento da arquitetura do software e que sua medida é sensível à refatoração, indicando variação de qualidade no momento de intervenções nos elementos arquiteturais envolvidos na evolução do software. Assim, os resultados indicam que o IAEv é uma métrica promissora para apontar áreas na arquitetura de software que tiveram impacto devido ao acoplamento evolutivo, ajudando equipes de desenvolvedores, arquitetos ou gerentes de projeto a procurar possíveis demandas de melhorias no software e implementá-las imediatamente.', 
  resumo_en: 'Changes to the software tend to degrade its structure, causing architectural decay. Parts of the software may present hidden dependencies, called evolutionary coupling, that violate its designed architectural principles. Unveiling these hidden dependencies is important because it may support the quality verification and indicate the need for refactoring before architectural debt repayment is impractical. The main goal of this study is to define a direct metric to measure the architectural decay of a software, using information about evolutionary coupling. The idea is to determine how much each evaluated architectural element is logically coupled with other elements and to compare the results with real instances of evolutionary coupling identified by the new sliding verification approach. Techniques have been proposed to evaluate the architectural debt using logical dependencies, such as History Coupling Probability (HCP). However, detecting evolutionary coupling is a hard task since it is difficult to confirm if, indeed, co-changes are logical dependency acquired. A workaround to this problem is to use data obtained  from a version control system in which all changes of a component can be viewed on the system level. Thus, this work proposes a variation to the HCP method, integrating information obtained from the version history with managerial information about tasks to better identify and qualify the cochanges. After classifying evolutionary coupling, it is possible to measure its impact, using the proposed metric. The metodology was applied to a large Military Command and Control System evolution scenario, developed by the Brazilian Navy. The results shown that the new metric indicates the software architectural decay level and that its measurement is sensitive to refactoring, indicating quality variation at the moment of intervention in the architectural elements involved in software evolution. Thus, the results indicate that IAEv is a promising metric to point out areas in the software architecture that have i pacted by evolutionary coupling, helping teams of the developers, architects or project managers look for possible demands for improvements to the software and implement them immediately.'},

 {numero: 732, ano: 2018, dia: '11/05', autor: 'Fábio Costa de Souza', orientador: [9], linha: 3, arquivo: '2018-Fabio.pdf', 
  titulo: 'Localização em Redes de Sensores sem fio por Método de Multilateração Sequencial e Otimização Não Linear Utilizando Veículo Aéreo Não Tripulado', 
  resumo_pt: 'O conhecimento da localização geográfica dos nós sensores nas Redes de Sensores Sem Fio (RSSF) é uma característica essencial, entretanto diversos estudos têm sido realizados com o objetivo de obter uma estimativa de melhor localização Access Points (MÜLLER, 2014; PRATA, 2018; SIQUEIRA, 2017; VILLAS et al., 2013). A questão de localização é, essencialmente, um problema de estimação da posição com base em observações das grandezas físicas emitidas para redes sem fio. Diversas técnicas como trilateração, multilateração, triangulação, abordagens probabilísticas e técnicas de otimização matemática são utilizadas para prover uma solução. Este trabalho aborda o problema de localização em RSSFs apresentando uma análise comparativa de algoritmos baseados no método Quasi-Newton para problemas de otimização não lineares, utilizando a técnica de multilateração sequencial, com o objetivo de melhorar a precisão da localização de Access Point em ambientes abertos (outdoors) para redes Wi-Fi nas faixas de 2,4 GHz e 5 GHz. Os algoritmos Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) e Sequential Least Squares Quadratic Programming (SLSQP) foram escolhidos por apresentarem baixo custo computacional, sendo adequados para sistemas embarcados, que, geralmente, contam com uma quantidade reduzida de recursos, como memória e poder de processamento. Devido à versatilidade e mobilidade para embarcar equipamentos para leitura de Received Signal Strength Indicator (RSSI) das RSSFs, foi construído um Sistema de Aeronaves Remotamente Pilotadas (SARP) do tipo asas rotativas (quadricóptero) e os algoritmos L-BFGS e SLSQP foram embarcados na aeronave. Foram conduzidos experimentos práticos em voo real e os resultados mostram que a abordagem utilizando algoritmo SLSQP produz um erro menor na estimativa da posição geográfica em comparação ao L-BFGS.', 
  resumo_en: 'The knowledge of the geographic location of the sensor nodes in the Wireless Sensor Network (WSN) is an essential feature, however, several studies have been carried out with the objective of obtaining a better location estimation. The localization problem is essentially a location estimation problem based on observations of the physical magnitudes emitted for wireless networks. Several techniques such as trilateration, multilateralization, triangulation, probabilistic approaches and mathematical optimization techniques are used to provide a solution to the area. This work addresses the problem of location in WSNs presenting a comparative analysis of algorithms based on the Quasi-Newton method for optimization problems using the sequential multilateration technique, to better estimate the location of the Access Point. The Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) and Sequential Least Squares Quadratic Programming (SLSQP) algorithms were chosen due to their low computational cost, being suitable for systems since they usually have a reduced amount of resources such as memory and processing power. Due to the versatility and mobility to loading equipment’s for reading the Received Signal Strength Indicator (RSSI), was build an Unmanned Aerial Vehicle (UAV) and the L-BFGS and SLSQP algorithms were loaded into the aircraft on-board computer. The experiments were conducted in real flight and the results showed that the approach using SLSQP algorithm produces a smaller error in the estimation of the geographical position in comparison to the L-BFGS.'},

 {numero: 731, ano: 2018, dia: '03/05', autor: 'Vanessa Quadros Gondim Leite', orientador: [12], linha: 2, arquivo: '2018-Vanessa.pdf', 
  titulo: 'Geração de Dataset a partir da Criação de uma Social Botnet', 
  resumo_pt: 'O Facebook é uma das maiores mídias sociais e está cada vez mais ameaçado por socialbots, que são perfis controlados por software capazes de realizar ações malignas ou benignas. Isso envidência a necessidade de criação de contramedidas para os cenários em que o uso dos socialbots e do Facebook seja direcionado para fins maliciosos. Entretanto, a detecção de comportamentos anômalos para o desenvolvimento de contramedidas é um desafio, principalmente por causa da ausência de dados descritos, completos e disponíveis. Com base nisto, o objetivo desta pesquisa é criar uma arquitetura conceitual que contemple desde a etapa de criação de uma social botnet até a geração dos dados. Além disso, através da implementação desta arquitetura, atinge-se o outra finalidade deste estudo, que é gerar um dataset com ações dos socialbots e interações entre eles e humanos no Facebook.', 
  resumo_en: 'Facebook is one of the largest social media and is increasingly threatened by socialbots, which are software-controlled profiles capable of performing malignant or benign actions. This envisages the need to create countermeasures for scenarios where use of socialbots and Facebook is targeted for malicious purposes. However, the detection of anomalous behaviors for the development of countermeasures is a challenge, mainly because of the lack of data described, complete and available. Based on this, the objective of this research is to create a conceptual architecture that contemplates from the stage of creation of a social botnet until the generation of the data. In addition, through the implementation of this architecture, we achieve the other purpose of this study, which is to generate a dataset with socialbots actions and interactions between them and humans on Facebook.'},

 {numero: 730, ano: 2018, dia: '03/05', autor: 'Mário Luiz Ferreira Junior', orientador: [12], linha: 2, arquivo: '2018-Mario.pdf', 
  titulo: 'Metodologia para Execução de Engenharia Social Automatizada', 
  resumo_pt: 'Redes sociais têm sido cenário de sucesso entre os internautas em todo o mundo, ampliando diariamente a quantidade de usuários. Contudo, estes usuários estão sujeitos às vulnerabilidades que existem nessas redes sociais. Este trabalho tem como objetivo demonstrar que os dados dos usuários de redes socais, especificamente, a rede social Facebook estão vulneráveis e suscetíveis a engenharia social. A validação do trabalho é confirmada por meio do desenvolvimento de um aplicativo, realizando engenharia social automatizada com a utilização da biblioteca de inteligência artificial WEKA e a API do Facebook, com intenção de presumir onde o usuário estará em determinado tempo e local. No presente texto foram analisados 21 perfis de voluntários sendo 5 perfis exemplificados. Com os resultados obtidos, podemos compreender que boa parte dos usuários do Facebook desconhecem a falta de privacidade e a falha na segurança de  seus dados na rede social.', 
  resumo_en: 'Social networks have been a scene of success among internet users around the world, increasing the number of users daily. However, these users are subject to the vulnerabilities that exist in these social networks. This work aims to demonstrate that the data of users of social networks, specifically the social network Facebook are vulnerable and susceptible to social engineering. Validation of the work is confirmed through the development of an application, performing socially automated engineering using the WEKA artificial intelligence library and the Facebook API, with the intent of presuming where the user will be at a given time and place. In the present text 15 profiles of volunteers were analyzed, 5 profiles being exemplified. With the results obtained, we can understand that a lot of Facebook users are unaware of the lack of privacy and the lack of security of their data in the social network. '},

 {numero: 729, ano: 2018, dia: '02/05', autor: 'Guilherme Baesso Moreira', orientador: [6], linha: 3, arquivo: '2018-Baesso.pdf', 
  titulo: 'Uma Ontologia para Tratamento de Incidentes de Segurança da Informação', 
  resumo_pt: 'A rápida evolução da tecnologia da informação nas últimas décadas levou a sociedade a um processo de crescente dependência nos sistemas computacionais e serviços baseados na Internet. Este cenário dinâmico e complexo implica em iniciativas cada vez mais desafiadoras na Defesa Cibernética e, embora a indústria empregue inúmeros esforços para garantir a Segurança da Informação, observa-se uma escalada na gravidade, volume e frequência de incidentes. Com ataques motivados por ganhos financeiros e políticos, utilizando cada vez mais recursos e, muitas vezes, financiados por Estados, a ocorrência de um incidente é praticamente inevitável. É preciso preparar-se para responder de forma rápida e eficiente aos ciberincidentes, porém as abordagens de segurança tradicionais estão muito mais voltadas para a prevenção do que para a resposta a incidentes. O objetivo deste trabalho é apresentar um novo modelo de tratamento de incidentes, descrito como uma ontologia, que seja facilmente extensível e integrável com outros modelos propostos, além de habilitar inferências por modelos computacionais e simplificar o processo de transferência de informações e conhecimento dentro de um contexto de ciberdefesa colaborativa. Dentre as contribuições deste trabalho, podemos destacar a criação da Computer Security Incident Handling Ontology (CSIHO), em formato OWL, com a utilização de um dataset criado com base num estudo de caso de um incidente com o ransomware Wanna-Cry, bem como a elaboração de outra ontologia OWL, a partir do modelo VERIS, onde foi realizada a conversão e carga do dataset original. Para demonstrar a aplicabilidade das ontologias foram criadas consultas SPARQL baseadas em perguntas de competência em três diferentes cenários, incluindo consultas que correlacionam dados entre as ontologias.', 
  resumo_en: 'The fast evolution of information technology in the last decades led the society to a growing process of dependency in computer systems and Internet-based services. This complex and dynamic scenario implies in more challenging cyberdefense initiatives and, although the industry applies many efforts to ensure the Information Security, considerable growth in volume, frequency and severity of incidents is still observed. With attacks motivated by financial and political gain, each time using more resources and, many times, financed by States, the occurrence of a security incident is practically inevitable. Proper preparation is needed to respond quickly and efficiently to the cyber incidents, but the traditional information security approaches are more focused on prevention than on incident response. The objective of this work is to present a new model for incident handling, described as an ontology, which is easily extensible and integrable with other models, and enables inferences by computational models and simplifies the knowledge transfer within a collaborative cyberdefense context. Among the contributions of this work, we can highlight the creation of the Computer Security Incident Handling Ontology (CSIHO), in OWL format, with the use of a dataset created based on a case study of an incident with the WannaCry ransomware, as well as the creation of another OWL ontology, based on the VERIS model, where the original dataset was converted and loaded into. To demonstrate the applicability of the ontologies, SPARQL queries were created based on competency questions of three different scenarios, including examples of queries that correlate data between both ontologies.'},

 {numero: 728, ano: 2018, dia: '04/04', autor: 'Anderson Chaves da Silva', orientador: [3], linha: 1, arquivo: '2018-Anderson.pdf', 
  titulo: 'Problema do Aumento da Conectividade Algébrica em Classes Particualres de Grafos', 
  resumo_pt: 'A teoria espectral de grafos busca identificar características dos grafos a partir da observação dos espectros de diferentes matrizes associadas aos mesmos. Um importante problema nesta área é conhecido como o Problema do Aumento Máximo da Conectividade Algébrica. O mesmo consiste em, dado um grafo maximizar o valor do parâmetro denominado conectividade algébrica através da adição do menor número de arestas possível. Uma vez que este problema é NP-Completo, existem na literatura algoritmos visando obter soluções aproximadas. Neste trabalho apresentamos os resultados de experimentos comparativos entre dois destes algoritmos: a heurística de perturbação, HP, e a heurística de excentricidade, HE. Apresentamos também resultados teóricos, os quais descrevem casos em que a inserção de arestas não aumenta a conectividade algébrica. Efetuamos os experimentos em duas classes de árvores: árvores double broom - em particular, uma subfamília das mesmas, as árvores broom - e uma subfamília da classe das árvores starlike. Para realizar os experimentos desenvolvemos um sistema na linguagem Python que permite efetuar comparações entre os resultados das heurísticas. Para as árvores double broom, os resultados obtidos nos experimentos com a HE, considerando até 2 arestas a serem inseridas, revelaram em todos os casos avaliados um aumento da conectividade algébrica maior ou igual aos da HP. Para as árvores broom, o aumento obtido pela HE considerando 1 aresta a ser inserida foi sempre maior que o resultado obtido pela HP. Ao serem inseridas 2 arestas, a HE é proporcionou um aumento maior em quase todos os experimentos realizados para as árvores broom. A partir dos experimentos realizados, foi possível identificar algumas arestas.', 
  resumo_en: 'The spectral graph theory is a research field that seeks to identify characteristics of graphs from the observation of the spectra of different matrices associated with them. An important problem on this Field is know as the Maximum Algebraic Connectivity Augmentation Problem. It consists of, given a graph, maximizing the value of the algebraic connectivity by adding as few edges as possible. Since the problem is NP-Complete, there are algorithms in the literature to obtain approximate solutions. On this work, we present the results of comparative experiments between two of these algorithms: the perturbation heuristic, PH, and the eccentricity heuristic, EH. We also present theoretical results, which describe scenarios in which the insertion of edges does not increase the algebraic connectivity. We performed the experiments on two classes of trees: double-broom trees - in particular, a subfamily known as broom trees - and a subfamily of the class known as starlike trees. To perform the experiments we developed a system, written in Python programming language, which is able to make comparisons between the results of the heuristics. For the double broom trees, the results obtained in the experiments with EH, considering up to 2 edges to be inserted, revealed in all cases an increase in algebraic connectivity greater than or equal to PH. For the broom trees, the increase obtained by the EH considering 1 edge to be inserted was always greater than the result obtained by PH. When 2 edges are inserted, EH provided a larger increase in almost all the experiments performed on the broom trees. Based on these experiments, it was possible to identify some edges which, when inserted, did not increase algebraic connectivity.'},

 {numero: 727, ano: 2018, dia: '07/02', autor: 'Felipe Alves de Oliveira', orientador: [8], linha: 3, arquivo: '2018-Felipe.pdf', 
  titulo: 'Mineração de Regras de Associação de Multirrelação em Datasets na Web de Dados', 
  resumo_pt: 'A Web de Dados é hoje uma fonte extremamente rica e que contém diferentes tipos de informações. Extrair informações que levem ao conhecimento é um grande desafio para o avanço da área científica. Diversos algoritmos e ferramentas são desenvolvidos para auxiliar no processo de extração de conhecimento, valendo-se de diferentes estratégias de busca, como por exemplo a mineração de dados por meio da mineração de regras de associação. Entretanto, esses algoritmos são custosos em termos computacionais, e por isso aplicá-los no contexto na Web de Dados parece uma tarefa impossível devido à complexidade de manipular as diferentes fontes de dados. Os datasets disponíveis na Web de Dados, são representados em grafo, por meio dos recursos em RDF. É possível analisar os dados em grafos com a mineração de regras de associação de multirrelação fazendo uso do algoritmo MRAR. Neste trabalho, propomos uma forma de minerar dados de um dataset na Web de Dados, visando estender tal dataset em análise com informações de fontes externas (datasets conectados), possibilitando a geração de regras úteis para o usuário, além de apresentar uma formalização e extensão do algoritmo MRAR. Foram realizados experimentos que comprovaram a viabilidade e validade da abordagem proposta em três diferentes cenários, apresentando resultados promissores.', 
  resumo_en: 'The Web of data is an extremely rich source containing different kinds of information. To extract information leading to knowledge is a major challenge. Several algorithms and tools were developed to assist in the process of knowledge extraction, using different search strategies, such as the data mining by means of mining association rules. However, these algorithms are computationally expensive, so applying them in the context of the Web of data seems an impossible task due to the complexity of the different data sources. The datasets available on the Web of data are represented as a graph, using RDF. It is possible to analyze data in graphs by mining multirelation association rules, making use of the MRAR algorithm. In this work, we propose an approach of mining datasets from the Web of data, in such a way that the analysis is performed on an extended dataset, with information from external sources (connected datasets), allowing the generation of useful rules for the user. In addition, we also present a formalization and extension of the MRAR algorithm. Three experiments were also reported and demonstrated the viability and validity of the proposed approach, showing promissing results.'},

 {numero: 726, ano: 2018, dia: '02/02', autor: 'Edson Barbosa de Souza', orientador: [2], linha: 2, arquivo: '2018-Edson.pdf', 
  titulo: 'Detecção de Ataques LR DDOS Slowloris Utilizando Entropia', 
  resumo_pt: 'Uma das principais dificuldades no estudo de ataques de negação de serviço é a ausência de datasets completos e publicamente acessíveis. Os disponíveis, em geral, não contém os ataques atuais, como os do tipo slow HTTP. Existem apenas poucos datasets com ataques LR DDoS disponíveis publicamente. Ataques LR DDoS (Low Rate Distributed Denial of Service) exploram protocolos como HTTP a fim de tornar um servidor indisponível aos usuários legítimos, porém com um volume de tráfego em geral menor que o empregado em ataques flooding. Exemplos de ataques LR DDoS são o slowloris e o sockstress. Ataques do tipo LR DDoS geram tráfego muito similar ao legítimo, tornando a sua detecção um desafio. Ao longo do texto, são descritos os experimentos que foram realizados para gerar o dataset de ataques distribuídos de negação de serviço (DDoS) slowloris, sockstress e comparadas algumas taxonomias de ataques DDoS. A entropia, que é a medida da incerteza relacionada com uma variável aleatória, mede a informação média associada às observações da variável e é um conceito da Teoria da Informação muito utilizado para a detecção de ataques DDoS. Dentre as diversas métricas para o cálculo de entropia, destacam-se a de Hartley, a de Shannon, a de Renyi e a de Kullback–Leibler ou distância de Kullback–Leibler. As mais efetivas para a detecção de tráfego anormal são a entropia de Shannon e a de Kullback–Leibler. O objetivo deste trabalho é propor um método para a detecção de ataques LR DDoS slowloris a um servidor web utilizando o cálculo da entropia de Shannon (entropia de IP origem ou entropia de IP destino) e mais duas métricas criadas nesta pesquisa: medidas de taxa de envio de pacotes vazios por segundo (PVS) e taxa de recebimento dos caracteres delimitadores de fim de cabeçalho do protocolo HTTP, FCS (Fim de Cabeçalho por Segundo).', 
  resumo_en: 'One of the main difficulties in the study of denial of service attacks is the absence of publicly accessible complete datasets. Those available, in general, do not contains the current attacks, like slow HTTP. There are only few datasets publicly available with DDoS LR attacks. Low Rate Denial of Service attacks exploit protocols such as HTTP in order to make a server unavailable to legitimate users, but with a volume of traffic generally lower than that employed in flooding attacks. Examples of LR DDoS attacks are slowloris and sockstress. LR DDoS attacks generate a lot of traffic similar to the legitimate one, making its detection a challenge. There are only few datasets publicly available with DDoS LR attacks. The experiments performed to generate the distributed denial of service attack dataset with slowloris, sockstress are described and some taxonomies of DDoS attacks are compared. Entropy, which is the measure of uncertainty related to a random variable, measures the average information associated with  the observations of the variable and it is a concept of the Information Theory that is much used for the detection of DDoS attacks. There are several metrics for calculating entropy, we highlight the Hartley entropy, the Shannon entropy, the Renyi entropy, and the Kullback-Leibler entropy or Kullback-Leibler distance. The most effective for detecting abnormal traffic are the Shannon entropy and the Kullback-Leibler entropy. The goal of this thesis is to propose a method for the detection of LR DDoS slowloris attacks to a web server using the Shannon entropy calculation (source IP entropy or destination IP entropy) and two more metrics created in this research: rate measures sending packets with empty payload (PVS) and receive rate of the HTTP protocol header end delimiter characters (FCS).'},

 {numero: 725, ano: 2018, dia: '30/01', autor: 'Igor Devulsky Prata', orientador: [9], linha: 2, arquivo: '2018-Igor.pdf', 
  titulo: 'Metodologia para Localização de Ativos por Técnicas de Fusão de Sensores para Rastreamento Autônomo', 
  resumo_pt: 'A crescente evolução tecnológica pode ser observada no avanço dos padrões de comunicação digital, expandindo os seus aspectos técnicos de transmissão e suas aplicações. A alta disponibilidade tecnológica, sua miniaturização e a facilidade de acesso possibilitaram diversos cenários de aplicação, principalmente na computação embarcada. Essa evolução é refletida na queda de preços de produção de bens de consumo, na popularidade de smartphones e computadores transportáveis - que contam com diversos recursos de comunicação embarcados. O acesso aos sistemas robóticos possibilita que veículos aéreos não tripulados (VANTs) de baixo custo possam efetuar tarefas com considerável nível de automação, propiciando maior diversidade de aplicações. Apesar da popularidade de padrões específicos de localização por satélite (GNSS, Global Navigation Satellite System), sua aplicação é limitada por fatores ambientais, como dentro edificações, onde o sinal é consideravelmente enfraquecido ou obliterado por obstáculos. Dentro desse cenário, sistemas originalmente utilizados para comunicação desempenham um papel versátil, provendo recursos de localização aos dispositivos em sua proximidade que por sua vez podem estimar a sua distância do transmissor a partir do decaimento de sinal recebido. Este trabalho propõe uma técnica que utiliza rádios em diferentes padrões com a finalidade de melhorar a estimativa de localização de uma fonte emissora de radiofrequência, através da aplicação de algoritmos de fusão de sensores. A estimativa de posicionamento por um único tipo de sensor gera imprecisões que podem ser acentuadas dependendo do meio. Diversos trabalhos sugerem que o problema de localização não deve ser tratado por uma abordagem sensorial única, que compreenda todos os desafios da área, mas sim pelo uso de múltiplas técnicas que possam sobrepor limitações e aumentar a acuidade do sistema. Essa dissertação apresenta uma metodologia desenvolvida para ser embarcada em VANTs, executando aplicações de wardriving em protocolos de rede bem difundidos, WiFi (IEEE 802.11) e bluetooth, mas com o potencial de suportar outros padrões, possibilitando à aeronave detectar alvos emissores de maneira autônoma. A técnica adotada utilizou fusão de sensores para incorporar dados de RSSI (Received Signal Strength Indicator), recebidos por rádios WiFi e bluetooth, na estimativa de distância e trilateração por múltiplos pontos de análise para determinar a posição do alvo. A metodologia foi validada em diversos testes simulados e em ambientes controlados para medir a performance e aplicabilidade. Foram realizados testes de voo reais para detecção de único smartphone, que atuou como transmissor de sinais em múltiplos padrões. Os resultados em campo aberto se mostraram precisos dentro dos limites operacionais de cada rádio adotado na fase experimental. Durante a confecção do trabalho foram gerados artefatos (presentes nos anexos dessa dissertação) sobre a implementação do software e montagem do sistema computacional embarcado, massa de dados coletados em voo, bem comdetalhamentos sobre a construção da aeronave experimental e procedimentos operacionais utilizados nos ensaios.', 
  resumo_en: 'The increasing technological evolution can be observed in the advance of digital communication standards, expanding its technical specifications and applications. The high technological availability, its miniaturization, and access allowed several application scenarios, especially in embedded computing. This evolution is reflected in the prices of consumer goods, smartphones and other computer gadgets, which have several embedded communication capabilities. The access to robotic system enables low-cost unmanned aerial vehicles (UAVs) to perform tasks with a considerable automation, providing a greater diversity of applications. Despite the popularity of satellite location standards (GNSS, Global Navigation Satellite System), its application is limited by environmental factors, such as indoor use, where the signal is considerably weakened or obliterated by obstacles. Within this scenario, systems originally used for communication play a versatile role: providing location resources to devices in their vicinity which, in turn, can estimate their distance from the transmitter from the received signal decay. This work proposes a technique that uses radios in different standards in order to improve location estimation of a radiofrequency source through the application of sensor fusion algorithms. Positions estimated by a single type of sensor generate inaccuracies that can be accentuated depending on the environment. Several studies suggest that the localization problem should not be addressed by a single sensory approach, that fulfill all the challenges of the area, but rather by multiple techniques that can overcome limitations and increase the precision of the system. This dissertation presents a methodology to be embedded in UAV, executing wardrive applications in well-known network protocols, WiFi (IEEE 802.11) and bluetooth, with the potential to support other standards, allowing the aircraft to autonomously detect emitting targets during a flight. The adopted technique uses sensor fusion to incorporate RSSI (Received Signal Strength Indicator) data, received by WiFi and bluetooth radios, to estimate distance and trilateration, by multiple points of analysis, to determine the position of a target. The methodology was validated in several simulated tests and in controlled environments to measure performance and applicability. Real flight tests were performed to detect a smartphone, which acted as a multi-pattern signal transmitter. The results in the open field were accurate within the operational limits of each radio adopted during the experimental phase. This work generated auxiliary artifacts (available in the annexes of this dissertation) about the software implementation and assembly of the onboard computer system, a mass data collected in flight, and further details on the construction of the experimental aircraft and operational procedures used on the experiments. '},

 {numero: 724, ano: 2017, dia: '20/12', autor: 'Érick da Silva Florentino', orientador: [13], linha: 3, arquivo: '2017-Erick.pdf', 
  titulo: 'Utilizando o Histórico da Evolução de Redes Complexas na Tarefa de Predição de Link', 
  resumo_pt: 'A predição de links, uma das mais importantes tarefas na análise de redes complexas, procura identificar pares de elementos da rede não conectados e que tenham propensão para se interligar no futuro. Diversos métodos para implementar essa tarefa têm sido desenvolvidos. Em geral, eles se baseiam na estrutura atualizada da rede, deixando de considerar informações históricas sobre como era a topologia da rede nos momentos em que as ligações existentes foram inseridas na estrutura. O presente trabalho se baseia na hipótese de que resgatar tais informações contribui para construir modelos preditivos mais precisos do que os existentes, uma vez que elas enriquecem a descrição do contexto da aplicação com exemplos que retratam justamente o tipo de evento que se deseja prever: o surgimento de novas conexões. Assim sendo, o presente trabalho tem como objetivo descrever um método de predição de links que se baseia no histórico de evolução das topologias de redes complexas. Resultados obtidos com os experimentos revelaram que por meio do método proposto foi possível obter resultados superiores aos do métodos clássicos existentes.', 
  resumo_en: 'The prediction links, one of the most tasks in analysis of the complex networks, aims to identify pair of elements of no connected network that have the tendency to interconnect in the future. Several methods to implement this work have been developed. Usually they are based on update structure of the network, without considering historical information on how was he network topology when the existing connections were introduced in the structure. The work is based on the hypothesis that the recovering of such information will contribute to construct predictive models more precise than the existing ones, since they enrich the description of the context application with examples that represent just the kind of event that we want predict: arising of new connections. Therefore, the present work has as scope to describe a links prediction method based on the evolution of complex networks. Results obtained with the experiments revealed that, by means of the proposed method, it was possible to obtain results higher than the classic existing methods.'},

 {numero: 723, ano: 2017, dia: '18/12', autor: 'Ricardo de Azevedo Brandão', orientador: [13], linha: 3, arquivo: '2017-Ricardo.pdf', 
  titulo: 'Clusterização de Dados Distribuídos no Contexto da Internet das Coisas – uma Abordagem para Redução do Tráfego de Dados', 
  resumo_pt: 'A Internet das Coisas surgiu com o objetivo de integrar objetos físicos às tradicionais redes de computadores. Esses objetos costumam gerar uma grande quantidade de dados, transferindo o gargalo do processamento de dados dos sensores para os sistemas de comunicação. Como exemplo pode-se citar a análise de dados gerados por dispositivos da Internet das Coisas geralmente exigem que os dados sejam centralizados antes de executar algoritmos de mineração. Porém, com o crescente aumento do volume de dados produzidos e com as limitações de recursos dos dispositivos, que praticamente inviabiliza soluções de processamento dos dados de forma distribuída, foi imposto um grande desafio para os pesquisadores da área. Assim, com o objetivo de reduzir o volume de dados transferidos, o presente trabalho propõe uma abordagem de sumarização de dados baseada em grid juntamente com um algoritmo de clusterização adaptado a essa abordagem. A proposta utiliza um grid uniforme, que particiona o espaço em células e sumariza os dados antes de enviá-los ao nó central. A sumarização é a responsável pela redução dos dados a serem enviados. No nó central os dados oriundos dos dispositivos são consolidados e submetidos ao algoritmo gCluster, proposto por este trabalho. Os experimentos foram executados em seis conjuntos de dados e os resultados das clusterizações foram comparados com o algoritmo DBSCAN. Para a avaliação do resultado foi utilizado o índice de Fowlkes e Mallows. Os resultados indicam que apesar da sumarização dos dados, não houve queda substancial na qualidade da clusterização. ', 
  resumo_en: 'The Internet of Things (IoT) emerged with the objective to integrate physical objects into classical computer networks. These objects usually generate larges amount of data, transferring the bottleneck of data processing from sensors to communication systems. For example, analyzing IoT data often demands data centralization before running a mining algorithm. However, with the increasing volume of data produced and the resource limitations of the devices, which virtually render data processing solutions unviable, researchers are facing a big challenge. Thus, in order to reduce the data transference commonly required by the data clustering task, the present work proposes a grid-based data summarization approach among a clustering algorithm adapted to this approach. The proposed approach uses a single uniform grid to partition the space into cells and to summarize data before centralization. Summarization ensures the reduction of the amounts of data transferred. In the central node the data originated from the devices are consolidated and submitted to the gCluster algorithm, proposed by this work. The experiments were run on six datasets and the results of the clustering were compared with the DBSCAN algorithm. The Fowlkes and Mallows index was used to evaluate the result. The results revealed that despite the data summarization, there was no substantial decrease in the quality of clustering.'},

 {numero: 722, ano: 2017, dia: '14/12', autor: 'Leandro de Mattos Ferreira', orientador: [5], linha: 2, arquivo: '2017-LeandroFerreira.pdf', 
  titulo: 'A Aplicação de Wavelets no Reconhecimento de Padrões Criptográficos', 
  resumo_pt: 'Este trabalho combina o uso de transformadas wavelet e técnicas de Recuperação de Informação, como ferramentas para realizar o agrupamento e classificação de criptogramas gerados em modo ECB por sistemas criptográficos distintos. Resolver os problemas de agrupamento e classificação servem como primeiro passo numa análise criptográfica, e portanto são relevantes em situações reais. Os sistemas analisados foram o AES, Twofish, 3DES, RC6 , Serpent e DES. Uma base de criptogramas foi gerada com estes sistemas utilizando textos em claro em inglês presentes na base Reuters-21578. As transformadas Wavelet foram utilizadas sobre os blocos do criptograma, juntamente com a aplicação de técnicas de Recuperação de Informação para agrupamento e classificação dos criptogramas. A divisão dos criptogramas em blocos menores do que o bloco de cifragem foi analisada, resultando em tempos menores de execução do agrupamento, e como principal resultado levando à criação de um classificador binário para 3DES. O uso de wavelets trouxe resultados similares aos do emprego apenas de técnicas de RI, traduzindo-se em vantagem apenas na redução no uso de espaço em disco.', 
  resumo_en: 'This work combines the application of wavelet transforms and Information Retrieval techniques, as tools to implement the grouping and classification of ciphers creatd by distinct algorithms in ECB mode. Solving the grouping and classification problem serves as a first step in a cryptanalysis, and therefore are relevant in real world applications. The cryptographic systems analyzed were AES, Twofish, 3DES, RC6 , Serpent and DES. A database of ciphers was created using these systems applied over plaintexts in English contained on the Reuters-21578 database. The wavelet transforms were applied over the cipher blocks, along with the use of Information Retrieval techniques. Also, the division of the cipher into blocks smaller than the encryption block was analysed, resulting in shorter execution times for grouping, and producing, as the main result, a binary classifier for the 3DES algorithm. The use of wavelets brought only results similar to those obtained using only Information Retrieval techniques, bringing only a reduction of disk space usage as a positive result.'},

 {numero: 721, ano: 2017, dia: '01/12', autor: 'Andressa da Silva Siqueira', orientador: [2], linha: 2, arquivo: '2017-Andressa.pdf', 
  titulo: 'Algoritmo de Localização de Redes', 
  resumo_pt: 'O crescente avanço tecnológico tem contribuído para a disseminação de novas tecnologias, especialmente redes de comunicação sem fio. A crescente evolução e generalização dessa tecnologia nos ambientes empresarias, acadêmicos e residenciais é devida a flexibilidade e mobilidade gerada por essas redes para seus usuários. Essa evolução e o crescente aumento de dispositivos portáteis sem fio inseridos em ambientes onde essas redes estão presentes, permite a criação de sistemas de geolocalização para todos os tipos de componentes envolvidos nessa rede de comunicação, podendo ser utilizado para monitoramento e gestão de recursos humanos dentro de uma organização, para localização e rastreamento de alvos durante a guerra, para socorro às vítimas de desastres naturais entre outros. Apesar da localização de dispositivos já ser assegurada, com um nível de precisão de aproximadamente de três metros, pelo serviço de GPS (Global Position System - Sistema Global de Posicionamento), para determinados sistemas, a diferença entre a posição real e a posição fornecida por sistemas que utilizam apenas esse serviço é considerada insuficiente, sendo necessária uma precisão maior, além de, nem sempre ser possível conhecer a localização do dispositivo previamente por falta de registros ou por falha nos meios de comunicação. Este trabalho aborda um problema crítico, especificamente para sistemas de geolocalização e rastreamento de alvos. O principal objetivo é apresentar um algoritmo de localização em ambiente outdoor baseado na força do sinal recebido em que o sistema não possui o conhecimento prévio da localização dos roteadores, também conhecidos como Access Points (APs). As técnicas usualmente utilizadas pelos sistemas de localização serão abordadas de forma sucinta neste trabalho. A técnica de trilateração, uma das menos custosas a ser implementada, pois não necessita de inclusão de nenhum equipamento além de um receptor de sinal Wi-Fi (802.11), é o objeto de estudo mais aprofundado devido a sua fácil implementação. Alguns algoritmos de referência na área, que são apresentados neste trabalho, utilizam recursos adicionais (tais como, sensores de som ou várias antenas) ou técnicas específicas (mapas de assinaturas, análise de angulação da chegada do sinal entre outros) que tornem vários algoritmos criados de difícil implementação ou alto custo ou não possuem uma boa precisão na localização fornecida. Como diferencial, a abordagem desse trabalho é criação de um algoritmo totalmente genérico que não necessite de recursos nem técnicas adicionais para fornecer a geolocalização de APs com uma boa precisão. Experimentos reais foram conduzidos com o uso de uma plataforma de VANT (Veículo Aéreo Não Tripulado) que permite o embarque de um microcomputador e um controlador de voo para a coleta de todos os dados de potência de sinal e de dados do GPS. Modelos matemáticos já conhecidos no meio acadêmico foram utilizados para os cálculos necessários para encontrar a localização estimada dos APs. ', 
  resumo_en: 'The growing technological advance has contributed to the spread of new technologies, especially for wireless networks. The increasing evolution and generalization of this technology in business, academic and residential environments is due to the flexibility and mobility generated by these networks for its users. This evolution and the growing increase of portable wireless devices inserted in environments where these networks are present, allows the creation of geolocation systems for all types of components involved in this communication network, and can be used for monitoring and management of human resources inside of an organization, for geolocation and tracking of targets during the war, for the rescue of victims of natural disasters among others. There are numerous applicability’s of this type of system. Although the location of devices is already assured, with a level of accuracy of about three meters, by the GPS (Global Position System) service, for certain systems, the difference between the actual position and the position provided by systems that use only this service is considered insufficient, requiring greater precision, besides, it is not always possible to know the location of the device previously due to lack of records or failure in the media. This work addresses a critical problem, specifically for geolocation systems and target tracking. The main goal is to present a localization algorithm based on the strength of the received signal where the system does not have the prior knowledge of the location of the routers, also known as Access Points (APs). The techniques commonly used by the localization systems will be summarized in this work. The trilateration technique, one of the least costly to be implemented, because it does not need to include any equipment other than a Wi-Fi (802.11) signal receiver, is the object of further study due to its easy implementation. Some algorithms of reference in the area, which are presented in this work, use additional features (such as sound sensors or several antennas) or specific techniques (signature maps, signal angulation analysis among others) that make several algorithms created difficult to implement or high cost or do not have good accuracy in the location provided. As a differential, the approach of this work is to create a totally generic algorithm that does not require additional resources or techniques to provide the geolocation of APs with a good precision. Actual experiments were conducted using an UAV (Unmanned Aerial Vehicle) platform that allows the shipment of a microcomputer and a flight controller to capture all signal power and GPS data. Mathematical models already known in the academic world were used for the calculations necessary to find the estimated location of the APs.'},

 {numero: 720, ano: 2017, dia: '22/11', autor: 'Igor de Souza Costa', orientador: [6], linha: 3, arquivo: '2017-Igor.pdf', 
  titulo: 'Um Serviço Interoperável para a Proveniência de Dados em Experimentos de Aprendizado de Máquina', 
  resumo_pt: 'O uso do Aprendizado de Máquina na solução de diversos problemas vem se expandindo nas últimas décadas, atraindo cada vez mais a atenção da comunidade científica. Como consequência de tal fato, diversas tecnologias e ferramentas vem surgindo ao longo do tempo, oferecendo diferentes formas para se trabalhar com tais problemas. Um dos problemas que a proliferação de tais tecnologias causa é a falta de interoperabilidade de dados entre ferramentas e tecnologias distintas. Além disso, existe uma escassez de recursos para a descrição dos dados e metadados de forma semântica, atrelando-os a um bom  nível de proveniência. O Projeto MEX tem como objetivo minimizar os problemas em relação à semântica e à proveniência dos dados através da criação de um vocabulário que permita descrever os dados gerados pelos experimentos de Aprendizado de Máquina, utilizando técnicas de publicação de dados para a Web Semântica. Entretanto, em sua concepção, o projeto conta com uma limitação no seu uso, as suas implementações estão escritas na linguagem Java, fazendo com que o vocabulário só possa ser utilizado diretamente em experimentos escritos usando nessa linguagem.  Neste trabalho é apresentado o desenvolvimento de um Serviço Web, Web4MEX, projetado para resolver a limitação dessas tecnologias e resolver o problema da interoperabilidade entre ferramentas de Aprendizado de Máquina implementadas em linguagens de programação distintas. Além disso, como demonstração do poder da solução, apresentamos um framework escrito em Python, que permite o acesso ao vocabulário MEX, intermediando os dados extraídos dos experimentos com o servidor responsável por gerar o vocabulário, de uma forma automatizada utilizando um esquema de anotação. ', 
  resumo_en: 'The use of Machine Learning in solving several problems has increased in the last decades, attracting, each time more, the attention of the scientific community. As a consequence of this fact, several technologies and tools appeared, offering different ways to work with such problems. One of the problems that the proliferation of such technologies is the lack of interoperability of data between different tools and technologies. In addition to this, there is a lack of resources for the description of the data and metadata in a semantic way, linking them to a good level of provenience. The MEX Project aims at minimizing these problems regarding semantics and the provenance of data through the creation of a vocabulary that allows the description of data generated by Machine Learning experiments, using techniques for data publication at the Semantic Web. However, in its design, the project has a limitation on its use: its implementations are written in the Java language, so that the vocabulary can only be directly used in experiments written using this language. In this work, we propose the development of a web service called Web4MEX, designed to solve the limitation of these technologies and solve the problem of interoperability among machine learning tools implemented in distinct programming languages. In addition, as a demonstration of the power of the solution, we present a Python-based framework that allows access to the MEX vocabulary, mediating the data extracted from the experiments with the server responsible for generating the vocabulary, in a automated way using an annotation scheme.'},

 {numero: 719, ano: 2017, dia: '11/10', autor: 'William Tavares de Lima Ladeira', orientador: [5], linha: 2, arquivo: '2017-William.pdf', 
  titulo: 'Uma Análise das Técnicas do AES em White-box e sua Aplicação em Dois Estudos de Caso', 
  resumo_pt: 'Em algoritmos de criptografia, a ideia de que o criptoanalista possui acesso apenas ao criptograma e, em alguns casos, ao texto claro, não pode ser considerada verdadeira quando o contexto é white-box. Em white-box, os algoritmos são executados no ambiente do cliente, o que permite ao detentor do host de execução, um nível de acesso muito superior ao previsto por aqueles que projetaram as cifras candidatas ao concurso AES, por exemplo. Alguns algoritmos, como o DES e AES foram implementados também para o ambiente white-box, de forma a torná-los mais seguros nesse novo contexto; contudo, nem todos os algoritmos propostos para black-box possuem uma implementação que seja mais resistente em um ambiente vulnerável. A fim de identificar as características presentes em uma cifra projetada para white-box, esta dissertação realiza uma análise das técnicas aplicadas as implementações white-box do AES, para que seja possível torná-las mais genéricas e construir um conjunto de técnicas recomendáveis para cifras simétricas implementadas nesse contexto. Para demonstrar a viabilidade dessas técnicas, esse trabalho gera ainda dois estudos de caso, baseados no Twofish: O WBTwofish e o WBTwofish+. O primeiro utiliza técnicas que apesar de mais simples, não alteram o criptograma do algoritmo original; além disso, se utilizado em conjunto com as codificações e bijeções, essa implementação pode alcançar uma resistência de 2^40 passos, necessários para que a chave seja inferida pela criptoanálise BGE-Attack, em cada uma das rodadas. Já o WBTwofish+, sob as mesmas condições descritas para o WBTwofish e, através de algumas técnicas propostas para o WBAES+, alcança em cada rodada, uma resistência contra o BGE-Attack de: 2^104 passos em sua sbox e 2^128 passos em todo algoritmo. No entanto, os criptogramas do WBTwofish+ e Twofish são distintos.', 
  resumo_en: 'On cryptography algorithms, the idea that the cryptoanalyst has only access to the cryptogram and, in some cases, to the plain text, cannot be considered true when the context is white-box. Under the white-box context, the algorithms run on the client environment, which allows a superior access by the execution host owner than that which was previously predicted from the AES context algorithms designers, for example. Some algorithms, like the DES and AES, were also implemented for white-box, in order to make them stronger in this new context; however, not all algorithms proposed for black-box have a more secure implementation for a vulnerable environment. In order to identify the characteristics present in a cipher designed for white-box, this thesis performs an analysis of the techniques applied to the white-box AES, so that it is possible to make them more generic and construct a set of recommended techniques for symmetric ciphers, implemented under this context. To show the feasibility of these techniques, this work further generates two case studies, based on Twofish: WBTwofish and WBTwofish+. The former uses techniques which, although simpler, do not change the original algorithm cryptogram; futhermore, if it is used with encodings and mixing bijections, this implementation can reach a resistance of 2^40 steps against the BGE-Attack cryptoanalysis, in each one of the rounds. On the other hand, WBTwofish+, under the same conditions as WBTwofish, and through some techniques proposed for WBAES+, reach in each one of the rounds, a resistance against the BGE-Attack of: 2^104 in its sbox and 2^128 in the whole algorithm. However, the cryptograms of WBTwofish+ and Twofish are different.'},

 {numero: 718, ano: 2017, dia: '11/08', autor: 'Raquel Ellem Marcelino de Oliveira', orientador: [4], linha: 2, arquivo: '2017-Raquel.pdf', 
  titulo: 'Ambiente Virtual para Tratamento de Acrofobias', 
  resumo_pt: 'A ansiedade é um transtorno que está afetando cada vez mais pessoas na sociedade. Se este mal não for tratado, pode transformar-se em fobia, o que ocorre em inúmeros  casos. A fobia pode ser definida como um medo incontrolável que um indivíduo sente em relação a um objeto ou situação. Os tratamentos comumente aplicados se baseiam na utilização de produtos farmacológicos e na psicoterapia. Neste último, o método mais aplicado é a Terapia Cognitivo Comportamental, precisamente a técnica de Terapia de Exposição. Esta técnica consiste em expor gradualmente o paciente à situação temida. Há pouco tempo existiam duas formas de exposição: in vivo e in imagino. Na primeira, o paciente é exposto diretamente ao objeto ou situação temida, enquanto que na segunda, pretende-se apenas que o paciente imagine o mesmo cenário. No entanto, atualmente é possível recorrer à Realidade Virtual como um mecanismo de auxílio à terapia. A utilização da Realidade Virtual na Terapia de Exposição implica, de um modo geral, na redução de custos e riscos, além de não comprometer a integridade física do paciente. O objetivo principal deste trabalho foi desenvolver uma solução utilizando Realidade Virtual aplicada na Terapia de Exposição, que permitisse a um terapeuta simular diferentes níveis de altura sem a necessidade de sair do seu consultório. A solução proposta possui um baixo custo de desenvolvimento e utilização, sendo facilmente aplicável em um ambiente clínico. Foram desenvolvidos dois Ambientes Virtuais imersivos, que possuem características potencialmente indutoras de ansiedade. Para proporcionar a imersão do usuário ao AV, foi utilizado o dispositivo Oculus Rift DK1. É importante destacar, que o paciente é capaz de interagir e controlar o nível de altura ao qual é exposto por meio de um joystick. Um dos aspectos mais relevantes do sistema desenvolvido encontra-se na relação entre o baixo custo e a diversidade das ações disponíveis ao paciente, como a controlabilidade e a interface simples e intuitiva. Foram realizadas três avaliações com psicólogos e voluntários (não psicólogos). O objetivo foi verificar a usabilidade do sistema, a sensação de presença dos usuários nos AVs e também a sensação de ansiedade após a simulação. No geral, os resultados foram satisfatórios e esclarecedores. ', 
  resumo_en: 'Anxiety is a disorder that is affecting more and more people in society. If this evil is not treated, it can turn into phobia, which occurs in numerous cases. A phobia can be defined as an uncontrollable fear that an individual feels in relation to an object or situation. Common treatments used on the basis of pharmacological use and psychotherapy. In the latter, the most applied method is a Cognitive Behavioral Therapy, precisely the technique of Exposure Therapy. This technique consists in gradually exposing the patient to a forced situation. Not long ago there were two forms of exposure: in vivo and in imagino. In the first, the patient is exposed to the object or the situation, in the one in the second, it is only intended that the patient imagine the same scenario. However, set yourself up for Virtual Reality as a mechanism to aid therapy. The use of Virtual Reality in Exposure Therapy generally means reducing costs and risks, as well as not compromising the patient’s physical integrity. The main objective of this work was to develop a solution using Virtual Application Reality in Exposure Therapy, which allowed a therapist to simulate different levels of height without the need to leave his or her office. A proposed solution has a low cost of development and use being is easily applicable in a clinical setting. Two immersive Virtual Environments were developed, which have potentially inducing characteristics of anxiety. To provide information about the AV user, it was used in the Oculus Rift DK1. It is important to emphasize that it is the patient with the ability to interact and control the level of the height to which it is exposed by means of a joystick. There is a comparison and a simple and intuitive interface, such as a controllability and a simple and intuitive interface. Three comments were made with psychologists and volunteers. The objective was to verify the usability of the system, the sense of presence of the users in the AVs and also a feeling of anxiety after a simulation. Overall, the results were satisfactory and enlightening.'},

 {numero: 717, ano: 2017, dia: '02/06', autor: 'Lauro Víctor Ramos Cavadas', orientador: [4], linha: 2, arquivo: '2017-Lauro.pdf', 
  titulo: 'Interação Natural em uma CAVE Através da Integração do Dispositivo Microsoft Kinect', 
  resumo_pt: 'Este trabalho apresenta o desenvolvimento e validação de um método de interação natural em um ambiente imersivo (CAVE). O dispositivo usado para rastrear as ações do usuário foi o Microsoft Kinect, mas apenas a câmera infra-vermelha dele é usada, através do modelo de reconhecimento de gestos desenvolvido por (LEITE, 2015). A partir do reconhecimento de gestos fornecido por este modelo, uma nova gramática foi criada, aplicada à utilização em um ambiente CAVE. Este estudo mostra que os resultados obtidos foram satisfatórios, especialmente no quesito \"usabilidade do sistema\", onde os usuários indicaram poder usar o sistema proposto no dia-a-dia.', 
  resumo_en: 'This work presents the development and validation of a natural interaction method in an immersive CAVE environment. The Microsoft Kinect was used to track the user actions, but only the infrared camera of the device was used, through a gesture recognition model developed by (LEITE, 2015). From such gesture recognition, a nre grammer was defined, as needed for an immersive environment such a the CAVE. Our study shows that the results were satisfactory, specially with regard to the \"system usability\", which was mentioned by users as something they could get used to use daily.'},

 {numero: 716, ano: 2017, dia: '10/05', autor: 'Guilherme Cruz Sampaio', orientador: [12], linha: 2, arquivo: '2017-Guilherme.pdf', 
  titulo: 'Escalonador Multicritério para Sistemas de Mensagens em Redes Tolerantes a Atrasos e Desconexões', 
  resumo_pt: 'As redes militares possuem características como baixa largura de banda, alta mobilidade dos nós, alta latência e taxas de erros elevadas, cenário semelhante ao previsto em uma rede tolerante à atrasos e desconexões (DTN). Nos sistemas de Comando e Controle (C2) atuais, o escalonamento de mensagens é realizado com apenas a precedência da mensagem, de forma que mensagens com baixa precedência podem nunca ser entregues ao destinatário. Outra característica das comunicações militares é a entrega direta das mensagens, não sendo previsto a possibilidade de entrega com múltiplos saltos na rede através da passagem de custódia de uma mensagem entre um nó e outro, que é a abordagem proposta em uma DTN. Com isso, faz-se necessário à priorização eficiente das mensagens militares de acordo com outros critérios a serem definidos e a avaliação do impacto de implantação de uma DTN em uma rede militar. O objetivo deste trabalho é avaliar o emprego de escalonadores multicritério para sistemas de mensagens militares em uma DTN, com o intuito de maximizar a probabilidade de entrega de mensagens. São propostos dois novos escalonadores de mensagens a serem utilizados em conjunto com os algoritmos de roteamento DTN: lexicográfico e função valor. Com esses escalonadores será possível a entrega de mensagens com baixa precedência em determinados cenários, mesmo que existam mensagens com precedência mais alta no sistema. Também é proposto o uso de algoritmos de roteamento DTN em substituição a abordagem de entrega direta de mensagens. É utilizada a infraestrutura de comunicações de um Pel C Mec como configuração da rede. Os resultados das simulações comprovam a vantagem de se utilizar os escalonadores propostos em conjunto com protocolos de roteamento DTN, chegando a um aumento de 58,83% na taxa de entrega de mensagens em cenários de variação no alcance do enlace de transmissão ao utilizar o roteamento MaxProp com o escalonador lexicográfico, em substituição ao roteamento direto com o escalonador tradicional.', 
  resumo_en: 'This work aims at evaluating the use of multicriteria schedulers for military message systems in delay and disruption tolerant networks (DTN). The military networks have characteristics such as low bandwidth, high node mobility, high latency and high error rates, similar to the scenarios predicted in a DTN. This makes it necessary to efficiently prioritize military messages according to criteria to be defined. In command and control systems the messages are classified by the software operator in four levels of precedence: flash, urgent, preferential and routine. For each level of precedence, a maximum time is fixed for the message to be delivered to the recipient. In the current approach, four message queues are used, one for each precedence level. The scheduler attend the highest priority queue until all messages are sent. Then, the scheduler starts to serve the next highest priority queue in the system. Messages with low precedence will only be attended when there is no message in the buffer with higher precedence. Thus, depending on the amount of messages with high precedence in the system and the baud rate, messages with low precedence may expire before they are delivered to the recipient. Another characteristic of military communications is the direct delivery of the messages, not being foreseen the possibility of delivery with multiple jumps in the network through the passage of custody of a message between a node and another, which is the approach proposed in a DTN. The dissertation proposes the use of several criteria in order to quantify the final priority of a message, such as: message lifetime and precedence level. With these criteria it will be possible to deliver messages with low precedence in certain scenarios, even if there are messages with higher precedence in the system. It is also proposed to use DTN routing algorithms instead of the direct message delivery approach. Two new message schedulers are proposed to be used in conjunction with DTN routing algorithms: lexicographic and value function. The communications infrastructure of a Pel C Mec is used as the network configuration. The results of the simulations prove the advantage of using the proposed schedulers in conjunction with DTN routing protocols in place of traditional scheduling and routing approach.'},

 {numero: 715, ano: 2017, dia: '28/04', autor: 'Eduardo Marsola do Nascimento', orientador: [5], linha: 2, arquivo: '2017-Eduardo.pdf', 
  titulo: 'Algoritmo de Criptografia Leve com Utilização de Autenticação', 
  resumo_pt: 'O ambiente da Internet das coisas (IoT) requer o uso de técnicas criptográficas que sejam leves, com baixo custo computacional. Os algoritmos existentes além de serem pouco flexíveis, normalmente fornecem autenticação por técnicas externas ao algoritmo, como os modos de operação. Este trabalho apresenta a cifra FlexAE, um novo algoritmo leve, flexível e autenticado para ser utilizado no ambiente de IoT. Ele trabalha com tamanhos variáveis de bloco (64 * 2 exp(x) bits, onde x >= 0) e chave (128 * 2 exp(x) bits, onde x >= 0). O algoritmo foi construído visando consumir poucos recursos computacionais. A base da cifra é a construção Even-Mansour (EVEN; MANSOUR, 1997), que utiliza duas operações XOR e uma permutação, e o modo paralelizável que considera integridade (IAPM) (JUTLA, 2001). A função de permutação também foi definida tomando o cuidado para permitir o uso de diversos tamanhos de blocos e chave, tornando a cifra flexível. A cifra foi validada quanto a capacidade de gerar sequência pseudo-aleatórias, leveza e resistência de ataques criptanálise diferencial e linear. Os testes estatísticos utilizados para validar a aleatoriedade das sequencias geradas foram o conjunto de testes estatísticos do NIST (RUKHIN et al., 2010) e a ferramenta DieHarder (BROWN; EDDELBUETTEL; BAUER, 2016). Para confirmar que a cifra é leve, ela foi comparada com outras cifras utilizando as métricas da ferramenta FELICS (DINU et al., 2015) - uso de memória RAM, tempo de processamento e tamanho de código. A dificuldade dos ataques de criptanálise diferencial e linear da cifra indica que a cifra é segura quando utiliza blocos de 64 e 128 bits. Para blocos maiores que 128 bits, a cifra é segura para trabalhar com chaves de até 256 bits. O trabalho também apresentou um sistema que permite a comunicação segura entre dispositivos em um ambiente IoT, utilizando a cifra proposta.', 
  resumo_en: 'The Internet of Things (IoT) environment requires lightweight cryptographic techniques, with low computational costs. The existent algorithms have little flexibility and normally provide authentication through external techniques, like the modes of operation. This work presents the FlexAE cipher, a new algorithm that is lightweight, flexible and authenticated. It works with variable size for blocks (64 x 2^x bits, where x >= 0) and keys (128 x 2^x bits, where x >= 0). The algorithm was designed aiming to require low computational resources. The cipher is bases on Even-Mansour construction (EVEN; MANSOUR, 1997), that uses only two XOR operations and a permutation, and the Integrated Aware Parallelizable Mode (IAPM) (JUTLA, 2001). The permutation function was also defined taking care to allow variable size blocks and keys, making the cipher flexible. The cipher was validated in relation to its capacity to generate pseudo-random sequences, lightweight and resistance against differential and linear cryptanalises attacks. The statistical tests used to validate the sequences randomness generate were the NIST Statistical Tests Suite (RUKHIN et al., 2010) and the DieHarder tool (BROWN; EDDELBUETTEL; BAUER, 2016). To assure the cipher is lightweight, it was compared to other ciphers using the FELICS (DINU et al., 2015) framework metrics – RAM use, process time and code size. The differential and linear complexity calc indicated that the cipher is secure when using 64 and 128 block sizes. For larger than 128 bits block sizes, the cipher is secure to work with keys up to 256 bits. This work also presented a system that allows secure communication among devices on a IoT environment, using the proposed cipher.'},

 {numero: 714, ano: 2017, dia: '06/02', autor: 'Hugo Andrade de Oliveira', orientador: [9], linha: 2, arquivo: '2017-Hugo.pdf', 
  titulo: 'Controle de Atitude Adaptativo Neuro-fuzzy Genético para um Veículo Aéreo não Tripulado Autônomo', 
  resumo_pt: 'Com o crescimento iminente e o progressivo interesse pela temática, os veículos aéreos não tripulados (VANTs) já são uma realidade em nosso cotidiano. As diversas aplicações, a versatilidade e o potencial atrelado a este campo de interesse, tem instigado recorrentes pesquisas na área. A busca por veículos aéreos, ambicionada por um futuro no qual os VANTs possam atuar de forma autônoma e segura, impulsiona continuamente este setor. O presente trabalho tem como propósito aplicar técnicas de inteligência artificial a sistemas de controle clássico, com a finalidade de controlar a atitude de uma aeronave de asa fixa de forma adaptativa. Inicialmente, apresenta-se o desenvolvimento do arcabouço que viabilizou a pesquisa, desde o ambiente simulado no qual é possível efetuar testes da arquitetura proposta, até o estudo de um hardware robusto o suficiente que se adeque aos requisitos do projeto, o que viabilizou a montagem de uma aeronave multipropósito e funcional. Foi utilizado como base para este projeto o software livre ArduPlane, um piloto automático implementado em C/C++, o qual dispõe de uma implementação otimizada de um controlador PID para a atitude. Percebeu-se a necessidade do frequente ajuste dos parâmetros atrelados ao sistema de controle de atitude, desde a sintonia dos ganhos do controlador PID, até parâmetros cuja calibração necessite conhecimentos técnicos específicos do sistema. Com a finalidade de automatizar este processo e garantir a otimização destes parâmetros durante toda a missão, foi proposta uma arquitetura neuro-fuzzy genética para tornar este procedimento implícito e transparente ao operador de voo. Com a utilização de um sistema de identificação (Adaptive Neuro Fuzzy Inference System - ANFIS) capaz de predizer a atitude da aeronave, junto a um sistema de otimização (algoritmo genético), é possível a construção de uma arquitetura de controle eficiente. Esta arquitetura assegura um controle aprimorado e que sempre busca por parâmetros otimizados durante toda a missão de forma autônoma. Para validação da arquitetura, foram realizados testes em ambiente simulado e ensaios em voos reais, os quais confirmaram a eficácia e viabilidade do sistema; para efeitos de comparação, também foram realizados ensaios em voos utilizando o controlador de atitude com os ajustes padrões do piloto automático. Por fim, a arquitetura do sistema de controle proposto se mostrou eficiente, plausível e de grande auxílio a operadores de voo, pois a mesma elimina a rotina na qual o operador deve realizar um pré-voo destinado à sintonia de parâmetros e garante o ajuste otimizado dos mesmos durante todo o cumprimento da missão.', 
  resumo_en: '--'},

 {numero: 713, ano: 2017, dia: '01/02', autor: 'Elias de Souza Gonçalves', orientador: [9], linha: 2, arquivo: '2017-Elias.pdf', 
  titulo: 'Fusão de Sensores para Localização de um VANT de Asa Fixa em Ambiente com Restrições do GPS usando Cointegração', 
  resumo_pt: 'O trabalho discorre sobre o problema de navegação e localização de um veículo aéreo não tripulado (VANT) que traduz-se na determinação da estimativa das variáveis de estado do veículo, isto é, o problema consiste em determinar a posição, a velocidade e a atitude do VANT em relação a um sistema de coordenadas. A navegação segura de um veículo é importante para aplicações relacionadas à coleta de dados, rastreamento e diversas outras tarefas civis e militares que utilizam diferentes recursos para melhorar a estimativa das variáveis de estado. A solução do problema é abordada em três etapas: (i) elaboração e montagem do VANT, (ii) pré filtro dos sensores inerciais e (iii) fusão dos sensores. Primeiro escolheuse um aeromodelo de asa fixa que possui todas as superfícies de controles tradicionais, de baixo custo e com modelos disponíveis para simuladores de voo. Na segunda etapa, aplicou-se o método de cointegração em um conjunto de sensores inerciais composto por dois acelerômetros e dois girômetros. O teste de cointegração serve como pré filtro, pois é uma técnica analítica para verificar tendências comuns em série multivariada e modelagem dinâmica de longo e curto prazo que, neste caso, permite descobrir a relação entre as medições dos sensores e utilizá-la para que a medição de um sensor seja substituída pela de outro. Então, dos dois sensores inerciais, preserva-se o que apresenta melhores características ou a combinação das melhores características de ambos. Desse modo, a contribuição inerente deste trabalho está na utilização da cointegração como forma de estimar o comportamento dos sensores inerciais do VANT. Assim, na última etapa, utiliza-se uma ferramenta amplamente empregada para a predição de variáveis de estado, o Filtro de Kalman Estendido (FKE), para realizar a fusão dos sensores da etapa anterior com o GPS, para eliminar imprecisões. Como metodologia de validação, propõe-se uma arquitetura de software in the loop que compreende um ambiente simulado composto pelo simulador de voo FlightGear, esta ção de controle em solo APM Planner 2 e da plataforma aérea Dronekit que fornece, através do DroneKit-Python, uma gama de rotinas e padrões para comunicação entre os módulos da simulação com o protocolo MAVLink encapsulado em Python. A principal vantagem desta metodologia é a possibilidade de aplicá-la com confiabilidade também nos experimentos reais, cenário em que o simulador de voo não é necessário. Para os experimentos reais, realiza-se somente a troca da string de conexão no Dronekit-Python de modo a estabelecer comunicação em tempo real com o VANT, que executa o software de piloto automático para veículos aéreos não tripulados de asa fixa, ArduPlane. Diante disso, elaborou-se três missões de voo para o aeromodelo, sendo um experimento simulado e dois experimentos reais com objetivos diferentes. A validação se deu mediante aplicação da arquitetura em voo simulado para o primeiro experimento, voo radiocontrolado com leitura de dados do voo pela estação de controle em solo, para o segundo experimento, e aplicação da arquitetura em voo real, no terceiro experimento. Os resultados mostram que as estimativas das variáveis de estado foram satisfatórias, mantendo-se sempre próximas daquelas consideradas verdadeiras e calculadas pelo software embarcado.', 
  resumo_en: '--'},

 {numero: 712, ano: 2016, dia: '09/12', autor: 'Ricardo Luis Dias Martins Ferreira', orientador: [2, 11], linha: 2, arquivo: '2016-Ricardo.pdf', 
  titulo: 'Um Método Prognóstico para Desenvolvimento Seguro de Aplicativo', 
  resumo_pt: 'Atualmente a plataforma Android domina o mercado de smartphones. Uma pesquisa realizada pela HP indicou que a maioria dos aplicativos para Android possuem vulnerabilidades. Isto ocorre porque os desenvolvedores, inconscientemente, introduzem códigos vulneráveis no aplicativo, e isto acarreta o vazamento de informações sensíveis e privadas do usuário quando uma vulnerabilidade é explorada. Vulnerabilidades inseridas em um código-fonte colocam as informações dos usuários em risco, uma vez que estas podem ocasionar o comprometimento de importantes aspectos de segurança, como a confidencialidade, a integridade e a disponibilidade. Contudo, a procura por vulnerabilidades em aplicativos tem sido objeto de estudo de diversos trabalhos que utilizam uma abordagem para detectar estas vulnerabilidades tardiamente, baseada no dex bytecode do aplicativo, quando este está pronto e disponível para download em lojas, o que pode ter afetado milhares de dispositivos. Este trabalho possui dois objetivos. Inicialmente será apresentada uma classificação das vulnerabilidades enumeradas em Top Ten Mobile Risks, relacionando-as com os aspectos de segurança da informação definidos pela ABNT NBR ISO/IEC 27.002:2013 e com os métodos Java que introduzem estas vulnerabilidades no sistema. Fundamentado nesta classificação o segundo objetivo apresentado é um método, baseado na técnica de análise estática com casamento de padrões, para identificar, analisar e avaliar códigos potencialmente vulneráveis, de forma antecipada, durante o desenvolvimento de aplicativos para Android. Este método foi utilizado para a criação da ferramenta appDroidAnalyzer, que foi usada para avaliar esta metodologia. Esta ferramenta permitirá ao desenvolvedor aprimorar o código-fonte do seu programa e evitar que brechas vulneráveis sejam inseridas neste. Esta técnica foi avaliada através de uma prova de conceito experimental em 859 aplicativos obtidos do catálogo de projetos open source F-Droid. Estes foram avaliados pela ferramenta appDroidAnalyzer que identificou 65 aplicações com potenciais vulnerabilidades em seu código-fonte.', 
  resumo_en: 'Android platform dominates the smartphone marketshare. A survey performed by HP indicated that the most of Android applications have vulnerabilities. This occurs because the developers unknowingly introduce vulnerable code into the application, which leads to leakage of sensitive and private user information when a vulnerability is exploited. User\'s information are put at risk when vulnerabilities are inserted into a source code as it can lead to compromising important security aspects such as confidentiality, integrity, and availability. However, the search for vulnerabilities in applications has been the subject of many studies that use the approach of late detection of vulnerabilities, based on the application\'s dex bytecode, when it is ready and available for download in stores, which may have affected thousands of devices. This study has two objectives. Initially, a classification of the vulnerabilities listed in Top Ten Mobile Risks will be presented, relating them to the information security aspects defined by ABNT NBR ISO/IEC 27.002: 2013 and the Java methods that introduce these vulnerabilities into the system. Based on this classification, the second objective presented is a method, based on the technique of static analysis with pattern matching, to identify, analyze and evaluate potentially vulnerable codes, in advance, during the development of Android applications. This method was used to create the appDroidAnalyzer tool, which was used to evaluate this methodology. This tool will allow the developer to enhance the source code of the program and prevent vulnerable loopholes from being inserted into it. This technique was evaluated through an experimental concept proof in 859 applications obtained from the F-Droid open source project catalog. They were evaluated by the appDroidAnalyzer tool, which identified 65 applications with potential vulnerabilities in their source code.'},

 {numero: 711, ano: 2016, dia: '07/12', autor: 'Richard Paúl Arias Tapia', orientador: [12], linha: 2, arquivo: '2016-Richard.pdf', 
  titulo: 'Metodologia para Análise de Resiliência em Redes Militares de Computadores', 
  resumo_pt: 'As redes de computadores estão presentes em todas as atividades do cotidiano, além das aplicações conhecidas no mundo civil e estão cada vez mais presentes em ambientes militares de Comando e Controle (C2) para defesa civil e situações de conflito. Um dos principais problemas relacionados à resiliência das topologias de C2 é determinar a capacidade de se defender e manter um nível aceitável de serviço ante a presença de falhas. Este trabalho propõe uma Metodologia para a Análise de Resiliência em Redes Militares de Computadores a través de uma métrica que leva em consideração aspectos da topologia, comportamento de tráfego e um critério probabilístico. Como esta métrica utiliza as demandas de tráfego existentes na rede, é fundamental considerar a otimização do roteamento, para o qual se propõe um algoritmo de modificação de custos dos enlaces sobre o protocolo de roteamento OSPF. Combinando a métrica com o algoritmo de modificação de custos atinge-se o objetivo de melhorar o nível de resiliência da rede. A Metodologia foi avaliada em topologias militares reais apresentando um aumento na robustez da rede tornando-a menos vulnerável a falhas.', 
  resumo_en: 'Computer networks are present in all daily activities, besides the known applications in the civilian world and they are increasingly present in military environments of Command and Control (C2) for civil defense and situations of conflict. One of the key problems related to resilience topologies is to determine the ability to defend and maintain an acceptable level of service in the presence of failures. This paper proposes a Methodology for Analysis of Resilience of a Military Computer Network through a metric that takes into consideration aspects of topology, traffic behavior and a probabilistic approach. As this metric uses the demands of existing network traffic is essential to consider the optimization of routing, for which a modification algorithm cost links in OSPF routing protocol is proposed. Combining the metric with the algorithm of modification of costs to improve the resilience of the network level is reached. The Methodology was evaluated in real military topologies showing an increase in the robustness of the network making it less vulnerable to failures.'},

 {numero: 710, ano: 2016, dia: '07/12', autor: 'Carlos Pedro Monaco Tullio Muniz', orientador: [13], linha: 3, arquivo: '2016-Carlos.pdf', 
  titulo: 'Investigando a Utilização de Atributos Temporais no Problema de Predição de Links', 
  resumo_pt: 'Nos últimos anos, um considerável aumento de atenção tem sido dado para a pesquisa sobre a tarefa de predição de links (LP) em redes complexas. Este problema tenta prever uma associação entre dois nós não conectados numa rede. Embora existam muitas tentativas para solucionar esta tarefa através de diferentes abordagens, nenhuma delas tem discutido sobre a combinação entre o comportamento temporal e as informações que são tratadas na rede. Neste trabalho foram introduzidas três novas estratégias para a tarefa de predição de links que combinam informações temporais topológicas e contextuais da rede. A estratégia que combina o tempo com a topologia da rede (TEMTOP) apresenta um novo método para ponderar as arestas da rede baseado nas informações temporais. A estratégia que combina informações temporais, topológicas e contextuais (TEMCONTOP), apresenta um novo método para ponderar as arestas da rede baseado nas informações temporais e contextuais. E por fim, a estratégia que combina as informações temporais e contextuais (TEMCON), define um novo método chamado Context Based Time Score (CTS). Foram realizadas análises sobre a performance destas estratégias em diferentes configurações e data sets. Os resultados revelam uma melhoria significante para a tarefa de predição de links.', 
  resumo_en: 'In the last years, a considerable amount of attention has been devoted to the research about the link prediction (LP) problem in complex networks. This problem tries to predict the likelihood of an association between two not interconnected nodes in a network to appear in the future. Although there have been a lot of attempts to solve the (LP) problem through different approaches, none of them have discussed about the temporal behavior and the information treated in the network. In this paper three new strategies have been introduced for the link prediction task combining topological, temporal, and contextual information from the network. The strategy that combines time with the network topology (TEMTOP) presents a new method to weigh the edges of the network taking the temporal information into account. Finally, the strategy that combines the temporal and contextual information (TEMCON) defines a new method called Context Based Time Score (CT S). The effectiveness of these strategies has been analyzed under different configurations and data sets. The results have revealed a significant improvement in predicting future links. '},

 {numero: 709, ano: 2016, dia: '06/12', autor: 'Silas Pereira de Lima Filho', orientador: [8], linha: 3, arquivo: '2016-Silas.pdf', 
  titulo: 'Heurísticas para Migração de Dados do Modelo Relacional para o Modelo em Grafo', 
  resumo_pt: 'A importância de trazer dados do modelo relacional para outros modelos e tecnologias tem sido amplamente debatidos, como por exemplo a publicação de dados como grafos RDF. Este modelo facilita a interoperabilidade de dados enquanto garante o uso uniforme de vocabulários para descrever dados. Além disso, esse modelo permite executar análises topológicas sobre os grafos, tal como ocorre nas análises de redes sociais, predição de ligações sistemas de recomendação. Existem inclusive, iniciativas para mapear de um banco de dados relacional para a representação em grafo. No entanto, eles não consideram as diferentes maneiras de gerar esses grafos, especialmente quando o objetivo é realizar análises topológicas. Este trabalho propõe um método de suporte para mapear os dados do modelo relacional para a representação em grafos. A principal contribuição é que a escolha do modelo do grafo deva considerar o tipo de análise topológica que será realizada pelo usuário. Experimentos são apresentados e mostram resultados intressantes, incluindo heurísticas e a confirmação de resultados em conjuntos mais complexos.', 
  resumo_en: 'The importance of bringing the relational data to other models and technologies has been widely debated, as for example their publication as RDF graphs. This model facilitates data interoperability in the meantime it grants the use of uniform vocabularies to describe the data. Moreover, it allows to perform topological analysis over these graphs, such as social analysis, link predictions or recommendations. There are already initiatives to map from a relational database to graph representation. However, they do not take into account the different ways to generate such graphs, specially when the goal is to perform topological analysis. This work shows a supporting method for data mapping from the relational model to graph representation. The main contribution is that the graph model choice must consider the topological analysis intended. Experiments are reported and show interesting results, including modeling heuristics and the result endorsement with more complex data.'},

 {numero: 708, ano: 2016, dia: '02/12', autor: 'Victor Dias de Oliveira', orientador: [10], linha: 2, arquivo: '2016-Victor.pdf', 
  titulo: 'Alocação de Ambientes Virtuais Baseados em Perfis através do Monitoramento e Aprendizado de Padrões de Consumo de Recursos em CMPD', 
  resumo_pt: 'A evolução das ciências Humanas, Médicas e Tecnológicas motivou o desenvolvimento de pesquisas nas respectivas áreas, com o intuito de proporcionar contribuições sociais, econômicas e para a indústria. Tais pesquisas geralmente envolvem simulações complexas, com elevada quantidade de dados a serem processados. A demanda por capacidade de processamento e por armazenamento de dados torna necessário o uso de infraestruturas computacionais de alto desempenho que permitam o desenvolvimento, a implantação e a execução das soluções implementadas, na forma de aplicações científicas, de maneira a obter resultados em tempo hábil. No entanto, é comum que as infraestruturas computacionais com alta capacidade de processamento e armazenamento permaneçam ociosos por períodos de tempo. Isso pode ocorrer devido à uma redução temporária da demanda, ou por uma taxa de utilização reduzida de um determinado recurso, tal como CPU, memória ou acesso a dispositivos de entrada e saída (I/O), por uma aplicação. Modelos de negócio, como a Computação em Nuvem, políticas de acesso aos recursos e a ordem em que aplicações devem ser executadas são alguma das alternativas propostas pela literatura para aumentar a taxa de uso de recursos computacionais. Com o objetivo de abordar o aumento do uso dos recursos computacionais, no contexto de Computação em Nuvem, esta dissertação de mestrado apresenta o desenvolvimento de um escalonador de máquinas virtuais com comportamentos estático, no qual há um conhecimento prévio sobre o perfil de uma aplicação, e dinâmico, no qual há a análise do perfil da aplicação e a tomada de decisões sobre o escalonamento em tempo de execução. O algoritmo de escalonamento desenvolvido visa permitir que máquinas virtuais que executam aplicações classificadas como Computação Massivamente Paralela e Distribuída (CMPD) executem simultaneamente e de forma harmoniosa em uma mesma infraestrutura, compartilhando os recursos de CPU, memória principal e I/O. Com o propósito de permitir que o escalonamento fosse realizado sobre os recursos compartilhados, foi aplicado o conceito de Afinidade entre as Aplicações, com a finalidade de acomodar o uso de diferentes recursos computacionais ociosos por diferentes aplicações, em um mesmo instante de tempo. Para viabilizar a implementação do algoritmo de escalonamento, foi desenvolvido e aplicado o monitoramento online, com o objetivo de obter o histórico de consumo de recursos de uma máquina virtual para obter o seu perfil, além de realizar o monitoramento online da intensividade de uma aplicação ao longo de sua execução, isto é, quantificar o uso do recurso mais usado por uma aplicação. Como contribuições, esta dissertação pretende estimular a pesquisa e o desenvolvimento de algoritmos de escalonamento de aplicações científicas em nuvens computacionais, com o foco na melhoria do aproveitamento dos recursos através da alocação de máquinas virtuais baseada na afinidade e no perfil de consumo de recursos. Almeja-se com isso, minimizar o impacto sobre o desempenho das aplicações, além de reduzir o makespan de um fila de execução. Os resultados obtidos indicam que a análise das particularidades das aplicações contribui para melhorar o aproveitamento dos recursos e reduzir os problemas causados em função da concorrência.', 
  resumo_en: 'The evolution of Human, Medical and Technology sciences motivated researches in these fields with the objective of achieve industrial, social and economic contributions. Such researches usually involve complex simulations, with a large volume of data to be processed. The demand for data storage and processing implies the use of high performance computing infrastructures, in order to develop, to consolidate and to run implemented solutions as scientific applications, to obtain accurate results in a timely manner. However, it is a common thing for high performance computing infrastructures to stay idle for periods of time. This might be caused by a temporary reduction of the overall demand, or by the low usage of certain resources by an application, such as CPU, memory or I/O devices. Business models, such as Cloud Computing, resource access policies and the orders that applications are executed are some alternatives proposed by the known literature, in order to improve the usage rate of computing resources. In order to address the increase of resources usage rate for Cloud Computing infrastructures, this Masters dissertation presents de development of a scheduler of static and dynamic behavior virtual machines. A static behavior is defined as one in which there is previous knowledge about an application profile. A dynamic behavior is that in which the analysis of an application profile and the decision based in it is made during runtime. The scheduling algorithm that was developed allows virtual machines that host Massively Distributed and Parallel Computing applications to run concurrently and harmoniously, sharing CPU, memory and I/O devices. In order to schedule applications that share resources, it was used the concept of Application Affinity to allow different applications to share different idle computing resources, at the same time. In order to enable the proposed scheduling algorithm, it was necessary to develop and to use a proactive tool, that is responsible for the online monitoring of computing resources consumption by the virtual machines and to define its profile, based on historical data. As its main contributions, this Masters dissertation intends to encourage the reasearch and the development of scientific applications scheduling algorithms, with the focus on improving the usage of computing resources through the affinity based virtual machine allocation and on the resource consumption profile. By doing so, an impact minimization over application performance, due to resource sharing, is expected to be achieved, as well as a reduction of the makespan. The results indicate that the analysis of applications\' particularities may improve the resource usage and to reduce problems caused by application concurrence.'},

 {numero: 707, ano: 2016, dia: '28/11', autor: 'Julio Cesar Cardoso Tesolin', orientador: [8], linha: 3, arquivo: '2016-Julio.pdf', 
  titulo: 'A Caracterização das Fontes de Dados na Escolha de Abordagens de Integração em Ambientes Big Data', 
  resumo_pt: 'A era do Big Data é a consequência inevitável de nossa capacidade de gerar e coletar dados. Por consequência, as fontes de dados apresentam um comportamento mais dinâmico. Neste novo ambiente, o desafio de gerenciar o processo de integração de dados e o tráfego de rede entre as fontes de dados e os sistemas consumidores foi exacerbado pela quantidade de fontes e pelo volume de seu conteúdo, pela variedade de suas estruturas e de seus formatos e pela velocidade de surgimento de novas fontes para consumo. Para enfrentá-los, uma sistemática de escolha e adaptação de abordagens de integração é desenvolvida, utilizando para isso a caracterização das fontes de dados, com o objetivo de minimizar a intervenção humana no processo de integração e reduzir o tráfego nas redes de comunicação que conectam os entes integrados. Esta sistemática é comparada com uma abordagem de materialização das fontes de dados no sistema de integração, utilizando um procedimento inspirado pelo TPC-DI como base para medição.', 
  resumo_en: 'The Big Data era is an inevitable consequence of our capacity to generate and collect data. Because of that, data sources got a more dynamic behavior. In this new environment, the challenge to manage the data source\'s integration process and the network traffic between data producers and consumers has been overwhelmed by the number of data sources and their content\'s volume, by the variety of their structures and formats and by the velocity of their appearance. To face these issues, this work presents a new method to help the users choosing data integration\'s approaches based on data sources characteristics. The goal is to reduce both human intervention in the integration process and the network traffic between the integrated peers. In order to evaluate the method, an integration environment was implemented based on TPC-DI sources and procedures.'},

 {numero: 706, ano: 2016, dia: '04/10', autor: 'Daisy Cristine Albuquerque da Silva', orientador: [12], linha: 2, arquivo: '2016-Daisy.pdf', 
  titulo: 'Análise da Eficiência dos Algoritmos de Aprendizado de Máquinas para Detecção de Botnets', 
  resumo_pt: 'O crescente número de ataques cibernéticos contra organizações públicas e privadas, além de usuários comuns, tem sido relatados nos últimos anos, trazendo à tona a preocupação do emprego desses ataques em ações de Guerra Cibernética. Em algumas dessas ações foram empregadas estruturas conhecidas como botnets. As botnets são redes formadas por hosts escravos, denominados bots, que são controladas por um ou mais atacantes, denominados botmasters, que tem por objetivo realizar algum tipo de ação maliciosa (ZEIDANLOO et al., 2010). Esse trabalho tem o objetivo de analisar a eficiência dos algoritmos de aprendizado de máquina na detecção de botnet. Nesse contexto, foi estruturada uma metodologia para detecção de botnet, onde após a captura e o pré-processamento do tráfego de rede, são aplicados algoritmos de aprendizado de máquina que classificam os dados em maliciosos ou não maliciosos de acordo com o conhecimento adquirido na fase de treinamento. A metodologia está divida em 3 fase. Na 1a fase, foram levantas e selecionadas as features do tráfego de rede relevantes à detecção de botnet, empregando técnicas de seleção de atributos. Na 2a fase, os algoritmos de aprendizado de máquina são treinados utilizando a base de dados da Czech Technical University (CTU) (GARCÍA et al., 2014) com as marcações Normal, Background e Botnet. As ferramentas Weka e LibSVM foram utilizadas como biblioteca de implementação a serem incorporadas na arquitetura. E para medir o desempenho dos algoritmos foram utilizadas métricas como acurácia, precisão e sensitividade (recall ), assim como as métricas eficiência (F-measure) e a área da curva ROC, que são métricas mais adequadas para bases desbalanceadas. E por último, na 3a fase, foi abordada a estruturação do algoritmo de aprendizado de máquina selecionado no raciocinador da Arquitetura de Sistema Integrado de Defesa Cibernética (SILVA; SALLES, 2012).', 
  resumo_en: 'The growing number of cyber attacks against public and private organizations, and ordinary users, have been reported in recent years, bringing up concerns the use of these attacks Cybernetics War actions. In some of these actions known structures were employed as botnets. The botnets are networks of host slaves, called bots, which are controlled by one or more attackers, called botmasters, which aims to perform some sort of action malicious (ZEIDANLOO et al., 2010). This work aims to analyze the efficiency of machine learning algorithms in botnet detection. In this context, a methodology for botnet detection was structured, where after the capture and preprocessing network traffic are applied machine learning algorithms to classify data on malicious or non-malicious according to knowledge gained in phase training. The methodology is divided into 3 stages. In Stage 1, they get up and selected the features of network traffic, using attribute selection techniques. In the 2nd stage, the machine learning algorithms are trained using the database Czech Technical University (CTU) (GARCÍA et al., 2014) with Normal markings, Background and Botnet. The Weka and LIBSVM tools were used as implementation library to be incorporated in architecture. And to measure the performance of the algorithms were used metrics such as accuracy, precision and sensitivity (recall) as well as the metrics F-measure and the area under the ROC curve, which are more suitable metrics for unbalanced bases. Finally, the 3rd stage was approached structuring the selected machine learning algorithm in the reasoner of the Integrated Architecture Cyber Defence (SILVA; SALLES, 2012).'},

 {numero: 705, ano: 2016, dia: '25/05', autor: 'Yuri de Freitas Waki', orientador: [5], linha: 2, arquivo: '2016-Yuri.pdf', 
  titulo: 'Alpha Cypher Revisitado', 
  resumo_pt: 'Durante o trabalho, o conceito de criptografia verde é aplicado para que se possa alterar os componentes de um algoritmo de criptografia. Porém, ao realizar-se qualquer alteração em um algoritmo, essa alteração precisa ser validada. Para que as alterações sejam validadas com eficiência são estabelecidas métricas para as funções modificadas. Neste trabalho, são analisadas funções de expansão de chaves, s-boxes e o tamanho do bloco do algoritmo. Essas análises são feitas focando em meios objetivos de se medir a qualidade da função em questão. Os resultados da análise são então aplicados a um estudo de caso: o cifrador simétrico de blocos chamado Alpha, proposto por Jorge Lambert em 2004 no IME. Por fim, o algoritmo como um todo é validado com a utilização de testes estatísticos.', 
  resumo_en: 'During this work, the concept of Green Cryptography is applied so that the components of a cryptography algorithm can be modified. However, any changes made in the algorithm must be validated. For the changes to be validated effciently, metrics are stabilished for each of the modified functions. In this research, key expansion, s-boxes and block size are analyzed. These analyzes focus on objective ways to measure the quality of the studied function. The analysis results are then applied to a case study: the symmetric cipher blocks named Alpha, proposed by Jorge Lambert in 2004. Finally, the algorithm as a whole is validated using statistical tests.'},

 {numero: 704, ano: 2016, dia: '25/05', autor: 'Leonardo Cardia da Cruz', orientador: [4], linha: 2, arquivo: '2016-Leonardo.pdf', 
  titulo: 'Um Ambiente Virtual Colaborativo CAVE/desktop para Treinamento em uma Plataforma de Petróleo', 
  resumo_pt: 'Ambientes Virtuais Colaborativos permitem que múltiplos usuários possam participar simultaneamente do mesmo cenário e interagir entre si em tempo real. Com a utilização de um ambiente virtual colaborativo, diversas simulações de treinamento podem ser realizadas, onde usuários em conjunto, devem realizar com êxito uma tarefa e até mesmo um avaliador através do cenário virtual, interagir com o participante. O Emprego desse tipo de ambiente em simulações de treinamento acaba permitindo que o treinamento seja realizado repetidas vezes, aumentando assim o aprimoramento sobre aquela tarefa, bem como diminuir os danos que um erro pode causar. O objetivo deste trabalho é apresentar um ambiente virtual colaborativo entre o ambiente CAVE e um computador desktop, para treinamento em uma plataforma de petróleo. Através do ambiente de imersão CAVE é realizado o treinamento, por meio deste, o participante consegue explorar a plataforma através do mouse 3D e realizar determinadas tarefas. A partir do computador desktop, outro usuário acompanha cada passo do participante por intermédio de um avatar, além de poder aplicar determinados exercícios de treinamentos. O desenvolvimento deste trabalho foi realizado através da implementação baseada na arquitetura Instant Reality e no padrão X3D.', 
  resumo_en: '--'},

 {numero: 703, ano: 2016, dia: '23/05', autor: 'Carina da Cruz Teixeira', orientador: [4], linha: 2, arquivo: '2016-Carina.pdf', 
  titulo: 'Sistema de Reabilitação Háptica', 
  resumo_pt: 'A Realidade Virtual está presente na área da saúde como ferramenta para treinamentos diversos e para permitir a realização de tratamentos de forma diferenciada. Os profissionais da área da fisioterapia utilizam tais recursos visando a realização de exercícios de reabilitação e para estimular os pacientes a recuperarem os movimentos e controle da destreza fina, como forma de mantê-los motivados. Para isso, são desenvolvidos ambientes virtuais em forma de jogos ou atividades lúdicas integrados a dispositivos específicos, utilizados para permitir a interação através do uso de partes do corpo tornando a simulação mais realista. Como exemplo o sistema CyberForce/ CyberGrasp/CyberGlove que consiste de um dispositivo de interação que permite identificar os movimentos e aplicar força ao braço e à mão. Com este sistema é possível simular toque e peso de objetos virtuais, além da capacidade de controlar os movimentos realizados pelo usuário. Neste contexto, o objetivo desta dissertação é apresentar o desenvolvimento de uma aplicação tátil utilizando o dispositivo acima descrito empregado à área de reabilitação, que consiste de um ambiente virtual de exercícios auxiliar ao processo de reabilitação de pacientes vítimas de afecções degenerativas, alterações neurológicas e traumas que ocasionaram lesões na mão e no braço. O sistema desenvolvido visa permitir que o usuário exercite o controle dos movimentos por meio de um exercício que foca no controle da força ao segurar determinado objeto e estimula a abertura da mão para soltá-lo, através de algoritmos específicos que identificam as ações do usuário e seu desempenho.', 
  resumo_en: 'For a while Virtual Reality has been used for training of procedures in health applications. It has also been used, more recently, as a mean for treatment of patients. Physiotherapists use it aiming at aiding in rehabilitation exercises, as a stimulus to the otherwise boring repetitive exercises which aim at recovering fine movements and dexterity of an injured limb. All that is accomplished through a game-like virtual environment which encourages the patient to perform the effort that will ultimately lead to his/her recovery. In order to make that possible, a number of tactile devices are used, for which an example is the CyberForce/CyberGrasp/CyberGlove system. The system allows one to both keep track as well as return some force feedback to the user arm, hard and fingers, henceforth simulating touch and actual interaction with virtual content. This work introduces a tactile application that uses the above-mentioned device which was developed with a focus on rehabilitation of the user fine hand/arm/finger movements. The user is considered to be someone who suffered neurological alterations, degenerative conditions or traumas that reduced the control the patient has on his/her arm/hand/fingers. The software developed enables exercises that aid the user to recover the otherwise reduced control of his hand, henceforth improving his quality of life.'},

 {numero: 702, ano: 2016, dia: '20/05', autor: 'Yasmmin Côrtes Martins', orientador: [8], linha: 3, arquivo: '2016-Yasmmin.pdf', 
  titulo: 'YLINK: Um Processo para Interligação de Dados na Web de Semântica', 
  resumo_pt: 'A web semântica vem sendo adotada como meio de troca de informação por uma série de domínios de aplicação ao longo dos últimos anos. Um dos domínios que mais possuem documentos e dados publicados é o biomédico. Há um grande esforço da comunidade científica em fazer com que estes documentos/dados sejam publicados de acordo com boas práticas, como o uso de padrões para facilitar a interoperabilidade. Por exemplo, o uso do formato RDF e a publicação dos dados como recursos identificados unicamente por URIs derreferenciáveis. Além disso, recomenda-se também que estes recursos estejam interligados com outras fontes de documentos/dados. Interligar um novo conjunto de dados com os que já estão publicados na web de dados é uma tarefa árdua que depende de mecanismos de mapeamento e de métricas para verificar a similaridade entre eles. No entanto, os mecanismos de mapeamento automáticos apresentam problemas com relação a precisão, e embora ajudem na realização desta tarefa, resolvem apenas uma parte do problema. Esta tarefa envolve outros desafios além do mapeamento, como encontrar fontes de dados com as quais interligar, e validar os mapeamentos sugeridos por aqueles mecanismos. Este trabalho apresenta um processo que abarca as principais atividades para realizar a interligação de dados. Tais atividades passam pela seleção e ranqueamento de datasets alvos; mapeamento do dataset fonte com os datasets alvos; validação humana dos pares encontrados e a integração destes pares validados com o dataset fonte. Tendo como resultado final um processo eficiente e intuitivo. Foi implementada uma ferramenta de software que materializou os conceitos do processo acima de acordo com cada atividade, e viabilizou a sua avaliação. Experimentos foram feitos seguindo o escopo de cada atividade, apresentando resultados satisfatórios e confirmando a aplicabilidade do processo proposto.', 
  resumo_en: 'The semantic web has been adopted as a way to information exchange by a serie of domain applications along the last years. One of the domains which most owns douments and published data is the biomedical one. There is a big effort from the scientific communiy to do that these documents/data be published according to the best practices, as the use of patterns to facilitate the interoperability. For example, the use of RDF format and data publication as resources identified uniquely by derreferenceable (those which returns valid content also using RDF) URIs. Futhermore, it is also recommended that these resources be interlinked with other document/data sources. Interlinking a new dataset with those already published in the web of data is a hard task which depends on mapping mechanisms and metrics to verify the similarity between these datasets (the new and the others already published). However, the automatic mapping mechanisms present problems in relation to precision, and although they help in doing this task, they solve a part of the problem. This task involves other challenges beyond the mapping, like finding data sources to link to, and validating the mappings suggested by those mechanisms. This work presents a process that embraces the main activities to achieve the data interlinking. Such activities pass by selection and ranking of target datasets; mapping between source dataset and the target ones; human validation of the found pairs and the fusion of these validated pairs with the source dataset. A software tool was implemented and it materialized the concepts of the process above according to each activity, and made possible its evaluation. Experiments in the biomedical domain were made following each activity\'s scope, showing satisfatory results and confirming the aplicability of the proposed process.'},

 {numero: 701, ano: 2015, dia: '27/08', autor: 'Felipe Juliani Fernandes', orientador: [10], linha: 1, arquivo: '2015-FelipeFernandes.pdf', 
  titulo: 'Um Método de Escalonamento Baseado no Comportamento de Aplicações HPC para Nuvens Computacionais Balanceando Desempenho e Eficiência Energética', 
  resumo_pt: 'Neste trabalho é apresentado o processo de desenvolvimento e validação de um método de escalonamento de VMs em nuvens computacionais focadas em HPC. O escalonador desenvolvido leva em consideração o tipo de carga de trabalho que as VMs irão executar (CPU-bound ou IO-bound) para decidir quando e em qual servidor as mesmas serão alocadas. O escalonador é capaz de reduzir o consumo energético do ambiente e a ocorrência de violações de SLA, sem reduzir o desempenho da VMs, ao alocar de maneira simultânea VMs que executam tarefas CPU-bound e outras que executam tarefas IO-bound de forma a consolidar recursos, aproveitando cada componente de hardware dos servidores para seus devidos fins. Antes da validação de desempenho do escalonador, mediante uso do simulador CloudSim, foram conduzidos testes de desempenho com benchmarks sintéticos, representativos de cada tipo de aplicação. Assim, foi possível implementar um método que utiliza uma política de alocação de VMs baseada nas particularidades de desempenho identificadas durante os experimentos. Os resultados obtidos indicam que analisar as especificidades de infraestruturas e aplicações contribui para melhorar o aproveitamento dos recursos, aumentar os níveis de oferta de serviço e reduzir os problemas causados pelo uso dos recursos de forma concorrente.', 
  resumo_en: '--'},

 {numero: 700, ano: 2015, dia: '26/08', autor: 'Fábio Santos de Araújo', orientador: [2], linha: 2, arquivo: '2015-Fabio.pdf', 
  titulo: 'Uma Métrica para Classificação de Vulnerabilidade Baseada na Topologia da Rede', 
  resumo_pt: 'A principal dificuldade encontrada pelos usuários de software ao consultar um relatório com as vulnerabilidades encontradas é saber o risco caso estas sejam exploradas. Os repositórios trazem informações genéricas, não levando em consideração o escopo ou particularidades. Muitos usuários utilizam as informações obtidas priorizando sem um padrão definido, de acordo com os seus próprios conhecimentos. Este trabalho apresenta uma métrica baseada na topologia da rede, onde as vulnerabilidades são classificadas de acordo com o cenário apresentado da rede em produção e um ativo considerado como principal pelo usuário. Tem como base a associação da rede a uma estrutura de árvore n-área através de um algoritmo sugerido na pesquisa, o repositório de vulnerabilidade NVD de onde serão retirados os dados importantes e a métrica a fim de ponderar a classificação feita pelo relatório retirado do portal do NVD.', 
  resumo_en: '--'},

 {numero: 699, ano: 2015, dia: '14/05', autor: 'Suzana Mattos da Costa', orientador: [2], linha: 2, arquivo: '2015-Suzana.pdf', 
  titulo: 'Classificação de Tráfego Malicioso em Redes de Computadores Utilizando Aprendizado de Máquina', 
  resumo_pt: 'Ataques distribuídos de negação de serviço tem por objetivo indisponibilizar um serviço impedindo ou dificultando o seu acesso. O ataque consiste basicamente do envio de uma grande quantidade de pacotes ao alvo ou a exploração de alguma vulnerabilidade de seus protocolos e serviços, a fim de exaurir seus recursos. Sua origem distribuída faz com que seja extremamente difícil impedir que ataques dessa natureza acorram, o que torna fundamental o desenvolvimento de técnicas de detecção, identificação, prevenção e mitigação dos ataques assim que estes comecem a ocorrer, a fim de garantir tempo hábil para que as medidas de segurança necessária sejam acionadas. Neste trabalho é apresentada uma metodologia para detecção e identificação de ataques DDoS, onde após a captura e o pre-processamento dos dados da rede, são aplicados algoritmos de aprendizado de máquina que realizam a classificação dos dados em múltiplas classes. Para as etapas de treinamento e avaliação da metodologia proposta, foi simulado um ambiente de redes de computadores com geração de tráfego normal, gerado por usuários legítimos da rede, e para criação do tráfego malicioso foram utilizadas ferramentas de ataques DDoS. E para medir o desempenho alcancado pelo método, foram utilizados as métricas de precisão, abrangência e acurácia média que alcançou 97,56% para classificação do tráfego entre dados normais e anômalos.', 
  resumo_en: '--'},

 {numero: 698, ano: 2015, dia: '06/05', autor: 'Geraldo de Souza Junior', orientador: [3], linha: 1, arquivo: '2015-Geraldo.pdf', 
  titulo: 'Recomendação em Redes Sociais baseada em Análise de Grafos', 
  resumo_pt: 'A interação social desempenha um importante papel na vida de todos hoje em dia, permitindo as pessoas alcançarem seus desejos, não importando o que seja: um produto que deseja comprar, um contato profissional, a retomada de uma amizade, entre outros. Nesse contexto, podemos receber recomendações que vão ajudar a cumprir nossos objetivos sociais, melhorando não só a nossa experiência como usuários mas também a experiência de outros. Neste trabalho, apresentamos uma técnica de recomendação de ligações entre itens baseada em uma abordagem mista entre as filtragens colaborativas e de conteúdo, e a análise de redes sociais. A partir de um item raiz, um grafo de relacionamentos é gerado e utilizado para a extração de métricas na rede. A similaridade do item raiz com os demais itens presentes no grafo é calculada e combinada com as métricas obtidas. Para demonstrar a metodologia, utilizamos o estudo de caso de uma rede de co-autorias, construída a partir de informações extraídas do Microsoft Academic Search (MAS), o motor de busca acadêmico gratuito da Microsoft, utilizando sua API oficial. A combinação da similaridade com as métricas de análise de rede social do subgrafo de relacionamento dos itens possibilita a configuração de algoritmos de recomendação de acordo com diferentes semânticas de utilização.', 
  resumo_en: '--'},

 {numero: 697, ano: 2015, dia: '29/04', autor: 'Diego Augusto Thomaz Quadrado Leite', orientador: [6], linha: 1, arquivo: '2015-Diego.pdf', 
  titulo: 'Aplicação de Técnica de Reconhecimento de Gestos Manuais Baseado em Aprendizado de Máquina para Interação Homem-Máquina em Ambientes Imersivos do Tipo CAVE', 
  resumo_pt: 'O campo de Realidade Virtual (RV), mais precisamente na área de ambientes imersivos, tem o objetivo de imergir o usuário dentro de um mundo sintético através do estímulo de seus sentidos. Quanto mais próximo da realidade for a interação, maior a imersão do usuário. Uma forma de interação no mundo real é a utilização de gestos. Utilizar gestos como forma de comunicação é um comportamento comum no cotidiano dos seres humanos. Na Interação Homem-Computador (IHC), este comportamento pode ser adotado para a construção de interfaces alternativas, tornando a interação entre o usuário e o mundo sintético mais natural e intuitiva. Este trabalho apresenta um framework de reconhecimento de gestos, em tempo real, baseado em aprendizado de máquina que combina imagens de infravermelho e mapas de profundidade para interação com aplicações CAVE. É apresentada também uma abordagem própria de tracking das mãos. Técnicas de Gesture Spotting são utilizadas basicamente para eliminar classicações erradas. Uma gramática baseada nas classes de gestos das mãos é utilizada para interpretar os resultados das classicações em comandos de controle para a aplicação CAVE. As abordagens de reconhecimento de gestos das mãos e geração/execução de comandos CAVE são compostas por um plugin cliente-servidor, o qual é parte do sistema CAVE implementado com base na arquitetura InstantReality e no padrão X3D. Uma aplicação teste foi desenvolvida para validar os resultados da metodologia proposta. Os resultados mostram que o plugin implementado é uma solução promissora. Alcançamos uma acurácia de reconhecimento adequada e uma manipulação de objetos eciente em um cenário virtual dentro da CAVE, intensicando assim a imersão do usuário dentro do mundo sintético. Todos os resultados se mostraram invariantes à iluminação do ambiente e permitiram uma maior liberdade de movimentação dentro da CAVE.', 
  resumo_en: '--'},

 {numero: 696, ano: 2015, dia: '15/04', autor: 'Felipe Macedo Castro', orientador: [12], linha: 1, arquivo: '2015-FelipeCastro.pdf', 
  titulo: 'Metodologia para Avaliar a Segurança de um Ambiente contra SPAM Malicioso', 
  resumo_pt: 'Esta proposta visa contribuir com a Segurança da Informação, avaliando, classificando e orientando na proteção dos ativos contra infecções oriundas de Spam malicioso, evitando problemas como: o não recebimento de emails, gasto desnecessário de tempo, aumento de custo, perda de produtividade, consumo desnecessário de link, investimento em equipamentos e pessoas, infecção por malware entre outros. Neste trabalho é proposta uma metodologia para avaliar a segurança de um ambiente contra Spam malicioso, proporcionando aos ambientes corporativos, uma forma de avaliar o quão sua infra estrutura está protegida contra esse tipo de ataque, podendo aderir a melhorias com base nos resultados obtidos. Com isso alcança-se resultados que sejam de significativa relevância a seguranca cibernética nacional, reduzindo a quantidade de ataques e volumes de Spam nas redes brasileiras, ajudando a tirar o Brasil da lista dos maiores ofensores.', 
  resumo_en: '--'},

 {numero: 695, ano: 2015, dia: '23/02', autor: 'Antonio Felipe Podgorski Bezerra', orientador: [6], linha: 2, arquivo: '2015-AntonioBezerra.pdf', 
  titulo: 'Metodologia BAUS: uma Abordagem Social para o Problema de Cold-start em Sistemas de Recomendação Baseada na Extração e Análise de Recurso Web', 
  resumo_pt: 'Em geral, um sistema de recomendação apoia pessoas na realização de escolhas entre diversas alternativas apresentadas, tentando maximizar as possibilidades de encontrar informações interessantes e valiosas que possam ajudar em sua tomada de decisão. Nos últimos anos, é claramente observado que a informação na web está crescendo exponencialmente, principalmente a partir de interações sociais, gerando uma sobrecarga informativa. No entanto, esta sobrecarga é proveniente de fontes heterogêneas e a forma como elas estão estruturadas fazem a sua extração e análise um processo complexo, motivando, dessa forma, diversas áreas de pesquisa atualmente ativas neste tema. Neste estudo é conceitualizada e desenvolvida a metodologia Baus. Esta metodologia é focada em uma abordagem social que possibilita utilizar o conhecimento proveniente de diferentes recursos web para expandir a matriz usuário-item, utilizada em sistemas de recomendação que empregam a técnica conhecida por filtragem colaborativa (FC), e melhorar o seu desempenho, tornando suas recomendações mais precisas em relação a itens em situação de cold-start, problema comumente observado em sistemas de recomendação baseados em FC. Tendo como motivação a sobrecarga informativa e o contexto limitado, no qual os sistemas de recomendação clássicos atuam, a abordagem metodológica proposta utiliza estas informações para criar modelos de usuários sociais, sem interferir na privacidade dos usuários que colaboram para a construção dos modelos, e expandir a matriz original. Este processo pode ser utilizado de igual maneira para melhorar a performance em sistemas de recomendação que possuam poucas avaliações em sua base de dados. Em um cenário real, os resultados observados nos experimentos demonstraram uma melhora na qualidade das predições e recomendações de itens em situação de cold-start, em torno de 70% e 119% respectivamente, se comparadas aos métodos clássicos de FC.', 
  resumo_en: '--'},

 {numero: 694, ano: 2015, dia: '12/02', autor: 'Raquel Lima Façanha', orientador: [8], linha: 1, arquivo: '2015-Raquel.pdf', 
  titulo: 'Um Método para Apoiar o Resgate do Compromisso Ontológico de um Esquema de Dados Conceitual Legado', 
  resumo_pt: 'Organizações governamentais produzem e disseminam uma grande quantidade de informação diariamente. O movimento de dados governamentais abertos tornou esses dados disponíveis para reúso e acessibilidade. No entanto, a mera publicação dos dados recuperados de sistemas legados n˜ao é suficiente para prover reúso e integração. As abordagens propostas para publicação de dados governamentais abertos fazem uso de tecnologias como XML, RDF e OWL, as quais n˜ao est˜ao aptas para representar o significado real pretendido de esquemas de dados conceituais de sistemas legados. Algumas abordagens prop˜oem a construção de ontologias de domínio a partir de esquemas legados. O resgate do compromisso ontológico dos esquemas legados permite enriquecer semanticamente os dados, preparando-os para publicação. Outras abordagens se apoiam em ontologias de fundamentação de forma a resolver conceitos ambíguos e alcançar melhor interoperabilidade semântica. No entanto, o resgate do compromisso ontológico de esquemas legados n˜ao é uma tarefa fácil. Este trabalho é uma das primeiras iniciativas no sentido de propor uma abordagem sistemática para identificação e categorização dos conceitos do domínio a partir de um esquema legado com o intuito de estabelecer um método para apoiar o resgate do compromisso ontológico de esquemas conceituais legados com o auxílio de ontologias de fundamentação. A abordagem definida e sua respectiva aplicação s˜ao ilustradas pela descrição de dois casos de estudo. Um deles no domínio Legal e outro na área administrativa. O método proposto consiste na seleção e no recorte do esquema legado, e posterior seleção e recorte da teoria ontológica de apoio. Um conjunto de algoritmos conduz o usuário na obtenção de correspondências entre os conceitos do domínio e os conceitos da teoria, sugerindo metacategorias (classificação do conceito do domínio segundo as categorias da ontologia de fundamentação) possíveis dentro do universo de discurso. A principal contribuição deste trabalho é que o método proposto pode auxiliar e facilitar a publicação de dados com riqueza semântica e, consequentemente, melhorar a interoperabilidade.', 
  resumo_en: '--'},

 {numero: 693, ano: 2015, dia: '10/02', autor: 'Antonio de Lemos Pecli da Silva', orientador: [8], linha: 2, arquivo: '2015-AntonioSilva.pdf', 
  titulo: 'Um Estudo Comparativo de Estratégias de Seleção de Variáveis Aplicadas na Abordagem de Aprendizado Supervisionado para Predição de Links', 
  resumo_pt: 'Nos últimos anos, uma quantidade considerável de atenção tem sido dedicada à pesquisa de redes complexas e suas propriedades. Ambientes de colaboração, redes sociais e sistemas de recomentação são exemplos populares de redes complexas que surgiram recentemente. Essas redes são objeto de interesse na academia e na indústria, e se caracterizam por possuir algum tipo de interligação de dados. O estudo da evolução deste tipo de rede requer a compreensão sobre como tais interliga ções são formadas. Uma importante tarefa neste cenário é a predição de links., que consiste em indicar quais ligações fazem (ou farão) parte de uma rede complexa mas que, no momento, não estejam representadas. Uma das abordagens para tratar esta tarefa é baseada em aprendizado supervisionado e converte os dados originais da rede em um problema de classicação binária. Embora muitos trabalhos tenham reportado resultados promissores ao utilizar a abordagem de classicação binária na predição de links., a escolha do conjunto de atributos para gerar os modelos preditivos é reconhecida como um grande desao. Por outro lado, diversos trabalhos reportam a bem sucedida aplicação de métodos de seleção de variáveis em problemas de aprendizado de máquina. Assim, o presente trabalho tem como objetivo comparar a precisão de modelos preditivos construídos a partir da aplicação de diferentes estratégias de seleção de variáveis baseadas na abordagem de construção de modelos para predição de links. Para tanto, quatro estratégias de seleção de variáveis foram implementadas e avaliadas. Os experimentos foram realizados em três conjuntos de dados distintos, utilizando cinco algoritmos de classicação, e os resultados sugerem que o uso de estratégias de seleção de variáveis podem conduzir à melhores modelos de classicação, principalmente as estratégias baseadas em metaheurísticas evolucionárias.', 
  resumo_en: '--'},

 {numero: 692, ano: 2015, dia: '04/02', autor: 'Anderson Crivella de Carvalho Rodrigues', orientador: [9], linha: 1, arquivo: '2015-Anderson.pdf', 
  titulo: 'Fusão de Sensores para um VANT via Suavização Incremental Baseada em Grafos-fatores', 
  resumo_pt: 'Diversos problemas da robótica e da visão computacional como simultaneous localization and mapping (SLAM), bundle adjustment (BA) e navegação podem ser formulados como uma otimização, via método dos mínimos quadrados, de funções resíduo representadas por um grafo. Nestas abordagens, cada movimento do robô ou cada medição realizada por um sensor é representada como um nó em um grafo. Arestas representam restrições entre esses nós. A primeira parte do problema, conhecida como front-end, é a construção desse grafo. A segunda parte, conhecida como back-end, objetiva encontrar a configuração que melhor explica as restrições modeladas pelas funções resíduo. O trabalho descreve a estrutura geral de tais problemas objetivando a fusão dos sensores de um VANT - sistema robótico aéreo de dimensões reduzidas, baixo custo e caráter dual. Para resolver o problema, o trabalho foi dividido em duas etapas: (a) a elaboração e a montagem do VANT e (b) o desenvolvimento da fusão dos sensores. Na primeira etapa, foi escolhido um modelo aerodinâmico de asa fixa com formato delta, por ser barato, largamente comercializado e de fácil emprego por simuladores de vôo. Completa a solução, um conjunto de sensores composto por uma unidade de medida inercial, um magnetômetro, um barômetro, um sensor de pressão diferencial e um GPS. Esses sensores tipicamente operam com diferentes taxas (1 100Hz) e são contaminados por ruídos não determinsticos, prejudicando a qualidade das informações e fazendo com que o grafo de navegação cresca rapidamente. Na etapa da fusão de sensores, o trabalho emprega como back-end um método inovador chamado de suavização incremental (incremental smoothing) via grafos-fatores (factor-graphs), o front-end. Esse método otimiza apenas um pequeno subconjunto dos nós do grafo reduzindo consideravelmente a carga computacional. O subconjunto representa os nós que são afetados toda vez que uma nova medição sensorial é incorporada. Como metodologia de validação propõe-se um teste de HIL (hardware-in-the-loop) composto do simulador de vôo X-Plane R, de uma estação de controle em solo e do hardware que contém as funcionalidades de piloto automático. O simulador de vôo foi estendido com o modelo aerodinâmico da aeronave real e com o plugin responsável pela fusão dos sensores. Para a comunicação entre os módulos da simulação adotou-se o protocolo MAVLink. Além disso, elaborou-se três experimentos caracterizando missões de vôo a serem cumpridas pelo VANT: um vôo quadrado, um vôo montanha-russa e um vôo lawn-mower (util para busca e varredura de áreas). Finalmente, um experimento real é realizado para verificar a similaridade dos resultados.', 
  resumo_en: '--'},

 {numero: 691, ano: 2015, dia: '04/02', autor: 'Reinaldo José Mangialardo', orientador: [6], linha: 2, arquivo: '2015-Reinaldo.pdf', 
  titulo: 'Integrando as Análises Estática e Dinâmica na Identificação de Malwares Utilizando Aprendizado de Máquina', 
  resumo_pt: 'Sistemas de Análise e Classificação de malwares utilizam técnicas de análise estática e dinâmica, juntamente com algoritmos de aprendizado de máquina, para automatizar a tarefa de identificação e classificação de códigos maliciosos. Ambas as técnicas de análise possuem pontos fracos que permitem o emprego de técnicas de evasão da análise, dificultando a identificação de malwares. Neste trabalho, propomos a unificação das análises estática e dinâmica de malwares, como um método que permita obter as características dos malwares, diminuindo a oportunidade de sucesso das técnicas de evasão. A partir dos dados obtidos na fase de análise, usamos os algoritmos de aprendizado de máquina C5.0 e Random Forest, implementados no framework de aprendizado de máquina FAMA, para executar a identificação e classificação dos malwares em duas classes, malwares e não malwares e também em múltiplas classes. Mostramos que a acurácia da análise unificada obteve um resultado de 95,75% para a classificação dos códigos como malwares e não malwares e resultado igual a 93,02% para os malwares categorizados de acordo com seus tipos. Em todos os experimentos, a análise unificada produziu melhores resultados do que os obtidos pelas análises estática e dinâmica de malwares realizadas isoladamente.', 
  resumo_en: '--'},

 {numero: 690, ano: 2015, dia: '02/02', autor: 'Laurinete do Nascimento Barcelar dos Reis', orientador: [12], linha: 1, arquivo: '2015-Laurinete.pdf', 
  titulo: 'Resiliência em Redes de Computadores baseada na Teoria da Confiabilidade', 
  resumo_pt: 'O crescente numero de aplicações executadas na Web tem incentivado o estudo da Resiliência em Redes de Computadores. Pesquisadores e especialistas de todo mundo buscam a padronização de métricas capazes de gerar a correta leitura do nível de resiliência nas redes. O presente trabalho propõe um indicador de resiliência que busca uma boa infraestrutura, desde o início do seu projeto. Tal projeto sera possibilitado por meio da Teoria da Confiabilidade, cujo objetivo é o de aumentar-se a confiabilidade dos componentes da rede, reduzindo o custo com manutenção, o tempo de reparo, o tempo médio entre a ocorrência das falhas e, consequentemente, a redundância de equipamentos e enlaces, permitindo a economia do projeto. O indicador proposto e testado em backbones de topologias reais, demonstrando bons resultados, o que permite aos gerentes de redes a utilização da presente metodologia no bom planejamento, além da recuperacão em eventuais incidentes de falhas. A utilização de links mais confiáveis, proporcionará uma estrutura mais bem preparada para enfrentar as ameaças, além de promover o aumento da Qualidade dos Servicos fornecidos pela rede e, o consequente cumprimento dos Acordos de Nível de Serviço.', 
  resumo_en: '--'},

 {numero: 689, ano: 2015, dia: '26/01', autor: 'Tarcísio do Nascimento Araújo', orientador: [12], linha: 2, arquivo: '2015-Tarcisio.pdf', 
  titulo: 'Modelo de Gerenciamento de Redes Virtuais em Ambiente OpenFlow', 
  resumo_pt: 'Uma das principais propostas para o desenvolvimento de soluções para a internet do futuro e que apresenta menor impacto no tráfego em produção e a virtualização de rede. Dentro desse paradigma destacam-se as Redes Defidas por Software (SDN), redes que tem como principal inovação a separação dos planos de controle e dados, viabilizando a programação dos equipamentos e não apenas a sua configuração como nas redes tradicionais. Neste sentido a tecnologia OpenFlow apresenta-se como uma das mais promissoras e com maior utilização dentro das solucões apoiadas no paradigma SDN. Sua arquitetura tem como caracterstica a figura do controlador, responsavel por definir a maneira como os pacotes devem ser tratados, proporcionando um gerenciamento central de toda a rede. Dentro da tecnologia OpenFlow existe um controlador de proposito especial chamado FlowVisor que permite a divisão das capacidades dos dispositivos de rede, atraves da criação de slices, que viabilizam o funcionamento de diversas redes virtuais sobre uma mesma infraestrutura física, apoiada na implantação de regras para o encaminhamento dos pacotes junto aos controladores OpenFlow e dos dispositivos de rede. Contudo, o FlowVisor possui uma limitação na sua arquitetura relacionada a ausência de um modelo específico para o gerenciamento dos slices (redes virtuais). Deste modo, este trabalho apresenta um modelo para o gerenciamento dos slices com foco nas caractersticas de transmissão dos dados da aplicação em operação na rede, utilizando-se de interfaces automatizadas com o proprio FlowVisor e os controladores em funcionamento na rede para criação e ajuste dos slices e das suas policies (topologias) dentro de uma rede gerenciada pelo FlowVisor.', 
  resumo_en: '--'},

 {numero: 688, ano: 2014, dia: '19/12', autor: 'Andres Fernando Velastegui Carrera', orientador: [7], linha: 2, arquivo: '2014-Andres.pdf', 
  titulo: 'Encadeamento de Artigos Utilizando Algoritmos Genéticos', 
  resumo_pt: 'A pesquisa científica e uma das atividades mais importantes das comunidades acadêmicas, pois através dela se motiva a discussão científica e a disseminação do conhecimento. A primeira fase dessa atividade requer uma busca exaustiva de informações relevantes que possam contribuir na redação de artigos e teses. Através dos sistemas de recomendações, um pesquisador tem acesso a diversos documentos científicos, não obstante, a ordenação coerente das informações obtidas pode constituir um grande problema. Alguns trabalhos propõem soluções para resolver problemas similares, focando seu estudo na ordenação de frações de textos, a organização de uma mínima quantidade de documentos, o tratamento de informações repetidas e a diminuição da complexidade computacional. Este trabalho propõe uma alternativa complementar aos sistemas de recomendações existentes. O objetivo e apresentar uma ou várias sequências logicas de artigos que facilitem a leitura compreensiva do usuario sobre um tema específico. Para isso, foi desenvolvida uma ferramenta baseada em Algoritmos Genéticos, considerando a hipótese de que as heurísticas envolvidas no processo genético podem permitir o encadeamento coerente de artigos. Os resultados alcançados mostraram que os Algoritmos Genéticos são uma forma aceitável de aproximar-se as sequências ordenadas da logica humana.', 
  resumo_en: '--'},

 {numero: 687, ano: 2014, dia: '12/12', autor: 'Leandro Alves de Sousa', orientador: [11], linha: 2, arquivo: '2014-Leandro.pdf', 
  titulo: 'Um Estudo sobre a Contribuição de Grupos de Desenvolvedores na Complexidade Estrutural do Software', 
  resumo_pt: 'A complexidade estrutural do software implica em alto esforço para execução das atividades de entendimento, manutenção e evolução de software, influenciando de forma negativa na realização das atividades que geram retornos em termos de custo e satisfação ao cliente. Considerando esse cenário, o presente trabalho apresenta um método que, a partir da classificação dos desenvolvedores em grupos considerando características comuns relacionadas às alterações realizadas no código-fonte, permite a análise da contribuição positiva e negativa de cada grupo de desenvolvedores considerando a variação da complexidade estrutural de um software. Foram analisados dados relativos a repositórios de projetos de software livre para determinar quais características estão relacionadas a alteração no código-fonte realizada pelos desenvolvedores. Uma vez determinadas essas características, foram realizados experimentos utilizando algoritmos de mineração de dados para agrupar os desenvolvedores e para determinar o tipo e o grau da contribuição de cada grupo de desenvolvedor. Dentre os resultados apurados através deste trabalho, e possível afirmar que há grupos de desenvolvedores com características específicas que contribuem de forma negativa para a variação da complexidade estrutural. Em contrapartida, o estudo demonstra que há grupos que trazem benefícios em relação a diminuição da complexidade estrutural do software.', 
  resumo_en: '--'},

 {numero: 686, ano: 2014, dia: '08/12', autor: 'Fabiano de Siqueira Freitas', orientador: [11], linha: 3, arquivo: '2014-Fabiano.pdf', 
  titulo: 'Near Field Communication Aplicado no Cuidado à Pessoa com Demência', 
  resumo_pt: 'De acordo com a Alzheimer Disease International (ADI) em 2013 foi estimado o número de 44,4 milhões de pessoas com demência em todo o mundo, e este número pode chegar a 75,6 milhões em 2030. Simultaneamente, a necessidade de soluções que auxiliem o tratamento e acompanhamento dessas pessoas se torna mais evidente. O fato e que esta estimativa representa um alto impacto para os cuidadores de pessoas com demência. Na verdade, um cuidador, ou assistente de cuidados, pode ter a tarefa de assistir muitas pessoas com demência, o que requer tempo e atenção. Para que o melhor cuidado possa ser oferecido, o cuidador precisa de informações sobre seus pacientes, e estas, muitas vezes, são incompletas e inconsistentes, registradas em cadernos e cartões diários. Neste estudo, será abordado o desafio de utilizar a tecnologia NFC (Near Field Communication) associada a dispositivos moveis como uma poderosa fonte de informações para auxílio no tratamento de pessoas com demência. A capacidade de armazenamento destas informações em etiquetas NFC será avaliada e técnicas para otimizar esta capacidade serão estabelecidas. Ao final, um aplicativo para dispositivos moveis com tecnologia NFC será apresentado como um front-end para a interação entre o cuidador e as informações sobre a pessoa com demência.', 
  resumo_en: '--'},

 {numero: 685, ano: 2014, dia: '26/09', autor: 'Regis de Souza Carvalho', orientador: [2], linha: 2, arquivo: '2014-Regis.pdf', 
  titulo: 'Proposta de Arquitetura para Coleta de Ataques Cibernéticos às Infraestruturas Críticas', 
  resumo_pt: 'A Defesa Cibernética das infraestruturas críticas nacionais tornou-se um importante desafio, principalmente diante do atual cenário de ataques cibernéticos aos diversos países, com o surgimento de diversos malwares atacantes de ativos críticos de instituições estratégicas para a segurança e soberania nacional. Honeypots são sensores baseados em anomalias, sendo considerados como ferramentas importantes na detecção de ataques cibernéticos, para a mitigação dos riscos e ameaças às infraestruturas críticas. A utilização de honeypots simulando os ativos críticos alvos das ameaças cibernéticas se faz necessária para identificação real dos ataques e ameaças existentes. Este trabalho propõe uma arquitetura denominada honeySCADA que coleta ataques cibernéticos direcionados aos sistemas industriais SCADA, através da simulação da interação entre um equipamento do operador do sistema SCADA com um honeypot que simula um CLP (controlador lógico programável), ativo importante do sistema SCADA, na rede de uma empresa da infraestrutura crítica nacional. Foi realizada a avaliação do honeySCADA proposto analisando a sua similaridade através de uma avaliação estatística do serviço Modbus IP, que se faz essencial para o funcionamento destes ativos em redes de produção industrial. Com base na arquitetura proposta e a identificação dos diversos ataques recebidos, faz-se uma comparação com trabalhos recentes, resultando em maiores quantidades de ataques específicos ao equipamento do operador e ao honeyCLP. É realizada uma simulação de ataque explorando vulnerabilidades do equipamento do operador, exploradas no ataque Stuxnet, sendo coletados os ataques semelhantes recebidos pela arquitetura para comparação, contribuindo para as pesquisas existentes no âmbito da defesa cibernética nas infraestruturas críticas.', 
  resumo_en: '--'},

 {numero: 684, ano: 2014, dia: '02/09', autor: 'Diego Campos Moussallem', orientador: [11], linha: 2, arquivo: '2014-DiegoMoussallem.pdf', 
  titulo: 'Utilização de Ontologias para Desambiguação de Termos Homônimos em Traduções Simultâneas para Comunicadores de Tempo Real', 
  resumo_pt: 'Com a evolução tecnológica, a informação que antigamente era trocada por cartas, jornais ou até por pombos correios, passou a ser transmitida quase em tempo real. A alta velocidade na difusão das informações, influenciou na forma de comunicação da população atualmente. As pessoas passaram a se comunicar por meio de ferramentais chamadas comunicadores, essas ferramentas se tornaram um habito na vida da sociedade com a criação dos computadores de mão, chamados smartphones. Logo, a população começou a utilizar estes comunicadores para entrar em contato com pessoas distantes e até de nacionalidades diferentes. Entretanto, esta comunicação entre indivíduos de nacionalidades diferentes só se concretiza por meio do conhecimento de ambos em um mesmo idioma. Quando não se há este conhecimento mutuo, não é possível a concretização do diálogo. Com este problema para realizar o diálogo. Os usuários que não possuam o domínio do idioma de seu companheiro de conversa, passaram a utilizar de ferramentas de tradução automática para traduzir as mensagens a serem enviadas. As mensagens trocadas em um diálogo são na maioria das vezes sentenças pequenas de texto. Uma sentença ao ser enviada a um tradutor automático, necessita de um contexto para que haja a tradução de forma correta. Quando não se há um contexto especifico, a frase pode ser traduzida de forma errônea. Pois, as ferramentas de tradução em sua maioria implementarem algoritmos estatísticos. Estes algoritmos se baseiam na maior frequência de utilização das palavras. A palavra que for mais vezes enviada ao tradutor e a tradução mais utilizada. Logo, uma sentença curta que possuir um termo ambíguo, terá a tradução que possuir a maior frequência, independente do contexto da frase. O contexto e responsável por definir os significados das palavras. A importância do contexto está relacionada diretamente com o problema da ambiguidade. A ambiguidade e um problema em aberto na comunidade científica, sendo responsável por inúmeras pesquisas. No âmbito da ambiguidade são encontrados diversos problemas. O problema a ser tratado por esta pesquisa e a homônima. A homônima são palavras que possuem mais de um significado e que possuem a mesma grafia e pronuncia. Estes termos são chamados de homônimos. Este trabalho se propõe a reduzir a ambiguidade dos termos homônimos em pequenos trechos de texto em traduções simultâneas aplicadas a comunicadores de tempo real. Esta redução se dará por meio da utilização de conceitos extraídos das ontologias. As ontologias são responsáveis por descrever conceitos mundanos. As ontologias no meio computacional são utilizadas para descrever significados semânticos a dados estáticos. A extração de conceito e largamente utilizada em pesquisas na área da web semântica. As ontologias estão sendo aplicadas em novas áreas da computação, como a da tradução automática. Com base em pesquisas da área, surgiu a ideia do desenvolvimento de um componente semântico. Este componente implementara a solução proposta e auxiliara os tradutores automáticos a um traduzir um diálogo.', 
  resumo_en: '--'},

 {numero: 683, ano: 2014, dia: '28/08', autor: 'Luciene Carvalho Correa de Souza', orientador: [201], linha: 3, arquivo: '2014-Luciene.pdf', 
  titulo: 'Uma Abordagem para Fusão de Dados Utilizando o Modelo JC3IEDM e a Técnica de Relacionamento de Dados', 
  resumo_pt: 'Atualmente a gestão da sobrecarga de informações tornou-se um grande desafio. Como gerenciar todos estes dados, que são apresentados em formatos diversos e oriundos de fontes heterogêneas? Como melhor subsidiar a tomada de decisão, cuja eficiência está diretamente relacionada à qualidade apresentada pelas informações disponíveis? O problema vai além da integração de esquemas e a fusão de dados é apresentada como uma boa solução para a redução da sobrecarga e melhoria da qualidade da informação. Entretanto, a fim de realizar a fusão de dados de modo eficaz, outro problema precisa ser abordado: como identificar, dentre toda a informação recebida, quais são os dados que representam o mesmo objeto (entidade) no mundo real? Assim, esta Dissertação propõe uma arquitetura para fusão de dados provenientes de arquivos XMLs distintos, em um ambiente de Comando e Controle, permitindo identificar corretamente as informações referentes ao mesmo objeto e fusioná-las de forma mais precisa, acrescentando qualidade e confiança à informação final exposta ao Comando. E assim, contribuir para a obtenção da consciência situacional, melhoria da tomada de decisão e minimização da sobrecarga de dados. Para isto, foi utilizada a técnica de relacionamento de dados (record linkage), inspirada na aplicação de uma matriz esparsa e múltiplos critérios para obter a similaridade entre os objetos, além da preocupação em mensurar a qualidade da informação que, neste trabalho foi obtida pela construção de uma rede bayesiana a partir dos critérios precisão, confiabilidade e credibilidade da entidade REPORTING-DATA do Modelo de Dados de Interoperabilidade da Organização do Tratado do Atlântico Norte (OTAN), denominado JC3IEDM (Joint Consultation, Command and Control Information Exchange Data Model).', 
  resumo_en: '--'},

 {numero: 682, ano: 2014, dia: '27/08', autor: 'Natalia Queiroz de Oliveira', orientador: [12], linha: 2, arquivo: '2014-Natalia.pdf', 
  titulo: 'Emprego de SDN para o Balanceamento de Carga em Redes de Computadores com Suporte a Multiplos Caminhos', 
  resumo_pt: 'A atual arquitetura das redes de computadores é formada de protocolos projetados para serem definidos de maneira isolada, provendo soluções específicas. Isso resultou em uma das principais limitações das redes atuais: a complexidade. Os métodos de balanceamento de carga são em sua maioria complexos de serem implementados nas redes atuais, pois os algoritmos e soluções propostas são limitadas pelos fabricantes e disponíveis em hardwares proprietários. As arquiteturas de redes existentes não foram projetadas para atender os requisitos dos usuários atuais, tornando assim a rede limitada, sem espaco para inovação. Surge então uma grande necessidade de gerenciar toda essa demanda de forma ágil e segura. Redes definidas por software conhecida como SDN, acrônimo do inglês, (Software Defined Networking) é uma tecnologia emergente onde o plano de encaminhamento e o plano de controle estão claramente separados. O plano de controle pode ser diretamente programado a nível de aplicação. Em SDN, a inteligência da rede é centralizada em controladores, os quais mantem uma visão global da rede. OpenFlow é o protocolo e o elemento essencial para o conceito de SDN. Atualmente, é o unico protocolo padronizado que permite a manipulação direta do plano de encaminhamento dos dispositivos de rede. O presente trabalho propõe uma arquitetura para balanceamento de fluxos, chamada LBX, a qual define o caminho de custo mínimo, baseada no algoritmo Dijkstra, em uma topologia com suporte a múltiplos caminhos. Para a implementação da proposta foi utilizado o controlador Floodlight e o emulador de redes Mininet. Os resultados dos experimentos mostram que o balanceador de fluxos LBX oferece vantagens comparado com os testes sem o balanceamento de carga e com a solução ECMP (Equal Cost Multipath Routing) implementado nos roteadores da empresa Cisco.', 
  resumo_en: '--'},

 {numero: 681, ano: 2014, dia: '27/08', autor: 'Phillipe da Silva Cavalcante', orientador: [201], linha: 2, arquivo: '2014-Phillipe.pdf', 
  titulo: 'Mecanismo de Encadeamento de Notícias Web Baseado em de Implicação Textual', 
  resumo_pt: 'Uma das principais vantagens dos sites de notícias sobre os jornais tradicionais é a capacidade de complementar o conteúdo de uma notícia por meio da recomendação de notícias relacionadas. A recomendação é feita tipicamente pelos editores, geralmente, através de buscas em uma coleção de notícias. No entanto, ela é considerada custosa e estática, além de ser limitada ao conhecimento dos editores sobre as notícias. A maioria das soluções usa técnicas tradicionais de recuperação da informação, como métodos de classificação por relevância ou sistemas de recomendação personalizada. Estas técnicas proporcionam notícias muito parecidas, o que pode não atender a necessidade de informação do usuário. Este trabalho apresenta um mecanismo de encadeamento de notícias capaz de fornecer notícias organizadas segundo um critério de implicação textual, proporcionando ao usuário uma progressão do conteúdo que está sendo lido. Para isso, o mecanismo utiliza um sistema de reconhecimento de implicação textual, para o encadeamento das notícias, e técnicas tradicionais de indexação, busca e recuperação de notícias. Os resultados obtidos com este mecanismo foram comparados com as abordagens de ordenação por tempo e relevância usando os métodos TF-IDF e BM25F e mostraram que o mecanismo de encadeamento de notícias por implicação textual proporciona encadeamentos de notícias que atendem a necessidade de informação dos usuários com uma precisão maior do que as técnicas tradicionais de recuperação de informação para a recomendação de notícias.', 
  resumo_en: '--'},

 {numero: 680, ano: 2014, dia: '12/08', autor: 'Edgar Honorato Cardoso Bernardo', orientador: [201], linha: 2, arquivo: '2014-Edgar.pdf', 
  titulo: 'Arquitetura para Suportar Sobrecargas Momentâneas em Ambiente de Computação em Nuvem', 
  resumo_pt: 'Este trabalho apresenta uma arquitetura computacional capaz de suportar os efeitos produzidos pelas sobrecargas momentâneas em servidores físicos e virtuais hospedados em ambiente de computação em nuvem. A arquitetura proposta objetiva a automatização da gerência de máquinas virtuais hospedadas neste ambiente, combinando uma estratégia proativa, que realiza o balanceamento de carga nos momentos em que não há sobrecarga das máquinas físicas e/ou virtuais, com uma estratégia reativa, que é acionada caso ocorra sobrecarga nestas máquinas. Em ambas as estratégias, é observado o acordo de nível de serviço (ANS) estabelecido para cada serviço hospedado de acordo com o modelo de infraestrutura como serviço (IaaS). As principais contribuições deste trabalho são a proposição em uma nova abordagem para tratamento de sobrecargas momentâneas neste ambiente e a implementação de uma arquitetura computacional, chamada Phoenix, capaz de realizar o tratamento de sobrecargas momentâneas, observando os ANSs dos serviços de infra-estruturas hospedados (IaaS), considerando os recursos CPU, memória e rede das máquinas físicas e virtuais. Os resultados comprovam que a arquitetura Phoenix é eficaz e possui destacada atuação no tratamento de sobrecargas de rede de máquinas virtuais, onde conseguiu realizar o isolamento da sobrecarga momentânea na máquina física hospedeira evitando a propagação de seus efeitos no cluster.', 
  resumo_en: '--'},

 {numero: 679, ano: 2014, dia: '07/08', autor: 'Diogo Soares dos Santos', orientador: [2], linha: 2, arquivo: '2014-Diogo.pdf', 
  titulo: 'Plataforma para Simulação de Ataques DDoS', 
  resumo_pt: 'Nos últimos anos houve um gradativo aumento na quantidade e qualidade dos ataques distribuídos de negação de servico (DDoS). Alem dos alvos tradicionais como portais de bancos e e-commerce existe hoje uma grande quantidade de ataques direcionados a orgãos governamentais. Tal fato aumenta ainda mais a importância na pesquisa nesta área. Uma ferramenta importante para apoiar os pesquisadores é a simulação destes ataques. Apesar disso, as ferramentas tradicionalmente disponveis não apresentam facilidades necessárias para a modelagem das características deste tipo de ataque. Neste trabalho de pesquisa é proposto um ambiente de simulação de ataques DDoS utilizando como base o simulador de redes ns-3. A plataforma permite que o pesquisador realize experimentos com exibilidade e escalabilidade.', 
  resumo_en: '--'},

 {numero: 678, ano: 2014, dia: '05/08', autor: 'Patrick Baptista Amaral de Lara', orientador: [11], linha: 3, arquivo: '2014-Patrick.pdf', 
  titulo: 'Um Protocolo para Integração de Sistemas de Comando e Controle', 
  resumo_pt: 'O cenário de uma operação conjunta pode ser descrito como um ambiente de guerra heterogêneo, onde existe a necessidade de atualização de uma consciência situacional compartilhada, baseada em uma constante troca de informações entre as unidades participantes. No ambiente das operações militares não é usual existir uma grande capacidade de processamento e uma infraestrutura de redes com banda larga, o que torna necessária a elaboração de um protocolo para troca de mensagens, que aborde especialmente estas restrições, baseado nas tecnologias de troca de mensagens. No âmbito de uma operação conjunta, torna-se necessário uma correta coordenação, em tempo hábil, dos sistemas de consciência situacional existentes nos Comandos de Força, a fim de permitir uma maior interoperabilidade. Essa integração deve ser abordada em dois níveis, por meio de um modelo de dados consolidado, onde é utilizado o Joint Consultation, Command and Control Information Exchange Data Model (JC3IEDM), e de um protocolo para troca de mensagens, que trata o encaminhamento de mensagens XML, utilizando SOAP, atendendo a requisitos de integração. Nesse contexto, também é utilizada uma arquitetura orientada a serviços (SOA), que é considerada como adequada para a integração de Sistemas de Comando e Controle (SC2s). Este trabalho apresenta um modelo de protocolo para troca de mensagens entre os SC2 utilizados nas Operações Conjuntas, a fim de permitir a interoperabilidade entre os sistemas de informação de comando e controle, utilizando o JC3IEDM, o modelo de dados para a interoperabilidade de SC2s desenvolvido pelo Multilateral Interoperability Programme (MIP).', 
  resumo_en: '--'},

 {numero: 677, ano: 2014, dia: '15/07', autor: 'Eduardo Alves de Oliveira', orientador: [11], linha: 2, arquivo: '2014-Eduardo.pdf', 
  titulo: 'Um Processo de Acompanhamento de Produtividade para o Desenvolvimento de Software', 
  resumo_pt: 'Este estudo demonstra de forma quantitativa e qualitativa os impactos positivos e negativos na produtividade dos projetos de software. O objetivo é ajudar o gerente de projeto no monitoramento da produtividade do desenvolvimento do software (OLIVEIRA, 2013). O processo de monitoramento de produtividade (PMP) de um PDS deve ser utilizado em paralelo com o processo de desenvolvimento de software (PDS) para controlar a produtividade do projeto. O estudo de impactos na produtividade auxilia e direciona as decisões que o gerente de projeto deve tomar no planejamento e acompanhamento de seus projetos. Para chegarmos as conclusões deste estudo foi analisada uma base histórica de projetos de um empresa de desenvolvimento de software que possui uma grande variedade de tipos diferentes de sistemas, tecnologias, linguagens e processos de desenvolvimento. Para complementar a base histórica, um questionário foi respondido por uma amostragem qualitativa de projetos desta base histórica. Grande parte das questões são relacionadas a questões qualitativas dos impactos na produtividade dos projetos. Os potenciais impactos na produtividade dos projetos foram obtidos de pesquisa bibliográca.', 
  resumo_en: '--'},

 {numero: 676, ano: 2014, dia: '09/06', autor: 'Diego Nascimento Esteves da Silva', orientador: [6], linha: 1, arquivo: '2014-DiegoSilva.pdf', 
  titulo: 'Predição de Tendência de Ativos em Séries Financeiras Utilizando Algoritmos de Aprendizado de Máquina', 
  resumo_pt: 'Prever o comportamento de ativos nanceiros é um desao que vem sendo estudado com diversas técnicas nos últimos anos, seja por modelos de regressão ou de classicação. Apesar da grande quantidade de pesquisas acadêmicas, antecipar o movimento do preço nal de um ativo ainda é um desao a ser superado. A maior diculdade está no comportamento não-linear e não estacionário das séries temporais nanceiras, assim como toda incerteza e ruídos encontrados no mercado nanceiro. Este trabalho foca em encontrar tendências para os preços de ações do mercado Brasileiro em determinados períodos de tempo e abrange o estudo do comportamento de três algoritmos de aprendizado de máquina, Naive Bayes (NB), Regressão Logística (RL) e Support Vector Machines(SVM) aplicados a dados históricos de séries nanceiras. O comportamento foi comparado com um algoritmo base (Baseline - BLS) criado para ser o limite inferior de qualidade para os modelos gerados. Como apresentado em estudos similares da área, o algoritmo SVM apresentou melhores resultados que os algoritmos comparados nesse estudo. Uma análise detalhada mostra que o ajuste de parâmetros traz benefícios para o resultado dos modelos, com ganhos de até 32%. A biblioteca LibSVM foi integrada ao framework FAMA, ambos disponíveis livremente. A m de obter modelos com maior capacidade de generalização, foi alterado o comportamento do método de validação cruzada padrão para se ajustar as características do problema, tornando-a deslizante em função do tempo. Observou-se que retreinar o modelo em determinado período de tempo melhora positivamente a taxa de acerto quando comparado ao modelo de validação cruzada padrão.', 
  resumo_en: '--'},

 {numero: 675, ano: 2014, dia: '05/05', autor: 'Marcus Albert Alves da Silva', orientador: [8], linha: 2, arquivo: '2014-Marcus.pdf', 
  titulo: 'Composição de Módulos de Ontologias para Anotação Semântica de Textos Científicos', 
  resumo_pt: 'A anotação semântica utilizando conceitos de ontologias possibilita a inserção de informação semântica nos mais variados tipos de documentos disponíveis na Web. Tal operação torna este vasto conteúdo processável por meio de agentes de software, facilitando a pesquisa e o acesso a informações relevantes. No cenário científico, esta operação é bastante útil e necessária, apoiando os cientistas na tarefa de análise bibliográfica. Na área biomédica esta operação é especialmente complexa, pois o conteúdo a ser anotado poderá estar associado a um grande volume de conceitos presentes em ontologias de grande porte. Esta característica torna a anotação, neste domínio, uma atividade muito custosa, mesmo quando automatizada. Este trabalho apresenta uma abordagem que a partir da associação de métodos e técnicas ligadas a modularização de ontologias, viabiliza a construção de uma estrutura constituída de módulos compostos por conceitos de mais de uma ontologia. A ideia é que esta estrutura reduzida seja viável e útil para a anotação semântica automatizada de textos científicos. Além disso, a estrutura deve atender a um foco de interesse de pesquisa específico. Foi implementada uma ferramenta de software que materializou os conceitos da abordagem, e viabilizou a sua avaliação. Um experimento no domínio biomédico, sobre um corpus de 500 textos científicos, foi realizado e apresentou bons resultados, confirmando a aplicabilidade da abordagem proposta.', 
  resumo_en: '--'},

 {numero: 674, ano: 2014, dia: '24/03', autor: 'Graziele Weinchutz Kapps', orientador: [9], linha: 2, arquivo: '2014-Graziele.pdf', 
  titulo: 'Utilizando o LapVR como Dispositivo Haptico Programável para Suporte de Procedimentos Cirúrgicos', 
  resumo_pt: 'A Realidade Virtual é comumente utilizada em diversas áreas, como militar, engenharia, medicina, entre outras, e muitas vezes é utilizada com a finalidade de treinar seus usuários em uma determinada situação ou aspecto. As aplicações de realidade virtual no campo de cirurgias têm sido constantemente desenvolvidas. Treinamentos em que a exposição do paciente em uma sala cirúrgica não é necessaria, se tornam mais seguros e às vezes até mais eficientes. O risco de um erro fatal é totalmente descartado em simulações cirúrgicas e o aprendiz pode treinar quantas vezes forem necessárias até que o mesmo sinta seguranca em realizar tal cirurgia no mundo real. O simulador de laparoscopia (LapVR) é um sistema utilizado em treinamentos de cirurgias minimamente invasivas, que tem por objetivo treinar a destreza dos cirurgiões. Este simulador é composto por um computador, um monitor, duas ferramentas hápticas que consistem em simular as pinças laparoscópicas, e uma ferramenta que simula a câmera do sistema. É composto também por pequenas aplicações que auxiliam no treinamento de destreza do usuário. Porém, o mesmo não é composto por uma simulação de cirurgia laparoscópica completa. Diversas aplicações baseadas em desenvolvimentos de simuladores laparoscópicos, utilizando dispositivos hápticos como o Phantom ou CyberGrasp, estão sendo realizadas. Porém nenhuma até o presente momento utilizou as próprias ferramentas hápticas do sistema LapVR como um recurso de dispositivo para tais aplicações. O objetivo da presente dissertação é apresentar a utilização do dispositivo LapVR, como um novo dispositivo háptico programável para aplicações em realidade virtual que não sejam as já contidas no próprio sistema. Para isso foram implementadas aplicações na área de medicina utilizando o sistema como um dispositivo háptico e de interação.', 
  resumo_en: '--'},

 {numero: 673, ano: 2014, dia: '07/03', autor: 'Tanilson Dias dos Santos', orientador: [3], linha: 1, arquivo: '2014-Tanilson.pdf', 
  titulo: 'Conectividade Algébrica em Grafos Aleatórios', 
  resumo_pt: 'A conectividade algébrica é um parâmetro espectral associado a um grafo e denido como o segundo menor autovalor de sua matriz Laplaciana. Este parâmetro é utilizado para determinar a conexidade de um grafo e pode ser calculado em grafos de grande porte com o objetivo de aproximar ou estimar algumas informações sobre o grafo (diâmetro, distribuição de graus do grafo, tamanho de corte de arestas). Neste trabalho é investigado o comportamento da conectividade algébrica nos principais modelos de grafos aleatórios citados na literatura (Erdos-Renyi, Small World e Ligação Preferencial ). São efetuadas algumas observações a respeito dos limites para o segundo menor autovalor 2 denidas na literatura. Este trabalho também propõe dois modelos de grafos aleatórios híbridos e faz análise de duas redes reais. Dentre as contribuições desta pesquisa estão a verica- ção do relacionamento de 2 com outros invariantes do grafo, apontamentos sobre casos particulares não observados pelos experimentos encontrados na literatura, apresentação de um novo modelo de grafo híbrido e detecção de comportamentos dos autovalores para cada modelo de grafo estudado, além da análise de redes complexas colhidos em ambientes reais.', 
  resumo_en: '--'},

 {numero: 672, ano: 2014, dia: '31/01', autor: 'Carla Chystina de Castro Pacheco Ferreira', orientador: [12], linha: 2, arquivo: '2014-Carla.pdf', 
  titulo: 'Modelagem e Simulação para a Geração de Traces de Bot na Arquitetura Centralizada', 
  resumo_pt: 'O número crescente de roubo de informações e de ataques que paralisam sistemas e servi¸cos compõe o cenário de amea¸cas representadas pelas botnets. O acesso público aos rastros de seus tráfegos na rede (traces) é difícil de ser obtido. Este trabalho propõe uma modelagem e simulação configurável de traces de bot, baseando-se nas fases do ciclo de vida do bot na área de identificação e prevenção de atividades maliciosas. Essas são as fases que geram menor volume de tráfego. Traces simulados de bot foram gerados e, através da modelagem em fases, foi possível identificar o comportamento do bot na rede. Experimentos foram realizados e seus resultados confirmaram a qualidade e corretude da abordagem proposta.', 
  resumo_en: '--'},

 {numero: 671, ano: 2014, dia: '30/01', autor: 'Laion Luiz Fachini Manfroi', orientador: [10], linha: 2, arquivo: '2014-Laion.pdf', 
  titulo: 'Avaliação de Arquiteturas Manycore e do Uso da Virtualização de GPUs em Ambientes de HPDC', 
  resumo_pt: 'Atualmente a virtualização encontra-se presente tanto nas diversas estratégias para a consolidação de recursos em Data Centers, como no suporte às pesquisas em Nuvens Computacionais. Ao mesmo tempo em que novos modelos de infraestruturas de HPC combinam arquiteturas de processamento multi-core e manycore (aceleradores), ainda há uma grande expectativa de como a camada de virtualização afeta o acesso a estes dispositivos e seu desempenho. Este trabalho busca estabelecer uma avaliação de desempenho dos diferentes hipervisores, quando associados ao uso destas arquiteturas multi-core e manycore. Além disto, é estabelecido um comparativo entre as diferentes arquiteturas disponíveis no mercado atual de HPC.', 
  resumo_en: '--'},

 {numero: 670, ano: 2014, dia: '27/01', autor: 'João Paulo Abreu Maranhão', orientador: [12], linha: 3, arquivo: '2014-Joao.pdf', 
  titulo: 'Modelagem Matemática de Redes Tolerantes ao Atraso com Roteamento Epidêmico', 
  resumo_pt: 'Redes tolerantes ao atraso (Delay Tolerant Networks ou DTN) podem ser utilizadas para a troca de informações entre usuários em ambientes desafiadores, caracterizados pela conectividade intermitente e atrasos longos e/ou variáveis, sem a necessidade de uma infraestrutura de telecomunicações previamente instalada. A tecnologia IEEE 802.11, presente em vários dispositivos móveis pessoais, tornou-se praticamente ubíqua nos grandes centros urbanos, surgindo como uma opção para a padronização de DTNs. Em ambientes urbanos, a propagação do sinal através do canal de comunicação está sujeita aos fenômenos de atenuação de percurso e desvanecimento, cujas características podem ser descritas por vários modelos consagrados existentes na literatura. Outro fenômeno que influencia o desempenho de redes sem fio é a interferência, provocada por transmissões concorrentes realizadas por nós vizinhos, na mesma faixa de frequência. Vários modelos clássicos de proliferação de doenças infecciosas em populações podem ser adaptados para o estudo de DTNs com roteamento epidêmico, dentre eles o modelo epidemiológico SIR (Suscetível, Infectado, Recuperado). Em DTNs com roteamento epidêmico, a mensagem é transmitida todas as vezes em que ocorre um contato entre transmissor e receptor, com uma determinada probabilidade de sucesso, a qual é influenciada pelas condições de propagação rádio e interferência do ambiente. Tal dinâmica é similar, por exemplo, a difusão epidêmica de um vírus numa população. A presente dissertação apresenta uma análise da modelagem epidemiológica SIR aplicada em DTNs, com a elaboração de dois novos modelos analíticos, chamados de DTPL (Disponível, Transmissor, Path Loss) e DTLNS (Disponível, Transmissor, Log-Normal Shadowing). Estes modelos consideram que o canal de transmissão é modelado segundo as aproximações path loss e log-normal shadowing, respectivamente. A validação foi realizada através da comparação entre os valores obtidos por simulação e as predições fornecidas pelos modelos. Os resultados indicam que os modelos apresentam um bom ajuste aos valores esperados, desde que sejam obedecidas algumas condições relacionadas ao alcance de transmissão e a densidade de nós. Por fim, este trabalho realiza uma análise da evolução do processo epidêmico, ao longo do tempo, nos modelos propostos. De acordo com os resultados obtidos, o processo epidêmico representado pelo modelo DTLNS apresenta maior velocidade, visto que os efeitos de desvanecimento aumentam a conectividade da rede. Por outro lado, foi observado que o processo epidêmico representado pelo modelo DTPL apresenta menor velocidade, pois considera somente os efeitos de atenuação e, portanto, não dispõe do aumento de conectividade proporcionado pelo desvanecimento.', 
  resumo_en: '--'},

 {numero: 669, ano: 2013, dia: '09/08', autor: 'Laércio Correia de Vasconcelos Filho', orientador: [12], linha: 2, arquivo: '2013-Laercio.pdf', 
  titulo: 'Análise da Transmissão em Vídeo 3D em Redes de Comunicações', 
  resumo_pt: 'Existem muitos desafios para se fornecer servic¸os de vi´deo 3D em redes de pacotes. Um deles seria a capacidade do enlace entre dois no´s da rede, um outro seria a variac¸a~o de atrasos na entrega dos pacotes, e um terceiro seriam os efeitos causados por eventuais perdas de pacotes. Vi´deos, 2D ou 3D, passam por um processo de compressa~o antes de serem transmitidos, visando diminuir as suas taxas de bits. O processo de compressa~o almeja manter uma determinada qualidade dada uma limitac¸a~o de taxa imputada pelo enlace. Vi´deos 3D demandam uma banda-passante mais elevada, o retardo na entrega de pacotes e´ muito danoso para a transmissa~o de vi´deos, 2D ou 3D, e perdas de pacotes de dados de informac¸o~es comprimidas e´ um problema que nem sempre consegue ser mitigado por ferramentas de error concealment no lado do decodificador. Esta dissertac¸a~o implementa e analisa diferentes configurac¸o~es de rede para propor aquelas que permitem a transmissa~o desses vi´deos com minimizac¸a~o de erros. Para as diferentes configurac¸o~es de rede sa~o codificadas e transmitidas diferentes combinac¸o~es de vi´deos estereosco´picos que permitem a visualizac¸a~o da informac¸a~o 3D do lado do decodificador. Os resultados das simulac¸o~es indicam as combinac¸o~es de configurac¸o~es de estruturas de codificac¸a~o e de rede mais resilientes. Desta forma, este trabalho ilustra os problemas a serem mitigados por aplicac¸o~es que demandam enlaces de elevada capacidade, como a transmissa~o de vi´deo 3D, apresentando algumas soluc¸o~es.', 
  resumo_en: '--'},

 {numero: 668, ano: 2013, dia: '07/06', autor: 'Alberto Torres Angonese', orientador: [9], linha: 2, arquivo: '2013-Alberto.pdf', 
  titulo: 'Estação de Controle em Solo com Funcionalidades de Vôo Múltiplo para VANTs', 
  resumo_pt: 'Neste trabalho, desenvolvemos um sistema computacional para uma ECS (estação de controle em solo) capaz de controlar o voo e a navegação de múltiplos vants (Veículos Aéreos Não Tripulados), implementando funcionalidades de voo em formação e desvio de vants em rota de colisão. O sistema foi estruturado em dois módulos, o de planejamento de missão e o modulo de controle do voo múltiplo. O planejamento de missão é composto por uma ferramenta de planejamento da formação e planejamento de trajetória que foram implementadas com base na API NASA World Wind, possibilitando que o planejamento da formação e da trajetória de voo pudessem ser realizados diretamente em mapas 3D. No módulo de controle do voo múltiplo foi desenvolvida uma metodologia baseada em campos potenciais. Foi estruturado um ambiente de simulação, para os testes e validação da metodologia proposta. Com o simulador, é possível realizar um planejamento de missão e avaliar os parâmetros do campo potencial, através da visualização das trajetórias simuladas. Outra funcionalidade deste ambiente e a integração da ECS, com o simulador de voo XPlane R , implantando o voo múltiplo em um ambiente muito próximo ao de uma situação real. Além do trabalho de implementação e validação do software da ECS, também foram realizados testes correlacionados ao tema dessa dissertação. O primeiro, foi um teste de HIL (Hardware In te Loop) para o projeto vant-Lanu do IME, em que foi realizada a integração de um piloto automático com uma solução open source de ECS. O segundo experimento foi a estruturação de uma plataforma de testes de voo para vants. A plataforma é composta por dois dirigíveis robóticos não tripulados indoors que são controlados remotamente, para possibilitar testes das funcionalidades de voo múltiplo. Neste trabalho, descrevemos o modelo aerodinâmico e aerostático do dirigível e é apresentada a plataforma ZigBee (IEEE 802.15.4) como um sistema de telemetria e controle da atitude do dirigível.', 
  resumo_en: '--'},

 {numero: 667, ano: 2013, dia: '07/06', autor: 'Vinícius Prado da Fonseca', orientador: [9], linha: 2, arquivo: '2013-Vinicius.pdf', 
  titulo: 'Sistema de Localização de Objetos para Apoio a um Assistente Robótico Móvel na Casa Inteligente', 
  resumo_pt: 'Nossa pesquisa apresenta um sistema de posicionamento para auxiliar um assistente móvel na casa inteligente. O atual sistema é capaz de calcular a posição de um objeto usando a intensidade do sinal recebido por um dispositivo móvel. Medições do indicador da intensidade do sinal recebido foram feitas em laboratório para a aplicação de um método de estimação da posição. Dois modelos de propagação foram utilizados: (a) log-distance pathloss (LogDistPL) é um modelo em que a perda de sinal tem uma influência aleatória com distribuição log-normal; e (b) free space decay law (FSDL) é com base na lei de decaimento de um sinal em espaço aberto. A solução de posicionamento implementada passa pelo desenvolvimento de um framework para captação de dados. Esta necessidade surgiu da percepção de que na literatura e na comunidade não havia uma contribuição nos moldes da solução pretendida. Neste trabalho foram abordadas as estruturas, dinâmicas e ferramentas utilizadas na construção de uma rede de sensores funcionando como infra-estrutura para o framework de captação de dados que forneceu dados para o sistema de posicionamento interno. Na etapa de avaliação, parte dos testes offline foram feitos com dados captados em laboratório utilizando a infraestrutura de rede instalada. Outra parte dos testes foram executados utilizando o dataset público WPR.B. Ambos os datasets utilizam as características e a organização do framework desenvolvido. Os resultados alcançados, onde o erro de posicionamento esteve em média entre 1,6 a 2,4 metros, passaram pela investigação das características das amostras coletadas, por testes sobre a calibração desenvolvida no trabalho e finalizando com experimentos sobre o método de posicionamento implementado na casa inteligente.', 
  resumo_en: '--'},

 {numero: 666, ano: 2013, dia: '24/05', autor: 'Stefano Henrique Rodrigues', orientador: [9], linha: 2, arquivo: '2013-Stefano.pdf', 
  titulo: 'Sistemas Autônomos e Inteligentes para Robôs Cooperativos no Ambiente Small Size League', 
  resumo_pt: 'Este trabalho apresenta o desenvolvimento de sistemas robóticos para um conjunto de agentes robóticos cooperativos e colaborativos, no ambiente de futebol de robôs para a categoria small size league. Para alcançar o objetivo proposto foram desenvolvidos os seguintes sistemas: (a) planejamento de movimentação baseado em física; (b) predição da localização dos agentes (robôs e bola); (c) estratégia cooperativa ofensiva para dois robôs.', 
  resumo_en: '--'},

 {numero: 665, ano: 2013, dia: '21/05', autor: 'Rodolfo Soares Alves Dantas', orientador: [12], linha: 2, arquivo: '2013-Rodolfo.pdf', 
  titulo: 'Diferenciação de Ataques DDoS e Flash Crowds', 
  resumo_pt: 'Ataques distribuídos de negação de serviço (DDoS) e fenômenos de Flash Crowd são duas grandes preocupações para a estabilidade e segurança da Web. Fenômenos de Flash Crowd são grandes quantidades de acessos legítimos aos sites da Web, enquanto ataques DDoS Web são pedidos maliciosos em grande volume, cujo objetivo é subverter o funcionamento normal do site impedindo o acesso de usuários legítimos. Dado que ambas as situações são, à primeira vista, caracterizadas por um aumento no volume de tráfego da rede direcionado a um ponto, diferenciar ataques DDoS Web e Flash Crowds é um desafio. Nesse trabalho, é apresentada uma proposta para a diferenciação de ataques DDoS Web e fenômenos de Flash Crowds através da análise integrada das características dos sites-alvo e do tráfego de rede. Também é apresentada uma técnica de aprendizado por reforço para que haja adaptação às mudanças de acordo com os erros e acertos descobertos na detecção das anomalias. No trabalho, é utilizada uma autenticação gráfica para distinguir quais os acessos são provenientes de usuários humanos e quais acessos são feitos por robôs. Para a implementação da proposta, foi criada uma arquitetura conceitual baseada em camadas e uma aplicação chamada StopBots. A técnica proposta é avaliada através de alguns experimentos em um ambiente real de comércio eletrônico.', 
  resumo_en: '--'},

 {numero: 664, ano: 2013, dia: '20/05', autor: 'Stanley Rodrigues', orientador: [3], linha: 1, arquivo: '2013-Stanley.pdf', 
  titulo: 'Sobre a Conectividade Algébrica e seus Autovetores na Classe das Árvores', 
  resumo_pt: 'Sobre a Conectividade Algébrica e seus Autovetores na Classe das Árvores.', 
  resumo_en: '--'},

 {numero: 663, ano: 2013, dia: '07/02', autor: 'José Eduardo França', orientador: [10], linha: 2, arquivo: '2013-Jose.pdf', 
  titulo: 'Otimização de Checkpoint de Máquina Virtual para Tolerância a Falhas', 
  resumo_pt: 'Atualmente a virtualização de computadores é amplamente empregada, motivada principalmente pela redução de custos e pela facilidade de gerenciamento dos grandes data centers. Um outro motivo de optar por tal tecnologia deve-se ao fato de proporcionar um maior grau de disponibilidade dos serviços que estão executando sobre uma máquina virtual. E uma das formas de viabilizar a alta disponibilidade é replicar periodicamente o estado da máquina virtual para um host de backup. No entanto, esse procedimento, conhecido como checkpoint, compromete o desempenho das aplicações da máquina virtual protegida, além de gerar um custo adicional sobre a rede. A presente proposta apresenta uma abordagem para reduzir tais custos, ajustando dinamicamente o intervalo de tempo entre os checkpoints.', 
  resumo_en: '--'},

 {numero: 662, ano: 2013, dia: '06/02', autor: 'Fabiano de Moraes Domingues', orientador: [5], linha: 1, arquivo: '2013-Fabiano.pdf', 
  titulo: 'Prevenção Contra Fraude em Criptografia Visual com Base na Autenticação de Múltiplas Imagens em Transparências Circulares', 
  resumo_pt: 'Em criptografia visual, a partir de uma imagem secreta composta por pixels pretos e brancos, é possível gerar n transparências de forma que essa imagem se torne visível quando forem sobrepostas determinada quantidade q = k dessas transparências e totalmente invisível caso q < k. Foi demonstrado que este esquema é vulnerável contra fraude quando k = 2 e n > 2. Um esquema de prevenção contra a fraude baseado na autenticação de imagens fornece a capacidade de verificação da integridade das transparências antes da decodificação da imagem secreta. Os principais esquemas deste tipo, chamados de HCT e HT sofreram ataques às suas vulnerabilidades indicadas. O principal objetivo deste trabalho é apresentar a construção de um novo esquema de prevenção contra fraude que forneça a capacidade de detecção da fraude com suporte aos aspectos de segurança relacionados com a proteção aos ataques realizados contra os esquemas anteriores HCT e HT. A criptografia visual de múltiplas imagens forneceu a estrutura básica para a construção dos mecanismos de detecção da fraude e de proteção contra os ataques conhecidos. O suporte aos aspectos de segurança contra as vulnerabilidades dos esquemas HCT e HT podem ser resumidas em: possibilitar a verificação de cada transparência em toda sua extensão; limitar a quantidade de pixels pretos das imagens de autenticação; e permutar as posições das imagens que serão autenticadas durante o processo de verificação da integridade das transparências.', 
  resumo_en: '--'},

 {numero: 661, ano: 2013, dia: '31/01', autor: 'Cesar Augusto Borges de Andrade', orientador: [202, 6], linha: 2, arquivo: '2013-Cesar.pdf', 
  titulo: 'Análise Automática de Malwares Utilizando as Técnicas de Sandbox e Aprendizado de Máquina', 
  resumo_pt: 'A análise de código malicioso (malware) permite identificar características do comportamento do software, ou seja, como atua no sistema operacional, que técnicas de ofuscação são utilizadas, quais fluxos de execução levam ao comportamento principal planejado, uso de operações de rede, operações de download de arquivos, captura de informações do usuário ou do sistema, acesso a registros, entre outras atividades, com o objetivo de aprender como o malware funciona e criar formas de identificar novos softwares maliciosos com comportamento similar assim como formas de defesas. A análise manual para a geração de assinaturas torna-se inviável, pois demanda muito tempo, se comparado à velocidade de disseminação e criação de novos malwares. Sendo assim, a presente dissertação propõe a utilização de técnicas de sandbox e aprendizado de máquina para automatizar a identificação de softwares nesse contexto. Esse trabalho, além de apresentar uma abordagem diferente e mais rápida para detecção de malware, obteve uma taxa de precisão acima de 90%.', 
  resumo_en: '--'},

 {numero: 660, ano: 2013, dia: '28/01', autor: 'Marcelo José Camilo', orientador: [12], linha: 2, arquivo: '2013-Marcelo.pdf', 
  titulo: 'Mecanismos de Segurança em Redes de Rádio Cognitivos', 
  resumo_pt: 'Com o rápido crescimento da quantidade de dispositivos e aplicações que utilizam redes sem fio nos últimos anos (redes de sensores, redes veiculares, etc.), a demanda por espectro cresce de maneira sem precedentes (LIU, 2011), acarretando a necessidade de novas faixas de frequência para a operação das novas aplicações. No entanto, boa parte do espectro eletromagnético já se encontra alocado de maneira fixa a uma aplicação, o que tem se mostrado ineficiente, pois esta política acarreta uma baixa utilização da porção do espectro licenciada. Rádio Cognitivo é uma tecnologia que possibilita o compartilhamento do espectro de maneira oportunista. Dentre diversos casos promissores para utilização da tecnologia de RC, tem-se as aplicações comumente usadas em sistemas militares como o Sistema C2 e o SISTAC do Exército Brasileiro. Em um ambiente de RC sob ataques, a ausência de mecanismos de proteção ou a utilização de estratégias de mitigação equivocadas podem comprometer o desempenho desejado, seja por meio da redução da confiabilidade, redução da vazão ou no aumento da interferência causada ao UP. Considerando o exposto acima, o presente trabalho tem por objetivo estudar e propor mecanismos de segurança para RRC, objetivando mitigar ou atenuar os efeitos de ataques de interferência e de emulação de usuário primário. Nesta dissertação, propomos a arquitetura CMPS para RC que contempla a segurança espectral. Assim, mecanismos de defesa podem ser validados. Como proposta, a arquitetura CMPS apresenta o componente de Segurança Espectral como aprimoramento do ciclo cognitivo descrito em (AKYILDIZ, 2006) e considerando os quatro conjuntos de informações de entrada principais listados por (DOYLE, 2009): informações do ambiente rádio, informações dos requisitos de QoS da aplicação, informações dos recursos disponíveis para o dispositivos e informações da política regulatório do uso do espectro. Estudamos os efeitos do ataque de interferência em redes de rádios cognitivos. Avaliamos os efeitos dos perfis de ocupação espectral do usuário primário e do atacante, bem como de parâmetros da camada física na confiabilidade e na vazão média da rede. Propomos um mecanismo anti-interferência em RRC que calcula a melhor ocupação espectral para o usuário secundário, introduz a aleatoriedade na escolha de canais e transmite as mensagens de controle e os dados de maneira redundante em múltiplos canais. Estudamos os efeitos do ataque de emulação de usuario primário em redes de rádios cognitivos. Avaliamos os efeitos dos perfis de ocupação espectral do usuário primário e do atacante, bem como de parâmetros da camada física na vazão média da rede. Propomos um mecanismo de mitigação de ataque de EUP em RRC que introduz a aleatoriedade e utiliza informações da camada física para a escolha de canais. A contribuição esperada deste trabalho é auxiliar e orientar projetistas e desenvolvedores da tecnologia de RC a mitigar ou atenuar os efeitos dos ataques sofridos pelas arquiteturas desta nova tecnologia disponibilizadas para utilização, uma vez que os mecanismos propostos podem ser utilizados.', 
  resumo_en: '--'},

 {numero: 659, ano: 2013, dia: '21/01', autor: 'Rodrigo Mathias Praxedes da Silva', orientador: [12], linha: 2, arquivo: '2013-Rodrigo.pdf', 
  titulo: 'Arquitetura para Detecção Online de Bots P2P', 
  resumo_pt: 'Botnets são os principais veículos para atividades ilícitas na internet, o crescimento de suas atividades tem causado prejuízos de bilhões de dólares à economia mundial e colocado em risco a segurança de nações. Atualmente é observada uma migração das arquiteturas centralizadas (baseadas em protocolos HTTP e IRC) mais simples, para arquiteturas descentralizadas (baseadas em protocolos P2P) que são capazes de prover maior resiliência às suas redes devido à própria natureza do protocolo em que se fundamentam. Com o objetivo de mitigar esta ameaça, muitas propostas de detecção têm sido elaboradas, porém, a maioria ainda com seu foco nas arquiteturas centralizadas através de métodos que executam análises offline. Estas análises buscam por padrões em longos históricos de rede, muitas vezes utilizando algoritmos de clusterização, que podem demorar dias para obter uma resposta satisfatória. Neste trabalho é proposta uma arquitetura para a detecção de bots descentralizadas de forma online, ou seja, que não cause uma demora entre a produção dos dados a serem analisados e os resultados obtidos por estas análises. O tráfego de rede gerado pelos nós monitorados são transformados em fluxos de rede e filtrados por módulos de pronta execução que não necessitam de longos históricos, provendo desta forma resultados a tempo de serem utilizados com um maior proveito. A arquitetura proposta é dividida em duas fases: na primeira são buscados todos os nós que possuem aplicações P2P ativas para que posteriormente, na segunda fase, sejam diferenciadas as aplicações P2P legítimas das bots P2P. Ao final deste trabalho é feita uma validação em dois cenários distintos que comprovam a eficiência da arquitetura para a detecção online de bots P2P.', 
  resumo_en: '--'},

 {numero: 658, ano: 2013, dia: '14/01', autor: 'Gabriela Moutinho de Souza Dias', orientador: [12], linha: 2, arquivo: '2013-Gabriela.pdf', 
  titulo: 'Modelo Epidemiológico SIR Aplicado a Redes Tolerantes a Atrasos e Desconexões', 
  resumo_pt: 'Redes Tolerantes a Atrasos e Desconexões (Delay-Tolerant Network – DTN) são redes sem fio e sem infraestrutura cuja arquitetura foi definida para vencer obstáculos de comunicação, como conexão intermitente e longos atrasos. O campo de pesquisa da modelagem analítica para DTN apresenta carência de trabalhos comparado a outros campos da área. Nesse sentido, a presente dissertação apresenta uma análise de aplicabilidade do modelo matemático epidemiológico SIR (Susceptible–Infected–Recovered) na modelagem do processo de propagação de mensagens em DTN Epidêmica, ou seja, DTN que utiliza roteamento epidêmico. O modelo SIR foi originalmente construído para modelar processos de transmissão de doenças infecciosas entre indivíduos, e pode ser adaptado para DTN Epidêmica devido à semelhança entre a propagação de um vírus em uma população e o encaminhamento de mensagens via roteamento epidêmico. O modelo SIR adaptado foi batizado neste trabalho de Modelo Epi-DTN e possui duas abordagens para modelar o processo infeccioso: estocástica e determinística, esta última preferencialmente utilizada pela literatura. A análise de aplicabilidade ´e feita por meio da comparação de resultados de simulação com previsões de ambas abordagens do modelo. Os resultados obtidos indicam que a abordagem estocástica do modelo apresenta um ajuste ao resultado esperado bastante superior quando comparada com sua versão determinística equivalente. Além disso, são observadas restrições com relação à aplicação do modelo como ferramenta de previsão de comportamento que, quando ignoradas, podem distorcer bastante os resultados esperados. Sendo assim, conclui-se que, dentre as duas abordagens do Modelo Epi-DTN, a estocástica é a melhor opção para a modelagem do roteamento epidêmico em DTN, para todos os cenários modelados. Além disso, os resultados apontam para a necessidade de se definir condições pra ´ticas de uso do modelo para que o mesmo possa fornecer previsões úteis, válidas e confiáveis.', 
  resumo_en: '--'},

 {numero: 657, ano: 2012, dia: '11/12', autor: 'Rodrigo de Oliveira Pereira', orientador: [11], linha: 3, arquivo: '2012-Rodrigo.pdf', 
  titulo: 'Um Mecanismo de Coordenação Baseado em Redes de Transição para Agentes de Software', 
  resumo_pt: 'Um agente de software é um componente que apresenta autonomia como uma de suas propriedades fundamentais, ou seja, a possibilidade de delegar tarefas como uma transferência do poder de decisão. Normalmente, agentes são projetados de forma independente e, em geral, são construídos e gerenciados por organizações distintas e com pouco conhecimento umas das outras. Uma propriedade importante é a coordenação entre agentes que têm por finalidade atingir a cooperação necessária para completar uma determinada tarefa com sucesso. O principal motivo para uma preocupação com a coordenação é o fato de que um só agente, dentro de um sistema multiagente, não ter informação ou capacidade suficiente para resolver muitos dos problemas, pois os objetivos não podem ser atingidos, agindo isoladamente. O trabalho desenvolveu um mecanismo baseado em redes de transições que pode ser tomado como base para a construção de uma solução multiagente, quando há a necessidade de coordenação entre agentes. Este mecanismo é externo aos agentes, para que seus desenvolvedores possam se concentrar em sua construção, capacidades e objetivos. Com esta externalização, componentizou-se o mecanismo de coordenação e retirou do agente a responsabilidade de manter esta lógica. Para modelar as características de coordenação, foi proposta uma forma de documentar explicitamente os requisitos de coordenação e modelá-los através de um modelo de alto nível chamado Modelo de Coordenação.', 
  resumo_en: '--'},

 {numero: 656, ano: 2012, dia: '09/11', autor: 'Marcelo Machado Collares', orientador: [10], linha: 2, arquivo: '2012-Marcelo.pdf', 
  titulo: 'Um Resolvedor Numérico Baseado no Método de Lattice-Bolztmann Aplicado em Unidades de Processamento Gráfico', 
  resumo_pt: 'O método de Lattice Boltzmann (LBM) e uma técnica numérica alternativa para a modelagem e simulação de fluidos dinâmicos. Ele e indicado para problemas onde se deseja obter eficiência e facilidade de programação, por se tratar de um método discreto especificamente elaborado para o cálculo computacional. Possui como uma de suas principais características o grande potencial de paralelização. Algoritmos paralelos, em regra, eram destinados ao processamento em agrupamentos de computadores, mas a evolução computacional das atuais placas gráficas permite solucionar em um único computador o que antigamente era resolvido por vários. Nesse contexto, o objetivo da presente dissertação e propor duas versões paralelas de um resolvedor numérico baseado no LBM aplicado em unidades de processamento gráfico (GPU Graphics Processing Unit), utilizando-se da arquitetura de software Compute Unified Device Architecture (CUDA). A primeira implementação paralela executa um procedimento de decomposição de dados que favorece a coalescência de dados na memória global. A segunda utiliza a coalescência aliada a utilização da memória constante, que possui menor latência na hierarquia de memória da GPU. São avaliados ainda os ganhos de desempenho obtidos pelas versões paralelas, em relação a versão sequencial destinada ao processamento em uma única unidade central de processamento (UCP).', 
  resumo_en: '--'},

 {numero: 655, ano: 2012, dia: '09/11', autor: 'Carlos Eduardo Pantoja', orientador: [11], linha: 3, arquivo: '2012-Carlos.pdf', 
  titulo: 'Uma Metodologia para Apoio ao Desenvolvimento Semi-automático de Sistemas Multi-agentes', 
  resumo_pt: 'Este trabalho apresenta uma metodologia MDA de desenvolvimento de sistemas multi-agentes que utiliza de modelos em diferentes níveis de abstração, partindo da especificação do sistema, e após um conjunto de regras de transformações entre modelos, chegar até a codificação da plataforma de execução JASON/Moise+. A metodologia utiliza o FAML como modelo independente de plataforma, que é um meta-modelo que reúne os conceitos de diversas metodologias de desenvolvimento de sistemas multi-agentes em um único modelo. Como modelo específico de plataforma foi escolhido o modelo JaCaMo, que utiliza a linguagem de programação orientada a agentes JASON, o modelo organizacional Moise+ e o Cartago para programação de artefatos para o ambiente. As transformações entre o modelo independente de plataforma e o modelo específico de plataforma são apresentadas na linguagem QVT. A linguagem QVT foi escolhida por ser uma linguagem de transformação de modelos padronizadas pela OMG. As transformações entre o modelo específico de plataforma e a codificação da plataforma de execução são apresentadas na linguagem M2T, padronizada pela OMG, que consiste em um conjunto de templates para geração de artefatos de texto. Uma extensão da metodologia é apresentada para a geração de codificação para a plataforma UAVAS. A plataforma é utilizada para programar agentes inteligentes em JASON e AgentSpeak para Veículos Aéreos Não Tripulados. A extensão consiste em adições nas regras de transformações M2T. Este trabalho apresenta, também, um ambiente de desenvolvimento de sistemas-multi agentes, que consiste de um conjunto de plugins para a plataforma de desenvolvimento Eclipse. Os meta-modelos FAML e JaCaMo foram construídos utilizando o Eclipse Modeling Framework. As transformações entre os modelo independente de plataforma e o modelo específico de plataforma foram implementadas utilizando a ferramenta M2M e as transformações entre o modelo específico de plataforma para o plataforma de execução foram implementadas utilizando a ferramenta Acceleo. São apresentados três exemplos de sistemas multi-agentes. O Sistema Gold Miners que foi modelado na metodologia Prometheus, o sistema UAVAS modelado em Tropos exemplificando a codificação para VANT e o sistema Write Paper exemplificando a geração do modelo organizacional.', 
  resumo_en: '--'},

 {numero: 654, ano: 2012, dia: '22/08', autor: 'Wellington Hetmanek dos Santos', orientador: [2], linha: 2, arquivo: '2012-Wellington.pdf', 
  titulo: 'Método de Detecção de Ataques DDoS Compostos Baseado em Filtragem Sequencial', 
  resumo_pt: 'Nos últimos anos, ataques distribuídos de negação de serviço (DDoS) têm evoluído em termos de complexidade de detecção, tamanho da botnet e volume de tráfego gerado. Dois ataques ocorridos na Coreia do Sul, o primeiro em 2009 (ataque 7.7) e o segundo em 2011 (ataque 3.4), acrescentaram mais características aos ataques DDoS. Esses dois ataques lançaram uma ofensiva contra seus alvos a partir de diferentes tipos de ataques DDoS conhecidos e ao mesmo tempo, formando um ataque DDoS composto. Este tipo inovador de ataque possui características particulares que podem ser usadas para serem identificadas, seja separando cada ataque ou analisando-os conjuntamente. Neste estudo, é apresentado um método de detecção de ataques DDoS através do algoritmo desenvolvido, o Algoritmo de Detecção de Ataques DDoS Compostos, que utiliza uma sequência de filtros que objetivam identificar as origens, os destinos e o perfil de utilização de protocolos dessas origens de ataque que compõe uma botnet. Para comprovar a eficiência do algoritmo foram construídos ambientes de simulação, onde foi possível gerar tráfego malicioso com características de ataques DDoS compostos e coletar esses dados na saída da rede. O método proposto se mostrou eficaz em todas as simulações, identificando 100% dos ataques DDoS compostos existentes nesses ambientes.', 
  resumo_en: '--'},

 {numero: 653, ano: 2012, dia: '22/08', autor: 'Nilson Mori Lazarin', orientador: [5], linha: 1, arquivo: '2012-Nilson.pdf', 
  titulo: 'Método Não Supervisionado de Reconhecimento de Padrões Criptográficos', 
  resumo_pt: 'Identificar o algoritmo criptográfico empregado na cifração de um texto em claro, partindo apenas da análise do texto cifrado, é um dos atuais problemas em aberto na Criptologia. Nesse âmbito, diversas pesquisas têm sido apresentadas desde 2001, porém, ainda não existem técnicas consolidadas que possibilitem a descoberta do algoritmo utilizado na encriptação de uma mensagem. As várias técnicas existentes já publicadas demonstram eficiência no agrupamento e classificação de criptogramas de no mínimo 1KB, cifrados em modo ECB. Entretanto, outros modos de operação têm sido imunes a classificação através das técnicas baseadas em Recuperação da Informação. Este trabalho contribui para a evolução das técnicas de classificação de criptogramas, apresentando um método não supervisionado para reconhecimento de padrões em criptogramas, que através da métrica de distância de Hamming e coloração em grafos, pode auxiliar na identificação do algoritmo gerador de um dado criptograma, cifrado em modo CBC.', 
  resumo_en: '--'},

 {numero: 652, ano: 2012, dia: '20/08', autor: 'Thiago Fernandes Dias', orientador: [201], linha: 3, arquivo: '2012-Thiago.pdf', 
  titulo: 'Sistema de Apoio e Planejamento Operacional para Epidemias', 
  resumo_pt: 'Os sistemas de informação são utilizados na área da saúde há décadas e, conforme as tecnologias e metodologias evoluem, mais oportunidades para a criação de sistemas robustos surgem. No entanto, há ainda algumas lacunas na aplicação de conceitos como a reutilização e flexibilidade que torna as arquiteturas dos sistemas inflexíveis e difíceis de integrar com os outros. Ontologia é uma tecnologia promissora que está ganhando maturidade ao longo das linhas de pesquisa para representação do conhecimento e é capaz de resolver esses problemas. Nessa dissertação, descrevemos o nosso trabalho sobre a utilização da OWL 2.0 para representar um conjunto de dados epidemiológicos da Dengue no Município do Rio de Janeiro e também uma maneira inteligente de mostrar esses dados em mapas georreferenciados, incluindo ações de combate à epidemia sugeridas a os locais afetados por ela. Nós descrevemos a transformação de um simples dicionário de dados em um modelo baseado em conhecimento que pode auxiliar de forma inteligente o processo de tomada de decisões no período epidêmico. Também acreditamos que a nossa abordagem fornece uma solução que pode ser reutilizada e adaptável a vários tipos de sistemas de apoio a decisão com foco em epidemias.', 
  resumo_en: '--'},

 {numero: 651, ano: 2012, dia: '01/08', autor: 'Claudio Sá de Abreu', orientador: [12], linha: 2, arquivo: '2012-Claudio_Abreu.pdf', 
  titulo: 'Modelo Matemático para DTN Epidêmica', 
  resumo_pt: 'Uma DTN (Delay and Disrupting Tolerant Network), e´ um tipo de rede tolerante a falhas e desconexo~es onde, ao contra´rio das redes TCP/IP tradicionais, na~o e´ exigida a existe^ncia de conexa~o fim-a-fim para a transmissa~o de pacotes. Essa caracteri´stica permite a utilizac¸a~o de DTNs em situac¸o~es extremas, onde haja care^ncia de infraestrutura de rede tradicional, ou, a exemplo de aplicac¸o~es militares, onde na~o ha´ conhecimento da topologia do terreno de sua situac¸a~o. DTNs tambe´m sa~o especialmente u´teis em regio~es de cata´strofes e desastres, onde grande parte da infraestrutura de rede pode ter sido destrui´da, ou ter ficado com sua capacidade muito comprometida. A despeito da quantidade de estudos existentes sobre redes DTN, foi identificada uma lacuna na bibliografia sobre o assunto, referente a ferramentas anali´ticas que sirvam de suporte ao projeto e desenvolvimento desse tipo de rede. O objetivo desse trabalho e´, atrave´s do desenvolvimento de um modelo matema´tico, propor ferramentas para ajudar no projeto, desenvolvimento e, quando aplica´vel, avaliac¸a~o de uma DTN epide^mica. Em uma DTN epide^mica os pacotes ou mensagens sa~o copiados de um no´ para o outro todas as vezes que ocorre um contato, de maneira similar a propagac¸a~o de um vi´rus em uma epidemia. Por isso, um reconhecido modelo de propagac¸a~o de epidemias de doenc¸as, chamado de SIR (Susceptible, Infected and Recovered) foi usado como base para o desenvolvimento desse trabalho. Esse modelo foi estendido para representar melhor o funcionamento de uma DTN epide^mica. Para validar esse modelo estendido, foram gerados mais de cem cena´rios com para^metros diferentes, como nu´mero de no´s e velocidade dos no´s, e foram feitas, pelo menos, cem simulac¸o~es diferentes para cada um destes cena´rio. Para cada instante de tempo foi calculada a me´dia e o desvio padra~o de todos os resultados obtidos nessas simulac¸o~es, e essa seque^ncia de me´dias foi considerada o caso me´dio. Os resultados obtidos com o modelo estendido se aproximaram muito do caso me´dio em todos os cena´rios estudados, o que demonstrou que esse modelo era bastante acurado. Apesar dos bons resultados obtidos para o caso me´dio, o modelo obtido na~o foi suficiente para explicar ou prever o resultado de va´rios cena´rios que se afastavam muito do caso me´dio. Esses grandes afastamentos puderam ser avaliados atrave´s do desvio padra~o dos valores usados para calcular a me´dia de cada cena´rio em cada instante de tempo. Esse afastamento exigiu, enta~o, um melhor entendimento do processo de difusa~o das mensagens em DTNs epide^micas. Esse melhor entendimento tornou possi´vel uma nova extensa~o do modelo matema´tico que, com uso do me´todo estati´stico de Monte Carlo, permitiu a criac¸a~o de um algoritmo simples para se conseguir prever tanto o caso me´dio, quanto o desvio padra~o esperado para cada cena´rio. Com essa nova extensa~o do modelo matema´tico, foi possi´vel propor procedimentos para melhorar o desempenho de DTNs epide^micas que se mostraram bastante eficazes nas simulac¸o~es. ', 
  resumo_en: 'A DTN is a delay and disruption-tolerant network which, unlike the traditional TCP/IP networks, does not require end-to-end connectivity. This feature makes DTNs usable in extreme situations, where there is lack of traditional network infrastructure, or the topology of the land is not known, as in military applications. DTNs are also especially useful in regions of disasters and catastrophes, where much of the network infrastructure may have been destroyed. Despite the recent studies on the subject, a gap was identified on the development of analytical tools to support DTNs development and design. The aim of this work is to develop a mathematical model that can be used to propose tools to assist in the design, development and, where applicable, in the evaluation of an epidemical DTN. In an epidemical DTN, packets are copied from one node to another every time a contact occurs, similar to the spread of a virus in an epidemic disease. Hence a known model of spread of epidemics, called SIR (Susceptible, Infected and Recovered) was used as the basis for the development of this work. This model has been improved to better represent the epidemical DTN operation. To validate this extended model, about a hundred scenarios were generated with different parameters such as number of nodes and speed of the nodes. At least a hundred different simulations were done for each of these scenarios. For each point of time, the average and standard deviation of the values of each scenario where calculated. The sequence of means were considered the average case. The results obtained with the extended model is much closer to the average case in all studied scenarios, which show that this model is very accurate. Despite the good results obtained for the average case, the model was not able to explain or predict the outcome of various scenarios that are significantly deviated the average case. These large deviations could be addressed by the standard deviation of the values used to calculate the average of each instant of time. These deviations demanded a better understanding of the diffusion process of messages in epidemical DTN and as result, a new extension to the mathematical model was made, and as result, it was possible to, using the statistical method of Monte Carlo, create a simple algorithm that made possible to predict both the average case and the standard deviation for each scenario. With this new extension, it was possible to propose procedures to improve the DTN performance which proved quite effective in simulations.'},

 {numero: 650, ano: 2012, dia: '19/06', autor: 'Priscilia Correa e Castro Gomes', orientador: [8], linha: 3, arquivo: '2012-PriscillaGomes.pdf', 
  titulo: 'Suporte à Anotação Múltipla Baseada em Modularização de Ontologias: uma Experiência na Área Biomédica', 
  resumo_pt: 'A anotação de documentos com base em ontologias tornou-se uma tendência no contexto da Web Semântica. Atualmente, contamos com uma crescente proliferação de ontologias em domínios semelhantes, tendo cada uma delas uma estrutura individualizada de termos, propriedades e restrições. Dessa forma, a anotação de um documento utilizando apenas uma ontologia, na maioria das vezes, é insuficiente para uma recuperação eficiente e requer um esforço maior do usuário. A combinação de ontologias propicia uma melhor anotação do documento e, consequentemente uma recuperação mais eficiente. Dada a importância que a anotação múltipla possui na tarefa de anotação de documentos, a modularização surge como uma solução interessante para apoiar o usuário na anotação com a ajuda de múltiplas ontologias. Essa dissertação propõe uma arquitetura que tem como objetivo extrair o conhecimento disponível nas ontologias e gerar módulos menores para auxiliar o usuário na tarefa de anotação de seus documentos. A contribuição e o diferencial deste trabalho estão na geração de módulos a partir das anotações realizadas pelo usuário no documento. Com base nessa arquitetura, foi desenvolvida uma ferramenta que gera, a partir das anotações realizadas no documento, módulos de outras ontologias armazenadas em um repositório. Para atingir esse objetivo, este trabalho investiga conceitos e tecnologias essenciais voltados para o tema, bem como tecnologias associadas à anotação semântica e modularização de ontologias. De modo a avaliar a arquitetura proposta, foi realizada uma experiência sobre uma seleção de textos e ontologias no domínio Biomédico, com apoio de um especialista do domínio.', 
  resumo_en: '--'},

 {numero: 649, ano: 2012, dia: '03/07', autor: 'Nicolas Rocha e Silva', orientador: [12], linha: 2, arquivo: '2012-Nicolas.pdf', 
  titulo: 'Avaliação da Sensibilidade de Métricas para a Detecção de Ataques de Inundação', 
  resumo_pt: 'Cada vez mais ataques distribuídos de negação de serviço (DDoS) se tornam mais sofisticados e difíceis de detectar. O emprego de métricas mais sensíveis pode permitir a detecção antecipada destes ataques, onde há um baixo percentual de pacotes maliciosos. Sendo assim, a presente dissertação tem como objetivo analisar a sensibilidade de preditores de suavização exponencial usados para detectar ataques DDoS, e de medidas de Divergência. Comparou-se a capacidade de detecção de dois preditores (EWMA e Holt-Winters), com diferentes configurações e cenários, e duas medidas de Divergência (Divergência de Hellinger e Chi-Square). Foi verificado o desempenho dessas métricas a partir das taxas de falsos positivos e falsos negativos observados. Foi inserido ataques em traces reais (MAWILab), traces simulados (DARPA) e em amostras reais de tráfego oriundas do backbone da RNP, com o intuito de realizar simulações de ataques com diferentes volumes de inundação. Os experimentos mostram que a otimização de parâmetros dos preditores trazem melhores resultados, e que, desde que se conheça as características do tráfego monitorado e dos enlaces observados, pode-se selecionar as métricas mais eficientes para cada cenário.', 
  resumo_en: '--'},

 {numero: 648, ano: 2012, dia: '01/06', autor: 'Mattheus da Hora França', orientador: [4], linha: 2, arquivo: '2012-Mattheus_Franca.pdf', 
  titulo: 'Manipulação de Objetos Virtuais e Navegação Utilizando Dispositivos Sem Fio', 
  resumo_pt: 'Este trabalho visa incrementar interatividade em aplicações de Realidade Virtual através de dispositivos sem fio, apresentando o desenvolvimento de um navegador e manipulador de objetos virtuais controlado por interação natural, além de desenvolver um rastreador de posição de 6 graus de liberdade com a utilização do sensor de profundidade e um protótipo de uma unidade de medida inercial. A luva de dados é integrada, permitindo a movimentação dos dedos através dos seus 22 sensores. A associação destes dispositivos possibilita reproduzir com realismo os movimentos da mão, aumentando a imersão e interação do usuário com o ambiente virtual.', 
  resumo_en: '--'},

 {numero: 647, ano: 2012, dia: '01/06', autor: 'Thiago Eustáquio Alves de Oliveira', orientador: [9], linha: 2, arquivo: '2012-Thiago_Eustakio.pdf', 
  titulo: 'Sistema Autônomo para a Estimação da Pose de um Objeto com Faces Planas em Ambiente não Estruturado', 
  resumo_pt: 'Este trabalho apresenta o desenvolvimento de um sistema robótico para a estimação da pose de um objeto com faces planas em um ambiente não estruturado, assumindo que o sistema também deverá realizar atividades de mapeamento, localização e navegação. Para alcançar o objetivo proposto foi desenvolvido um sistema estruturado em 3 módulos: a) exploração e planejamento de trajetórias; b) mapeamento e localização; c) estimação da pose de um objeto com faces planas. Para a exploração e planejamento de trajetórias de trajetórias adotamos uma arquitetura híbrida que assume aspectos baseados em planejamento para trajetó- rias globais e um sistema baseado em comportamento para trajetórias locais. Na tarefa de mapeamento e localização foi utilizado o CoreSLAM, um método baseado em sensor laser, que prioriza operações inteiras na integração das leituras do sensor ao mapa, e utiliza filtro de partículas para estimação da pose do robô. Para a estimação da pose do objeto, foi desenvolvido um sistema para a estimação robusta de uma transformada homográfica H fazendo uso dos algoritmos de Transformação Linear Direta (DLT) e RANSAC. Pares de pontos extraídos através de características SURF (Speed-Up Robust Feature) da cena observada pelo robô e das imagens das faces do objeto são utilizados como entrada para o RANSAC. Com H satisfatoriamente estimada alcançamos a rotação através da decomposição homográfica e a translação através do sensor de profundidade do Microsoft Kinect.', 
  resumo_en: '--'},

 {numero: 646, ano: 2012, dia: '09/05', autor: 'Ulisses Mainart da Silva', orientador: [11], linha: 3, arquivo: '2012-Ulisses.pdf', 
  titulo: 'Uma Plataforma para Geração e Execução de Casos de Teste em Sistemas Web', 
  resumo_pt: 'Os erros na implementação de sistemas de software contribuem para vulnerabilidades, tornando a etapa de teste essencial para a garantia de qualidade. Em situações onde não é possível o acesso ao código fonte, o teste funcional é utilizado para o projeto de casos de testes. Em príncipio, o teste funcional pode detectar todos os defeitos ao combinar os valores de todas as entradas, realizando o teste exaustivo. Porém, o teste exaustivo é impraticável para sistemas Web cada vez mais complexos por ser oneroso, devido a explosão combinatorial proveniente da combinação de cada valor de cada parâmetro de entrada do sistema. Os métodos que empregam estratégias combinatórias podem reduzir o custo do teste mantendo a capacidade de detecção de defeitos ao evitar o problema da explosão combinatorial, contudo foi observado que diversos casos de teste projetados, ao serem executados, culminavam no mesmo resultado esperado. Neste sentido, a redundância deve ser eliminada para melhorar a efetividade dos casos de teste gerados e reduzir os custos da etapa de teste. Por outro lado, estratégias de redução por similaridade apesar de gerarem suítes de teste menores não mantém a mesma densidade de defeitos dos métodos combinatórios. Este trabalho visa desenvolver um método de teste sistemático funcional para geração e seleção de casos de teste funcionais para sistemas Web que se valha da especificação da funcionalidade a fim de melhorar a efetividade dos casos de teste gerados. Durante este trabalho foi desenvolvido um protótipo de uma plataforma de geração e seleção de casos de teste que utiliza o método proposto para viabilizar a execução dos casos de teste e avaliação dos resultados esperados. Para derivação dos resultados esperados, a partir de uma especificação escrita em linguagem natural controlada, foi incorporado ao método a tradução em estruturas executáveis, utilizadas por um oráculo de teste para obter os resultados esperados, necessários para eliminação dos casos de teste redundantes. Dois estudos de caso são conduzidos com a utilização do protótipo desenvolvido para analisar os ganhos em termos da efetividade. Os resultados obtidos demonstram ganhos promissores, proporcionados pelo emprego do método proposto.', 
  resumo_en: '--'},

 {numero: 645, ano: 2012, dia: '01/05', autor: 'Carlos Augusto Daniel da Costa Rubim Leão', orientador: [11], linha: 3, arquivo: '2012-Carlos_Leao.pdf', 
  titulo: 'Um Ambiente Baseado em Ontologias para Gestão de Artefatos de Software', 
  resumo_pt: 'Este trabalho propõe uma abordagem que utiliza duas ontologias, uma para processo de desenvolvimento de software e outra para o negócio da aplicação. Os conceitos dessas ontologias são usados para criar regras que permitam a criação de links de rastreabilidade. Além de rastreabilidade, a gestão de artefatos de software é considerada neste trabalho. Sistemas de controle de versão convencionais, como o CVS, Subversion, Git, entre outros, não possuem uma maneira de identificar qual tarefa do processo de desenvolvimento gerou cada artefato. Geralmente, mesmo que exista um processo de desenvolvimento padrão definido para a equipe de desenvolvimento, muitas vezes esse processo não é praticado. Dessa forma, além de uma solução em rastreabilidade, esse trabalho propõe o uso de ontologias para promover a organização dos artefatos, facilitando a organização e aquisição dos mesmos.', 
  resumo_en: '--'},

 {numero: 644, ano: 2012, dia: '24/04', autor: 'Camila Cristina Gomes Ferreira de Oliveira', orientador: [3], linha: 1, arquivo: '2012-Camila.pdf', 
  titulo: 'Inserção de uma Aresta num Grafo e o Efeito Produzido na Conectividade Algébrica', 
  resumo_pt: 'O problema do aumento máximo da conectividade algébrica pertence à classe dosproblemas NP-Completo. Este trabalho tem como objetivo estudar o problema anterior e propor uma heurística para o mesmo. Para determinar a escolha das arestas a serem inseridas num grafo de maneira a obter aumento da conectividade algébrica, foram realizados experimentos, baseados na excentricidade dos vértices, em grafos particularmente selecionados. Os resultados obtidos com a heurística aqui proposta foram comparados, nas classes de grafos selecionados, com os resultados obtidos pela heurística proposta por Ghosh e Boyd em 2006 e pelo algoritmo de força bruta. A heurística proposta nesta dissertação mostrou bom desempenho quando comparada aos dois procedimentos anteriores nos grafos analisados.', 
  resumo_en: '--'},

 {numero: 643, ano: 2012, dia: '17/04', autor: 'Alda Maria Ferreira Rosa da Silva', orientador: [8], linha: 3, arquivo: '2012-Alda.pdf', 
  titulo: 'Diretrizes para o Resgate do Esquema Conceitual e seu Compromisso Ontológico a partir de um Banco de Dados: um Estudo de Caso no Domínio da Litoestratigrafia', 
  resumo_pt: 'Recentemente, a integração de dados no domínio geológico ganhou mais atenção. Iniciativas de padronização ainda estão começando a ser adotadas no país. Existem muitos sistemas legados que foram desenvolvidos sem considerar tais propostas. Na literatura de banco de dados, as propostas de abordagens de integração de esquemas são fortemente baseadas na estrutura e sintaxe. Algumas abordagens recentes usam recursos semânticos, como ontologias para prover soluções mais promissoras para o problema da interoperabilidade. Embora visem facilitar a interoperabilidade, não chegam a explicitar como criar uma ontologia de domínio a partir de um banco de dados existente. Em paralelo, algumas abordagens têm se apoiado em ontologias de fundamentação para facilitar a interoperabilidade, e resgatar o que ficou perdido, isto é, a documentação, o esquema conceitual, etc; inexistentes em muitos esquemas de dados. Essas ontologias têm sido usadas para a criação ou revisão de esquemas conceituais, no sentido de explicitar seu compromisso ontológico, e com isso, chegar a esquemas bem fundamentados. Não há, no entanto, uma proposta que mapeie um caminho, uma direção, que tome como ponto de partida os bancos de dados existentes. Uma vez que a maioria dos bancos de dados possui pouca ou nenhuma documentação, seria então interessante sistematizar as ações necessárias para, a partir de um banco de dados, gerar um esquema conceitual que possa ser associado a uma ontologia de fundamentação, e, com isso, agregar semântica e aumentar sua capacidade de interoperar, e principalmente, resgatar a que ficou perdido, isto é, a documentação, o esquema conceitual, etc. Este trabalho apresenta como proposta um conjunto de diretrizes, que pode ajudar a capturar a semântica de um esquema de dados, através da representação do seu esquema conceitual. Essa representação é capturada de maneira bem fundamentada e explicita o compromisso ontológico do esquema favorecendo a sua interoperabilidade. Um estudo de caso foi realizado no domínio geológico, como uma maneira de identificar, especificar e validar as diretrizes propostas. O estudo envolveu o resgate do esquema conceitual do Banco de Dados de Litoestratigrafia da CPRM, com vistas a facilitar a sua interoperabilidade. Vale ressaltar ainda, que as diretrizes foram especificadas de modo genérico, podendo ser utilizadas em outros domínios.', 
  resumo_en: '--'},

 {numero: 642, ano: 2012, dia: '13/04', autor: 'Priscilla Inácia Neves', orientador: [8], linha: 3, arquivo: '2012-PriscillaNeves.pdf', 
  titulo: 'Uma Estratégia para Apoiar a Decisão baseada em Mineração de Textos Livres', 
  resumo_pt: 'Esse trabalho apresenta uma estratégia para extrair informação de textos não estruturados por meio de técnicas de Mineração de Textos. A proposta foi motivada devido à grande quantidade de dados textuais existentes no nível empresarial e governamental e a necessidade de se extrair informações úteis desses dados. O trabalho tem foco na extração de informação de textos livres, que são textos que apresentam uma estrutura linguística irregular. Em vários ambientes é possível encontrar os textos livres, como em portais da internet, mensagens SMS, registros de serviços de atendimento ao consumidor entre outros. A estratégia proposta tem como principais características a independência de domínio e a análise de documentos textuais livres escritos em Português Brasileiro através da combinação de técnicas de Mineração de Texto. Além disso, a estratégia utiliza as características da estrutura linguística para auxiliar os processos de extração e carga em um banco de dados analítico, por meio de heurísticas pré-definidas. Para verificar a viabilidade da abordagem foram implementados módulos, através dos quais foram realizados dois estudos de caso, em domínios distintos, como prova de conceito. Nesses estudos, um conjunto de documentos textuais livres foi processado e os dados resultantes mostraram que a abordagem proposta pode ser de grande valia aos gestores que precisam tomar decisões.', 
  resumo_en: '--'},

 {numero: 641, ano: 2012, dia: '27/03', autor: 'Débora Alvernaz Correa', orientador: [8], linha: 3, arquivo: '2012-Debora.pdf', 
  titulo: 'Uma Abordagem para Extração de Conteúdos Baseada em Características Estruturais e Navegacionais de Portais Web', 
  resumo_pt: 'Em um portal semântico, conteúdos são descritos e organizados com base em ontologias de domínio. Entretanto, com a quantidade crescente de informações geradas a cada dia na Web, a publicação dinâmica nesses portais, cujos conteúdos são oriundos de um grande e diversificado número de sites, ainda representa um grande desafio, uma vez que essa tarefa carece de mecanismos para atualizar e integrar informações automaticamente. Nesta dissertação, é proposta uma abordagem para solucionar o problema de interoperabilidade entre portais, considerando a possibilidade de popular, de forma dinâmica, um portal semântico, a partir da análise da estrutura navegacional de portais que apresentam algum potencial semântico. Assim, esta abordagem estende uma arquitetura proposta na literatura constituída de vários componentes que, em termos gerais, contribuem para alimentar uma ontologia a partir de documentos semânticos disponibilizado na Web. Para atingir esse objetivo, este trabalho estas dificuldades através da Web Semântica, é necessário que as informações disponibilizadas sejam adequadamente descritas e classificadas, de forma a permitir o compartilhamento de vocabulários num domínio de conhecimento, possibilitando a Para solucionar classificação e troca de informações entre máquinas e/ou indivíduos. conceitos e tecnologias essenciais voltados para o tema, de modo a viabilizar o enriquecimento de uma arquitetura genérica para realizar o objetivo proposto. Para tanto, tecnologias associadas à Web semântica, tais como taxonomias e ontologias, portais Web, extração de informações e crawlers tiveram grande enfoque nessa dissertação. está na atualização de portais semânticos a partir de conteúdos de sites e/ou portais da Web aberta.', 
  resumo_en: '--'},

 {numero: 640, ano: 2011, dia: '12/12', autor: 'Marcelo Figueira de Vasconcelos', orientador: [12], linha: 2, arquivo: '2011-Marcelo.pdf', 
  titulo: 'Emprego de Resiliência na Gerência de Redes de Computadores', 
  resumo_pt: 'A proteção da infraestrutura de redes e seus serviços são as maiores preocupações dos gerentes de redes nos dias de hoje. Este trabalho investiga o assunto através do estudo da resiliência em redes de computadores. E proposto um fator de resiliência que leva em consideração aspectos da topologia e também da quantidade de trafego perdido em condições de estresse. O fator proposto é testado em backbones de topologias reais, apresentando bons resultados. O fator pode ser utilizado para o suporte às decisões dos gerentes de redes em expansões na topologia, adição e retirada de enlaces. As alterações na topologia apresentam um aumento na robustez da rede, tornando-a menos vulneárvel a ataques e falhas.', 
  resumo_en: '--'},

 {numero: 639, ano: 2011, dia: '11/12', autor: 'Renato Hidaka Torres', orientador: [5], linha: 2, arquivo: '2011-Renato.pdf', 
  titulo: 'Desenvolvimento e Análise de Funções Criptográficas para Otimização dos Padrões de Dispersão em Criptogramas', 
  resumo_pt: 'Este trabalho apresenta duas novas funções para algoritmos criptográficos que tem como finalidade impedir a transmissão de assinaturas. A proposta dessas novas funções surgiram mediante a análise das vulnerabilidades do algoritmo AES perante ao ataque teórico Diferential Fault Analysis. Para a análise das funções, estas foram acopladas ao algoritmo AES e então foram geradas amostras de criptogramas a fim de verificar a transmissão de assinaturas existentes nos textos em claro. O algoritmo utilizado para a identificação de assinaturas também foi desenvolvido neste trabalho, assim como duas das quatro métricas de avaliação utilizadas no processo de avaliação da qualidade de dispersão das assinaturas. Com os resultados foi possível observar que o algoritmo AES, após as modificações realizadas, apresentou resultados satisfatórios em relação a dispersão das assinaturas existentes nos textos em claro, mesmo utilizando o modo de operação ECB.', 
  resumo_en: '--'},

 {numero: 638, ano: 2011, dia: '16/08', autor: 'Rodrigo Ribeiro Affonso Alves', orientador: [8], linha: 3, arquivo: '2011-Rodrigo.pdf', 
  titulo: 'Um Método para Auxiliar a Anotação de Serviços Web', 
  resumo_pt: 'Os serviços semânticos visam fornecer os meios necessários para a descoberta, composição e invocação automática de serviços Web. Os serviços semânticos facilitam a interoperabilidade entre sistemas distribuídos. O presente trabalho apresenta uma abordagem para auxiliar a anotação semântica em uma arquitetura orientada a serviços. A tarefa de anotação consiste em associar os elementos do serviço a conceitos representados por uma ontologia. O método proposto utiliza a categorização de documentos para classificar textos associados aos termos do serviço e a estrutura da ontologia para ponderar seus conceitos mais relevantes. Também é apresentado um método flexível para geração do corpus de treinamento do categorizador. Concluímos a proposta com os resultados preliminares gerados pela implementação do método e propomos uma discussão sobre as diferentes abordagens para automatizar a processo de anotação de serviços.', 
  resumo_en: '--'},

 {numero: 637, ano: 2011, dia: '03/08', autor: 'Eduardo Camargo', orientador: [203], linha: 1, arquivo: '2011-Eduardo.pdf', 
  titulo: 'Ferramenta de Visualização Científica Baseada no Algorítmo de Cálculo de Trajetória de Partículas em Unidades de Processamento Gráfico', 
  resumo_pt: 'A necessidade de processar grandes volumes de dados associada ao crescente poder computacional contido nas atuais placas gráficas têm incentivado a comunidade acadêmica a produzir técnicas paralelas aplicadas às placas gráficas destinadas ao processamento de aplicações científicas. Neste contexto, o objetivo da presente dissertação é apresentar a implementação de uma ferramenta de visualização científica baseada no algoritmo de cálculo de trajetória de partículas (CTP) em unidades de processamento gráfico utilizando-se, em especial, da Compute Unified Device Architecture (CUDA). Essa tarefa baseia-se na paralelização do algoritmo CTP já existente no laboratório de modelagem em hemodinâmica (HeMoLab), instalado no Laboratório Nacional de Computação Científica (LNCC). São ainda avaliados os ganhos de desempenho obtidos em relação a execução em uma única CPU e a combinação de algumas das configuraçõe oferecidas pela CUDA.', 
  resumo_en: '--'},

 {numero: 636, ano: 2011, dia: '18/07', autor: 'Victor de Almeida Thomaz', orientador: [4], linha: 2, arquivo: '2011-Victor.pdf', 
  titulo: 'Sistema de Controle para Ambiente CAVE', 
  resumo_pt: 'A Realidade Virtual permite oferecer uma experiência ilusória aos seus usuários, através da estimulação dos seus sentidos. Esta característica pode ser explorada no desenvolvimento de simulações para treinamento, sendo estas, extremamente úteis na formação e aperfeiçoamento de profissionais. Porém, imitar a realidade no contexto computacional não é uma tarefa simples. E preciso gerenciar recursos, apresentando imagens de forma coerente, além de permitir a interação em tempo real. Neste contexto, esta dissertação descreve a configuração e implementação de um sistema de controle, capaz de apresentar imagens em um ambiente imersivo, através de uma máquina hospedeira e um hardware dedicado para renderiação. O sistema suporta os dispositivos não convencionais de interação com o usuário, luva de dados e rastreador de posição, assim como os dispositivos joystick e mouse 3D. Uma aplicação, na qual o usuário consegue mover um objeto virtual, através dos dispositivos luva de dados e rastreador de posição, foi desenvolvida, para demonstrar os recursos do sistema. O objetivo do trabalho é apresentar o sistema de controle e sua implementação baseada na arquitetura Instant Reality e no padrão X3D. A dissertação descreve a configuração aplicada no sistema para exibir imagens estereoscópicas em grandes áreas de visualização, assim como a implementação e utilização de plugins dos dispositivos luva de dados e rastreador de posição. Por meio de plugins os movimentos dos usuários são detectados, possibilitando a manipulação de objetos 3D. O sistema também permite a navegação em ambiente virtual através do joystick e mouse 3D.', 
  resumo_en: '--'},

 {numero: 635, ano: 2011, dia: '26/05', autor: 'Celso Araujo Fontes', orientador: [8], linha: 3, arquivo: '2011-Celso.pdf', 
  titulo: 'Explorando Inferência em um Sistema de Anotação Semântica', 
  resumo_pt: 'A gestão da informação nas organizações tornou-se um importante desafio, especialmente quando documentos relevantes e estratégicos são armazenados na web, onde a maioria deles é gerada para a interpretação humana. Através do uso de metadados sobre alguns trechos de texto em um documento, aqui considerado como anotações semânticas, é possível facilitar a o processamento deste documento por meio de mecanismos automáticos, uma vez que facilita a recuperação de informações e a otimização da indexação deste documento. No entanto, devido à enorme quantidade de documentos existentes em uma organização, a ideia de gerar anotações semânticas para todos estes documentos parece ser uma tarefa complexa e não trivial. Este trabalho apresenta uma proposta para enriquecer automaticamente documentos com anotações semânticas, onde os termos do documento são anotados com o auxílio de uma ontologia de domínio. Atualmente já existem algumas ferramentas de anotação de documentos para automatizar esse processo. A contribuição e diferencial deste trabalho estão na sua capacidade de explorar a inferência ontológica e no conceito de metaanotação, que visa orientar os usuários e agentes automáticos no uso das anotações inferidas, através da informação sobre o raciocínio que as gerou.', 
  resumo_en: '--'},

 {numero: 634, ano: 2011, dia: '09/05', autor: 'Marlan Külberg', orientador: [4], linha: 2, arquivo: '2011-Marlan.pdf', 
  titulo: 'MINIVR:  Sistema Portátil de Exibição de Imagens Estereoscópicas', 
  resumo_pt: 'A visão estereoscópica oferece ao usuário uma sensação de imersão ambiente, uma vez que se promove a capacidade de identificar a profundidade de objetos ao observar simulações com essa tecnologia. Contudo, os equipamentos disponíveis para oferecer esse recurso, são de modo geral, de alto custo e possuem limitações quanto ao seu transporte. Dessa forma, para apresentar trabalhos de realidade virtual, geralmente são usados vídeos convencionais. Da mesma forma, aplicações de simulação que possam auxiliar usuários em treinamentos de determinadas situações, também ficam limitadas a necessidade de transportar esse usuário até onde existe o equipamento. O presente trabalho visa apresentar um aparelho 3D portátil e de baixo custo para exibição de imagens, chamado de MiniVR. Também é descrito o desenvolvimento de um aplicativo construído em X3D, com o propósito de prover uma simulação de treinamento de situações de emergência em uma plataforma de petróleo, de modo a permitir que tanto o equipamento quanto o software possam ser transportados até onde ocorrerá o uso, por parte dos interessados.', 
  resumo_en: '--'},

 {numero: 633, ano: 2011, dia: '17/01', autor: 'Gláucio Alves de Oliveira', orientador: [5], linha: 2, arquivo: '2011-Glaucio.pdf', 
  titulo: 'A Aplicação de Algoritmos Genéticos no Reconhecimento de Padrões Criptográficos', 
  resumo_pt: 'O princípio fundamental das cifras de bloco é a geração de criptogramas com uma distribuição que não estabeleça correlação com os dados de entrada (textos claros ou chaves). Estudos em modernos processos de criptografia evidenciam indícios de padrões de assinatura associados ao tipo de algoritmo ou a chave utilizados no processo de cifragem. Em um ataque somente com texto cifrado (Ciphertext-only attack), são poucas as informações disponíveis para um criptoanalista. Existe a necessidade de conhecer pelo menos o algoritmo criptográfico utilizado na cifragem. Neste contexto, este trabalho descreve o uso do Algoritmo Genético (AG) como um modelo de ferramenta para o agrupamento de criptogramas gerados por algoritmos criptográficos certificados pelo NIST (National Institute Standard Tecnology). Os ensaios realizados com o Algoritmo Genético realizaram o agrupamento de criptogramas gerados por algoritmos cifrantes distintos e pelo mesmo algoritmo cifrante com chaves distintas. Adicionalmente, uma técnica de classificação conhecida como Template Matching foi utilizada com o Algoritmo Genético. O Algoritmo Genético agrupador que utiliza a técnica de classificação é denominado neste trabalho como Algoritmo Genético modelado.', 
  resumo_en: '--'},

 {numero: 632, ano: 2011, dia: '11/01', autor: 'Max Silva Alaluna', orientador: [11], linha: 3, arquivo: '2011-Max.pdf', 
  titulo: 'Um Método para Extração e Anotação de Regras de Negócio em Sistemas JEE', 
  resumo_pt: 'Uma regra de negócio define ou restringe algum aspecto de um negócio. Isto pode definir a estrutura da organização ou influenciar no comportamento de um negócio em relação a um sistema de software. Além disso, os sistemas de software são alterados para corrigir falhas ou para adaptar um produto a uma mudança do ambiente do negócio, consequentemente modificar as regras de negócio. A medida que o software é alterado, ele se torna cada vez mais complexo e começa o problema de documentação deste sistema. Neste contexto, a extração das regras de negócio a partir do código fonte se torna importante principalmente se a documentação formal do sistema está desatualizada, incompleta ou inacessível. Este trabalho propõe um método semi-automático para realização de extração de regras de negócio em sistemas no padrão JEE, que consiste em vários passos como retirada das partes pouco significativas do código fonte, identificação das camadas do sistema, descoberta das variáveis de domínio, detecção das regras de negócio e anotação destas com meta-informação. O método proposto foi usado para desenvolvimento de uma ferramenta que permite uma extração das regras de negócio e anotação, ou seja, documentação.', 
  resumo_en: '--'},

 {numero: 631, ano: 2010, dia: '01/09', autor: 'João Carlos Santiago Filho', orientador: [11], linha: 3, arquivo: '2010-Joao.pdf', 
  titulo: 'Estudo e Aplicação de Mapas Auto-Organizáveis na Identificação e Delimitação de Regiões de Influência das Cidades Brasileiras', 
  resumo_pt: 'O estudo de redes complexas, como as urbanas e sociais, oferece subsídios para a compreensão da organização espacial da sociedade e ações de planejamento estratégico de empresas e de governos. Nesta dissertação propõe-se uma metodologia que, diferentemente de abordagens como a análise de centralidade em redes segundo aspectos meramente estruturais, ou da extração de grupos em grafos em função da similaridade entre os vértices, concilia e estende o conceito de associação dominante entre cidades com as reconhecidas propriedades de projeção e quantização vetorial dos mapas auto-organizáveis ou SOM. O modelo resultante é composto por uma topologia hierárquica e estruturalmente simplificada, mas capaz de simultaneamente representar adequadamente o mecanismo de associação observado em redes urbanas, oferecer suporte à determinação de centros locais e respectivas regiões de influência e complementarmente capturar padrões de grupos de cidades similares. Apresenta-se uma opção de representação visual dos resultados obtidos e é investigado o potencial de captura de grupos e de localidades centrais, exclusivamente por meio da análise das rotas que ligam os centros urbanos que compõem a rede. A metodologia aqui proposta foi aplicada ao conjunto das 5274 cidades brasileiras descritas por seus atributos sócio-econômicos e respectivos relacionamentos representados por dados oficiais do governo brasileiro sobre o transito inter-estadual de passageiros terrestres e aéreos. O resultados obtidos pela presente proposta quando comparados com àqueles resultantes por técnicas tradicionais demonstram um grande potencial desta abordagem, particularmente a análise de padrões de grupos determinados por conjuntos de conexões dominantes entre as cidades. O método, apesar de não testado explicitamente em outras redes que não as urbanas, possui propriedades que o torna plenamente adaptável para a análise de outros modelos de redes hierárquicas como as redes sociais.', 
  resumo_en: '--'},

 {numero: 630, ano: 2010, dia: '17/08', autor: 'Lucas Lacerda Albuquerque', orientador: [4], linha: 2, arquivo: '2010-Lucas.pdf', 
  titulo: 'Uma Camada de Comunicação de Dados para Construção de Aplicações de Realidade Virtual Colaborativas para o Motor EnCIMA', 
  resumo_pt: 'A necessidade de maior velocidade na transmissão de informações através da camada de comunicação de dados motivou a realização de melhorias no motor EnCIMA - destinado a construção de aplicações de RV. O motor conta com uma arquitetura que utiliza o protocolo TCP e um servidor centralizador de pacotes. A proposta foi alterar esta arquitetura, substituindo o protocolo de comunicação atual pelo UDP e deixando a comunicação direta entre os clientes, afim de ter, como objetivo, uma melhoria na velocidade da transmissão da informação para aqueles que participam da conexão, além de evitar o gargalo causado pelo servidor, existente na arquitetura original do motor. Para validar a modificação após o desenvolvimento da camada proposta, foi construído um cenário virtual utilizando o motor EnCIMA, com alguns clientes conectados na mesma cena recebendo as informações enviadas uns pelos outros. Experimentos também demonstraram que a camada sugerida apresentou um tempo médio de retardo inferior ao da arquitetura original do motor, aumentando a velocidade de transporte de pacotes, além de evitar o gargalo da comunicação anteriormente existente no motor devido à arquitetura centralizada.', 
  resumo_en: '--'},

 {numero: 629, ano: 2010, dia: '16/08', autor: 'Fabio Henrique Silva', orientador: [12], linha: 2, arquivo: '2010-Fabio.pdf', 
  titulo: 'Auto-Balanceamento de Carga em Redes de Computadores Utilizando Protocolos de Roteamento do Tipo Estado de Enlace', 
  resumo_pt: 'O desempenho da rede é visto pelos usuários finais de serviços de rede como o ponto mais importante a ser tratado. A aplicação de conceitos de Engenharia de Tráfego ajuda a melhorar as propriedades emergentes da rede visíveis aos usuários finais. Um desses conceitos é denominado Balanceamento de Carga, que visa minimizar o congestionamento e otimizar a alocação eficiente de recursos. Alguns estudos e tecnologias buscam a otimização da rede através da alteração dos caminhos por onde os pacotes de dados trafegam. Uma das propostas existentes inclui técnicas de implementação do Autobalanceamento de carga em um Sistema Autônomo que utilize o protocolo de estado de enlace OSPF. O presente trabalho propõe a construção de um Sistema Autônomo AutoBalanceado a se realizar através da construção de uma ferramenta que analisa os fluxos de dados nos enlaces, e ao constatar a ocorrência de sobrecarga, aciona um algoritmo baseado em heurísticas de cálculo de alteração do custo do enlace modelado. Resultados experimentais realizados em testes de laboratório mostram a viabilidade e o desempenho da proposta.', 
  resumo_en: '--'},

 {numero: 628, ano: 2010, dia: '10/08', autor: 'Debora Helena Job', orientador: [12], linha: 2, arquivo: '2010-Debora.pdf', 
  titulo: 'Arquitetura para a Camada de Agregação em Redes Tolerantes a Atrasos e Desconexões', 
  resumo_pt: 'A Internet é um fenômeno, devido em grande parte, a popularidade do protocolo TCP/IP. No entanto, a arquitetura da Internet não funciona eficientemente em alguns ambientes desafiadores de redes encontrados hoje. Tais ambientes compartilham a comum inabilidade de estabelecer e manter uma sessão de comunicação fim-a-fim com pouca perda e baixa latência. As Redes Tolerantes a Atrasos e Desconexões (DTN) surgem neste contexto propondo uma nova camada Bundle Layer para o modelo convencional de redes, com o objetivo de realizar dentre outras funcionalidades, o armazenamento persistente dos dados e o encaminhamento de mensagens salto a salto. A arquitetura DTN é baseada em uma abstração de entrega de mensagens muito similar ao correio eletrônico. As similaridades observadas tanto no correio eletrônico como na arquitetura DTN motivaram uma implementação para camada de agregação, fundamentada na arquitetura de correio eletrônico, que tem sido extensivamente testada, e provou ser confiável e robusta. Com base em todos os trabalhos pesquisados pode-se afirmar que este é o primeiro trabalho que emprega o correio eletrônico, não como uma aplicação, mas como o módulo principal para funcionar como se fosse a camada de agregação da arquitetura DTN. A arquitetura proposta foi elaborada, implementada e testada. Os testes foram realizados em um ambiente emulado e em um ambiente real, e os resultados obtidos confirmaram a viabilidade e a boa performance do modelo desenvolvido nesta dissertação.', 
  resumo_en: '--'},

 {numero: 627, ano: 2010, dia: '06/08', autor: 'Henrique de Medeiros Klôh', orientador: [10], linha: 2, arquivo: '2010-Henrique.pdf', 
  titulo: 'Modelo de Escalonamento Bi-critérios de Workflow em Grades', 
  resumo_pt: 'O trabalho a seguir apresenta uma proposta de um modelo de escalonamento bicritérios de Workflows em Grades que tem por base um algoritmo de escalonamento de Workflows híbrido bi-critérios com suporte a CoS. O modelo proposto tem por objetivo otimizar os critérios escolhidos pelos usuários e com base na ordem de prioridade por eles especificados. A fim de validar este modelo foi realizado um conjunto de testes que comparam o desempenho do modelo proposto com uma política de escalonamento do tipo Join the Shortest Queue (JSQ). Em função dos testes realizados e dos resultados obtidos foi possível observar um ganho de desempenho em termos de tempo, custo e confiabilidade, e uma melhoria na qualidade de serviços ao atender a priorização de critérios definida pelos usuários.', 
  resumo_en: '--'},

 {numero: 626, ano: 2010, dia: '05/08', autor: 'Thais Cabral de Mello', orientador: [10], linha: 2, arquivo: '2010-Thais.pdf', 
  titulo: 'Ambiente para Criação de Clusters Virtuais em Grids Computacionais', 
  resumo_pt: 'O desenvolvimento cientifico e tecnológico observa uma crescente necessidade e dependência de recursos computacionais de alto desempenho e capacidade de armazenamento. Buscando suprir essa demanda, surgiram novas tecnologias tanto em termos de capacidade de processamento, quanto em capacidade de redes de comunicação, o que possibilitou um avanço no uso mais efetivo e compartilhado dos recursos computacionais, principalmente pelo desenvolvimento de processadores dedicados à virtualização. Por conseguinte, esse avanço da tecnologia de virtualização possibilitou que seus usuários criassem ambientes personalizados, beneficiando, sobremaneira, os usuários da computação de alto desempenho ao permitir a criação de clusters compostos por máquinas virtuais configuradas para atender suas necessidade especificas de software e de hardware. Com base nesse conceito, a presente dissertação disponibiliza uma infra-estrutura que facilita a criação de ambientes de cluster virtual, permitindo, assim, que seus usuários sejam capazes de criar e utilizar um ambiente personalizado, recuperável, escalável e de fácil uso e configuração, seja para fins educativos, de treinamento, seja para testes ou desenvolvimento. Além disso, é realizada uma avaliação de desempenho de clusters virtuais criados através desta infraestrutura com o objetivo de avaliar a viabilidade de seu uso por aplicações HPC (High Performance Computing), ressaltando-se os fatores considerados determinantes para adoção desse tipo de solução.', 
  resumo_en: '--'},

 {numero: 625, ano: 2010, dia: '15/07', autor: 'Douglas Ericson Marcelino de Oliveira', orientador: [10], linha: 2, arquivo: '2010-Douglas.pdf', 
  titulo: 'Otimização das Aplicações de Visualização Científica usando o QEF', 
  resumo_pt: 'A visualização científica é uma área da computação interessada em técnicas que permitem aos cientistas criarem representações gráficas de conjuntos de dados gerados por simulações computacionais ou instrumentos de aquisição. Para abordar o custo computacional das tarefas de visualização, especialmente para grandes conjuntos de dados, os pesquisadores têm explorado ambientes de grade como uma plataforma para a sua avaliação e execução paralela. Contudo, não é trivial se adaptar cada técnica de visualização para executar em um ambiente de grade. Uma alternativa desejável seria separar as especificidades dos dados e distribuição dos processos nas grades do ponto de vista lógico do processamento da visualização. Este trabalho tem como objetivo utilizar o QEF (Query Evaluation Framework) para a computação da visualização científica com as características acima mencionadas. As técnicas de processamento da visualização são modeladas como operadores em uma álgebra e integradas com um conjunto de operadores de controle que gerenciam a distribuição de dados conduzindo à um QEP (Query Execution Plan) paralelo. A abordagem algébrica implementada pelo QEF, fornece uma extensibilidade para diferentes técnicas de visualização científica. E apresentado os benefícios da paralelização para três dessas técnicas de visualização: computação da trajetória de partículas, volume rendering e isosurface. Para as trˆes técnicas, os experimentos demonstram muitos aspectos positivos e limitações da solução apresentada, bem como oportunidades de trabalhos futuros.', 
  resumo_en: '--'},

 {numero: 624, ano: 2010, dia: '08/07', autor: 'Lívia de Souza Ribeiro', orientador: [8], linha: 3, arquivo: '2010-Livia.pdf', 
  titulo: 'Utilizando Proveniência para Complementação de Dados no Contexto do processo de ETL', 
  resumo_pt: 'Os dados contidos em um Data Warehouse (DW) típico são provenientes de diversas fontes. É necessário que os valores contidos no DW apresentem boa qualidade para que em uma futura análise dos mesmos seja apresentado resultado coerente. Entretanto, dados de algumas fontes podem não estar disponíveis em determinadas datas/períodos. Assim, não é incomum ocorrer ausência de valores na tabela de fatos do DW. No processo de transferência de dados para o ambiente de DW é conhecido como ETL (Extration, Tranformation, and Load). A etapa de Transformação, que faz parte deste processo, tem como objetivo principal melhorar a qualidade dos dados, amenizando os problemas que podem ocorrer nas fontes de dados. Assim sendo, é esta etapa que se encarrega de tratar os dados ausentes no ambiente de DW. Uma das abordagens utilizadas na resolução do problema da ausência dos valores nas tabelas é a técnica de imputação de dados. Esta técnica consiste no preenchimento das ausências em uma tabela com novos valores. Entre as técnicas de imputação, a mais utilizada é a observação dos valores presentes na tabela para a geração de um novo valor. No entanto, esta técnica não leva em consideração o enriquecimento da tabela com outros atributos. No contexto do processo ETL os valores das dimensões podem ser utilizados para o enriquecimento da tabela de fatos, no sentido de obter melhores resultados na imputação de dados. Este trabalho tem como proposta o desenvolvimento de uma estratégia para tratar do problema de ausência de valores na tabela de fatos de um DW, durante o processo de ETL, considerando o enriquecimento da mesma. A abordagem de imputação que utiliza o algoritmo k-NN foi utilizada na estratégia. A ferramenta ComplETL foi construída de acordo com a estratégia proposta e testes foram realizados para avaliá-la, mostrando resultados promissores.', 
  resumo_en: '--'},

 {numero: 623, ano: 2010, dia: '01/07', autor: 'Sidney Nicolau Venturi Filho', orientador: [8], linha: 3, arquivo: '2010-Sidney.pdf', 
  titulo: 'Tratamento Semântico de Documentos em Sistemas Gerenciadores de Banco de Dados Relacionais', 
  resumo_pt: 'O volume crescente de documentos gerados pelas organizações torna a sua administração um grande desafio, dificultando a recuperação das informações neles existentes, a partir das necessidades dos usuários. Para lidar com tal desafio sistemas de recuperação de informações emergiram, porém eles são fortemente baseados na sintaxe das palavras, fazendo pouco uso da semântica dos documentos. Para superar tal limitação tecnologias de Web Semântica, como OWL e anotações semânticas e raciocinadores, podem fornecer uma solução mais eficiente. Por outro lado Sistemas Gerenciadores de Banco de Dados Relacionais (SGBDR) são reconhecidamente eficientes para administrar grandes volumes de dados. Este trabalho propõe adicionar uma camada de tecnologia semântica sobreposta a um SGBDR típico. Esta proposta chamada Gerenciador de Documentos Anotados Semanticamente (GDAS) fornece suporte para o armazenamento de documentos e um mecanismo de recuperação semântica. Um documento é anotado semanticamente ao se associá-lo aos conceitos de uma ontologia. Baseando-se nestas anotações um mecanismo de mapeamento deriva novas associações, expandindo o poder de recuperação do sistema. A fim de avaliar a proposta foi criado um protótipo e realizados teste de validação da arquitetura.', 
  resumo_en: '--'},

 {numero: 622, ano: 2009, dia: '26/10', autor: 'Felipe Fernandes Albrecht', orientador: [10], linha: 2, arquivo: '2009-Felipe.pdf', 
  titulo: 'Algoritmo otimizado para comparação de sequências de busca em base de dados', 
  resumo_pt: 'A busca por sequências genéticas similares em base de dados é uma das tarefas básicas na bioinformática. Porém as bases de dados de sequências genéticas têm sofrido um crescimento exponencial tornando o baixo desempenho desta busca um problema. O aumento do poder computacional dos processadores tem sido alcançado através da utilização de vários núcleos de processamento em um único chip. As técnicas de busca de sequências genéticas que utilizam estrutura de dados mais otimizadas, como os índices invertidos, não utilizam estes núcleos de processamento extras. Este trabalho visa utilizar de forma conjunta as técnicas de indexação da base de dados de sequências genéticas e a paralelização do processo de busca de sequências similares. Durante este trabalho foi desenvolvido um protótipo que utiliza técnicas de paralelização e índices invertidos para a verificação da viabilidade de utilizar estas duas técnicas simultaneamente. Foram executados experimentos para analisar o ganho de desempenho quando utilizados índices invertidos e paralelismo e a qualidade dos resultados quando comparados com outras ferramentas de busca. Os resultados foram promissores, pois o ganho com paralelismo chega a ultrapassar o speedup linear, a execução com paralelismo é em média 20 vezes mais rápida do que a ferramenta NCBI BLAST quando este também usa paralelismo. O protótipo encontrou mais de 70% dos alinhamentos reportados pelo BLAST para e-values iguais ou inferiores a 10e-15, mostrando assim sua eficácia para encontrar sequências genéticas similares.', 
  resumo_en: '--'},

 {numero: 621, ano: 2009, dia: '13/08', autor: 'Donato Antonio Marino Junior', orientador: [12], linha: 2, arquivo: '2009-Donato.pdf', 
  titulo: 'Estratégias e Métricas para Resiliência em Redes de Computadores', 
  resumo_pt: 'O uso da Internet para aplicações críticas e aplicações de tempo real está crescendo a cada dia. As falhas nos roteadores ou nos enlaces e ataques direcionados contra as redes afetam todo o tipo de tráfego, principalmente as aplicações de tempo real. A lenta recuperação e convergência da infraestrutura de rede causada por estas falhas podem tornar estes serviços inviáveis. Esta dissertação propõe uma medida de robustez de redes, baseada em mé- tricas selecionadas a partir de estudos em teoria dos grafos. O proposto fator de resiliência reflete o grau de tolerância a falhas e ataques de uma rede, servindo de medida para novos projetos ou alterações na topologia já existente, objetivando a melhoria da confiabilidade e robustez. Este trabalho mostra que o fator de resiliência indica de forma eficaz a robustez de uma topologia, comparando-o com métricas utilizadas em trabalhos anteriores. Testes utilizando topologias reais, em simulações de ataques aos nós de maior centralidade da rede, validam o fator de resiliência apresentado. Este trabalho também apresenta duas estratégias de alteração de topologias, envolvendo o remanejamento e a inserção de enlaces, utilizando o fator proposto como indicador do aumento da resiliência das topologias alteradas. Os resultados obtidos com as estratégias sugeridas indicam uma melhoria na tolerância a falhas e ataques nas topologias testadas.', 
  resumo_en: '--'},

 {numero: 620, ano: 2009, dia: '24/06', autor: 'Claudio Vasconcelos Ribeiro', orientador: [11], linha: 3, arquivo: '2009-Claudio.pdf', 
  titulo: 'Um Ambiente para Previsão de Séries Temporais Utilizando Comitês de Aprendizado', 
  resumo_pt: 'Previsão de séries temporais é um desafio da área de Mineração de Dados. Prever valores futuros, em função de valores passados, tem se tornado um assunto de especial interesse na academia e na indústria, com aplicações em planejamento de produção, matriz energética e mercado de ações, dentre outras. No entanto, não existe um método de previsão que possa ser aplicado com eficiência a todos os tipos de séries. Neste sentido, o uso de Comitês de Máquinas de Aprendizado busca um melhor aproveitamento das potencialidades individuais de cada método ao procurar combiná- las. Contudo, a utilização de tais comitês requer do pesquisador o desenvolvimento de sistemas computacionais dedicados para integrar diversos métodos de previsão, que poderão ser modelados a partir de diferentes paradigmas. Este trabalho apresenta um ambiente para criação e uso de comitês de máquinas de aprendizado. Tal ambiente permite a adição, seleção, e avaliação de métodos de previsão de séries temporais de forma isolada ou combinada em comitês de máquinas de aprendizado. Alguns experimentos foram realizados para analisar o ambiente proposto. Estes experimentos também serviram para verificar o desempenho do uso de alguns comitês de máquinas de aprendizado na previsão de séries temporais.  ', 
  resumo_en: '--'},

 {numero: 619, ano: 2009, dia: '24/03', autor: 'Milene Pereira Guimarães', orientador: [8], linha: 3, arquivo: '2009-Milene.pdf', 
  titulo: 'Uma abordagem para capturar a proveniência de dados na área de bioinformática', 
  resumo_pt: 'Atualmente, os centros de pesquisa genômicos no Brasil e no mundo, utilizam sistemas de informação para processamentos e análises de genes e posterior armazenamento desses dados. Sistemas de anotação genômica foram desenvolvidos para dar suporte aos experimentos, muitos deles realizados com o auxílio de computadores. Mais recentemente, esses sistemas passaram a ser utilizados em conjunto com esquemas de bancos de dados genômicos, para permitir o armazenamento da quantidade massiva de dados produzidos por esses sistemas. Uma vez armazenados, torna-se possível a realização de análises sobre esses dados, e ainda a captura da proveniência, pois tão importante quanto os dados produzidos é a sua proveniência. Por exemplo, para facilitar a certificação de uma anotação genômica, é necessário efetuar a ligação entre esta anotação e os dados usados como fonte e o processo usado para gerá-la. Esta ligação é chamada de proveniência da anotação, atualmente já capturada por alguns esquemas de banco de dados genômicos. No entanto, os sistemas de anotação existentes ainda não fazem um amplo uso das estruturas providas para o registro da proveniência da anotação. Além dos esquemas de banco de dados genômicos, existem atualmente algumas iniciativas mais genéricas para a captura de proveniência, como as propostas de metamodelagem da proveniência de dados e os Sistemas de Gerência de Workflow Científicos (SGWfC), que apresentam facilidades para a captura da proveniência. No entanto, essas iniciativas ainda apresentam estruturas carentes para o registro de dados de proveniência e nenhuma delas oferece uma interface de consulta focada na recuperação de dados de proveniência, (e.g. proveniência da anotação). Tendo como base as carências no processo de captura da proveniência de dados, encontradas em aplicações e cenários da Bioinformática, este trabalho apresenta um Metamodelo para Captura de Proveniência e a arquitetura ArCaP, para dar apoio ao desenvolvimento de aplicações em que se garante esta captura e ainda facilite o acesso a tal informação, não só na área de Bioinformática como também em outras áreas. Para demonstrar a viabilidade e exemplificar a funcionalidade da abordagem proposta, o protótipo SisCaP foi implementado e foi apresentado um exemplo de uso real do protótipo, tendo como base uma parte do workflow de Bioinformática STINGRAY, utilizado por laboratórios da Fundação Oswaldo Cruz e por membros do grupo de pesquisa BioWebDB há alguns anos.', 
  resumo_en: '--'},

 {numero: 618, ano: 2009, dia: '06/02', autor: 'Matheus Bousquet Bandini', orientador: [12], linha: 2, arquivo: '2009-Matheus.pdf', 
  titulo: 'Qualidade de serviço em grades computacionais utilizando acordos em nível de serviço', 
  resumo_pt: 'Este trabalho apresenta um sistema que aplica um modelo de fornecimento de Qualidade de Serviço em Grades Computacionais (Grid–QoS) utilizando Acordos em Nível de Serviço (SLAs). Além disso, o sistema desenvolvido faz uso de uma interface web intuitiva que facilita a interação entre os usuários e o Sistema. Ao mesmo tempo, é proposto um novo processo de cobrança pelos serviços diferenciados. Essa cobrança é feita a partir da caracterização de Planos de Serviço que estabelecem níveis de prioridade e da classificação dos recursos disponíveis na Grade. O uso desses fatores foi considerado com o objetivo de garantir que os níveis de Qualidade de Serviço solicitados pelos usuários sejam, de fato, atendidos e para que os recursos da Grade não sofram degradação de desempenho em decorrência da elevada taxa de utilização de tais recursos.', 
  resumo_en: '--'},

 {numero: 617, ano: 2009, dia: '29/01', autor: 'Luciane Machado Fraga', orientador: [3], linha: 1, arquivo: '2009-Luciane.pdf', 
  titulo: 'Proposta de um Método para Otimizar Detecção de Colisões Utilizando Áreas de Interesse', 
  resumo_pt: 'Um dos principais desafios para aplicações de Realidade Virtual é a simulação em tempo real do comportamento realista de objetos em um ambiente virtual, principalmente no que se refere a detecção de colisão entre objetos em movimento. A quantidade de objetos envolvidos em uma simulação, bem como a complexidade geométrica dos objetos, está diretamente relacionada à quantidade de testes de interseção que devem ser realizados no processo de detecção de colisão. Isso faz com que no pior dos casos, testes sejam realizados entre todas as primitivas que compõem os objetos da cena, resultando em uma complexidade O(n2). Com a finalidade de otimizar o processo de detecção de colisão, a fase de descarte utiliza-se de métodos que identificam os pares de objetos que realmente podem colidir devido a estarem próximos no ambiente virtual. Desse modo, testes de interseção somente são realizados entre as primitivas dos pares de objetos que foram identificados pela fase de descarte. Devido a sua natureza estática, métodos utilizados na fase de descarte acabam não levando em consideração o caráter dinâmico dos ambientes virtuais, prejudicando deste modo o desempenho das aplicações devido a sobrecarga resultante das atualizações necessárias. Neste sentido, este trabalho estuda os principais métodos utilizados na fase de descarte e apresenta o método AoIP (Area of Interest Partitioning) para a redução do número de objetos a serem considerados nos testes de colisão em ambientes virtuais.', 
  resumo_en: '--'},

 {numero: 616, ano: 2009, dia: '28/01', autor: 'Robertson Schitcoski', orientador: [4], linha: 2, arquivo: '2009-Robertson.pdf', 
  titulo: 'Uma arquitetura modular para sistemas de treinamento militar em operações táticas', 
  resumo_pt: 'Desde os primórdios da civilização, os jogos de guerra têm sido empregados como uma importante ferramenta de simulação, para generais, e doutrinação, para seus comandados. No princípio, os jogos de guerra se limitavam a peças de madeira ou osso dispostas em tabuleiros ou caixas de areia. Com a evolução das tecnologias militares os jogos se tornaram mais complexos, tornando necessário o uso de vários manuais para decidir os combates simulados. Com o advento da Segunda Guerra Mundial, surgiram os computadores, empregados a princípio em cálculos balísticos e na quebra de códigos dos inimigos. Logo se percebeu que eles poderiam ser empregados como uma ferramenta de simulação de combates. Por muito tempo os principais jogos de guerra que eram executados pelos computadores foram os simuladores de veículos militares. Com o avanço das tecnologias computacionais e o advento dos jogos voltados para o entretenimento, começaram então a surgir vários jogos de guerra táticos e estratégicos, capazes de simular as mais variadas condições de combate presentes nos campos de batalha. O Exército Brasileiro ainda se encontra nos estágios iniciais de desenvolvimento de jogos de guerra, estando concentrado, sobretudo, em sistemas de treinamento a nível estratégico, com jogos como o AZUVER, o SISTAB e o SABRE. Levando tal fato em conta, este trabalho tem como proposta uma arquitetura de software que sirva como modelo de projeto e incentivo a pesquisa para o desenvolvimento de softwares de simulação táticos para o Exército Brasileiro.  A arquitetura aqui proposta foi fundamentada nos princípios modernos de engenharia de software, empregando o paradigma de orientação a objeto e os padrões de projeto. Ela é derivada do estilo arquitetural de sistemas centrados em dados, composta por um núcleo de gerenciamento de objetos e vários módulos independentes. Esses módulos são responsáveis pela manipulação dos dados dos seguintes domínios lógicos: entrada de dados, interface do usuário, gráficos tridimensionais, rede, inteligência artificial, áudio e efeitos físicos.', 
  resumo_en: '--'},

 {numero: 615, ano: 2009, dia: '27/01', autor: 'Silvano Maneck Malfatti', orientador: [4], linha: 2, arquivo: '2009-Silvano.pdf', 
  titulo: 'ENCIMA - Um motor para o desenvolvimento de aplicações de realidade virtual', 
  resumo_pt: 'A implementação de aplicações baseadas em Realidade Virtual não é uma tarefa trivial, tendo em vista que implementar um sistema que estimule diversos sentidos do participante em tempo real e mantenha o resultado coerente não é uma tarefa simples. Assim como para o desenvolvimento de jogos eletrônicos, as aplicações de Realidade Virtual necessitam gerenciar uma grande quantidade de recursos. Neste contexto, a utilização de um motor gráfico tende a agilizar o processo de criação, tendo em vista que é possível trabalhar por intermédio de uma interface de programação em alto nível. No entanto, apesar dos motores destinados ao desenvolvimento de jogos oferecerem diversas funcionalidades gráficas, eles possuem pouco suporte a dispositivos especiais de interação com o usuário, que são de fundamental importância para o desenvolvimento de aplicações de Realidade Virtual. Nesta dissertação, é apresentado o projeto e implementação de um motor gráfico denominado EnCIMA (Engine for Collaborative and Imersive Multimedia Applications) desenvolvido especialmente para facilitar a implementação de aplicações de Realidade Virtual Colaborativa. Dentre as principais características do EnCIMA estão o suporte a diversos dispositivos especiais de interação com o usuário como luva de dados, Phantom, rastreadores de posição, mouse 3D e joystick com force feedback, além de comunicação via rede e um conjunto básico de recursos gráficos, com suporte a estereoscopia, que permitem a implementação de aplicações com um grau maior de realismo. Como forma de validação para o motor proposto, foram implementadas duas aplicações como estudo de caso: um simulador de plataforma de petróleo e um atlas virtual 3D que utiliza modelos 3D formados por malhas complexas. Através dos resultados obtidos com estas aplicações, foi possível concluir que o EnCIMA apresentou desempenho igual ou superior a diversas ferramentas gráficas e motores avalidados durante o desenvolvimento do trabalho.', 
  resumo_en: '--'},

 {numero: 614, ano: 2009, dia: '26/01', autor: 'Marcelo Reis da Silva', orientador: [12], linha: 2, arquivo: '2009-Marcelo.pdf', 
  titulo: 'Adaptação da Camanda de Transporte Através da Correta Seleção de Algoritmo de Controle de Congestionamento', 
  resumo_pt: 'Hoje em dia, as transmissões sem fio, junto com a rápida expansão de redes de alta velocidade, impõem novos desafios aos protocolos de transporte de dados. Dentre estes desafios, destacam-se as conexões de longo round trip time (RTTs), as conexões com taxas de erros de pacotes (PER) não desprezíveis (HF, por exemplo), e grandes largura de banda. Para superar estes desafios, um grande número de variantes TCP é apresentado na literatura com finalidades e propósitos diferentes. Entretanto, como a maioria das propostas são desenvolvidas para solucionar problemas diferentes, elas representam otimizações para redes específicas. Consequentemente, dado o nível crescente de heterogeneidade das redes atuais e futuras, a escolha da melhor abordagem de transporte parece um problema em aberto. Este trabalho propõe uma metodologia para a seleção de diferentes versões de protocolos de transporte caracterizando uma camada de transporte adaptativa. A proposta é simples, de baixo custo e baseada em experimentos reais. Alguns critérios, a serem adotados para a seleção do transporte de dados, são discutidos. São fornecidos resultados relativos a topologias de rede reais, escolhidas para ilustrar as vantagens de uma camada de transporte adaptativa. Tais resultados são extremamente encorajadores e justificam as observações que se seguem sobre a viabilidade e implementação de uma camada deste tipo.', 
  resumo_en: '--'},

 {numero: 613, ano: 2009, dia: '26/01', autor: 'Jeronymo Mota Alves de Carvalho', orientador: [12], linha: 2, arquivo: '2009-Jeronymo.pdf', 
  titulo: 'Arquitetura para Controle de Congestionamento e Tarifação de Tráfego Não-Cooperativo', 
  resumo_pt: 'Este trabalho propõe uma arquitetura capaz de evitar o congestionamento do tráfego TCP causado pela intensa utilização de tráfego UDP, desestimulando o ingresso deste último sempre que o primeiro estiver experimentando baixo desempenho. Um esquema de tarifação e reserva de recursos é imposto a todo tráfego que utilize o protocolo UDP. O cálculo da tarifa é feito em função do prejuízo causado ao TCP pelo UDP. O conhecimento do estado dos enlaces é obtido através da utilização de tecnologia de fluxos. A reserva de recursos é feita através de regras de firewall e utilização do escalonador custom-queueing. Uma implementação da arquitetura foi realizada e testes comprovam a eficácia da solução.', 
  resumo_en: '--'},

 {numero: 612, ano: 2009, dia: '22/01', autor: 'Adriano de Oliveira Cunha', orientador: [11], linha: 3, arquivo: '2009-Adriano.pdf', 
  titulo: 'Catalogação do Requisito Não Funcional Transparência', 
  resumo_pt: 'A transparência do sistema é importante para garantir a seus atores a correta visualização das informações. A elaboração de métodos, técnicas e ferramentas para tratar este Requisito Não Funcional é fundamental para o desenvolvimento de softwares no futuro. É importante pensar a transparência desde os momentos iniciais do projeto, pois a sua inclusão tardia, após a conclusão do mesmo, apresenta custos significativamente mais elevados. Este trabalho apresenta um catálogo de conceitos e métodos de refinamento que contribuem para garantir a transparência dos sistemas desenvolvidos. A aplicação dos métodos propostos é demonstrada através de estudos de casos baseados em modelos de sistema amplamente utilizados na literatura. Através destes estudos de casos mostra-se a modelagem dos requisitos de transparência desde a sua definição até a sua operacionalização.', 
  resumo_en: '--'},

 {numero: 611, ano: 2009, dia: '15/01', autor: 'Ricardo Queiroz de Araujo Fernandes', orientador: [1], linha: 1, arquivo: '2009-Ricardo.pdf', 
  titulo: 'Uma Plataforma para Análise Sintática e sua Aplicação ao Português', 
  resumo_pt: 'Esta dissertação apresenta o desenvolvimento de uma plataforma de análise sintática, bem como seu uso para o desenvolvimento de um analisador sintático para o Português. Esse analisador sintático é ferramenta necessária para o desenvolvimento de diversas aplicações de processamento de texto escrito, em especial aplicações de Recuperação de Informações. Foram aplicadas técnicas de processamento de linguagens de programação para tratar problemas de explosão combinatória do número de árvores de derivação geradas e para tratar a manutenibilidade da Gramática Livre de Contexto. Entre essas técnicas, destacamos a codificação de fenômenos linguísticos, como concordância e regência, em Gramática de Atributos.', 
  resumo_en: '--'},

 {numero: 610, ano: 2008, dia: '25/11', autor: 'Rafael Castaneda Ribeiro', orientador: [11], linha: 3, arquivo: '2008-RafaelRibeiro.pdf', 
  titulo: 'Um Ambiente de Imputação Sequencial para Cenários Multivariados', 
  resumo_pt: 'Uma dificuldade comum aos processos de descoberta de conhecimento em bases de dados é a existência de valores ausentes, especialmente quando estes ocorrem de maneira multivariada. O procedimento de imputação sequencial é uma abordagem comum para a complementação dos dados, imputando os valores ausentes em cada atributo, um por vez. Porém, pesquisadores comumente divergem sobre as práticas de imputação sequencial, como a reutilização de valores imputados entre a imputação de diferentes atributos. Tal divergência é fruto, dentre outros fatores, da falta do estabelecimento de metodologias e ferramentas capazes de fornecer as bases para experimentação e avaliação dos métodos de imputação sequencial. Esta dissertação oferece uma abordagem teórica e prática em imputação sequencial, com a proposta de uma metodologia para execução e avaliação de métodos de imputação sequencial, e a implementação da metodologia na forma de um ambiente de imputação baseado nos principais conceitos de workflows. Este trabalho mostra que o ambiente é capaz de automatizar diversos experimentos, que seriam normalmente conduzidos manualmente, um-a-um. Um estudo de caso é conduzido no ambiente, para explorar questões em aberto relativas ao reuso de valores em processos de imputação sequencial, e para demonstrar as principais funcionalidades do ambiente. Este estudo de caso mostra que o reuso de valores foi capaz de aprimorar a precisão dos procedimentos de imputação, na maior parte dos experimentos realizados.', 
  resumo_en: '--'},

 {numero: 609, ano: 2008, dia: '29/08', autor: 'Flávio Flores Villaça', orientador: [9], linha: 2, arquivo: '2008-Flavio.pdf', 
  titulo: 'Planejamento de Trajetória e Coordenação Dinâmica de Robôs Cooperativos', 
  resumo_pt: 'Neste trabalho é abordado o problema de planejamento de trajetória para um sistema com dois robôs móveis realizando a tarefa de carregar uma barra de um ponto de origem a um ponto de destino. Para a execução desta tarefa, devido ao fato da barra estar apoiada sobre os dois robôs, os mesmos devem agir cooperativamente, de maneira que não haja colisões com eventuais obstáculos presentes no ambiente e, além disso, deve-se levar em consideração as restrições cinemáticas impostas ao movimento conjunto dos robôs. A solução proposta apresenta duas fases distintas para resolver o problema de planejamento de trajetória: (i) aplicar um algoritmo de planejamento de trajetória ao mapa e (ii) suavizar a trajetória gerada levando em consideração a cinemática dos robôs. A primeira fase consiste em aplicar ao mapa do ambiente um algoritmo de planejamento de trajetória e armazenar esta trajetória para ser utilizada na segunda fase. Nesta fase não são consideradas as dimensões dos robôs bem como a cinemática envolvida no movimento em conjunto dos mesmos, ou seja, o robô é considerado como um ponto em movimento no ambiente. A segunda fase consiste em, a partir da trajetória gerada na primeira fase, realizar uma suavização da mesma considerando as restrições cinemáticas do conjunto. Nesta fase, são consideradas as dimensões dos robôs e da barra, e devido a este fato pode ser necessário que as trajetórias contenham manobras quando não é possível realizar a suavização de acordo com as restrições, sem que haja colisões com os obstáculos. Para validação da solução proposta foi desenvolvido um simulador em duas dimensões que serviu como base para a implementação da solução e realização dos experimentos. Para a primeira fase da solução foram implementados 3 algoritmos de planejamento de trajetória. Os dois primeiros são baseados no paradigma de decomposição em células, sendo que o primeiro utiliza a meta-heurística Colônia de Formigas e o segundo a Busca Tabu. O terceiro algoritmo é uma implementação do método do Campo Potencial Artificial. Para a segunda fase foi proposto um algoritmo que utiliza uma estratégia líderseguidor que com base na trajetória resultante da fase anterior gera duas trajetórias (uma para cada robô) através da utilização das equações de movimento do modelo cinemático simplificado conhecido na literatura como tractor-trailer. Os experimentos demonstraram a eficácia da solução proposta, além de um bom desempenho, o que torna viável a utilização da solução em sistemas embarcados.', 
  resumo_en: '--'},

 {numero: 608, ano: 2008, dia: '29/08', autor: 'Marco Antonio Firmino de Souza', orientador: [9], linha: 2, arquivo: '2008-Marco.pdf', 
  titulo: 'Uma Plataforma para a Cooperação Autônoma de Mútiplos Robôs', 
  resumo_pt: 'O futebol de robôs se mostrou uma excelente plataforma experimental para desenvolvimento de aplicações voltadas a computação e engenharias. Possibilita a aplicação e validação imediata de técnicas de várias áreas do conhecimento humano. Sobretudo, para implantar um ambiente robótico para este fim, diversos são os sistemas fundamentais ao funcionamento de uma equipe de futebol de robôs para atuar especificamente na Small Size League da RoboCup, desde a implementação de softwares complexos até construção física de robôs. Nesse sentido, o presente trabalho estuda os principais times desenvolvidos e suas técnicas empregadas. Além disso, foram implementados todos os requisitos necessários para o pleno funcionamento de uma equipe da F180 da RoboCup, incluindo a construção de circuitos eletrônicos, do corpo dos robôs, o desenvolvimento de um sistema para planejamento e tomada de decisão para controle dos jogadores (robôs), uma máquina de conhecimento que atua como técnico detectando como deve ser a atuação do time em campo, e um sistema de comunicação robusto, necessário para estabelecer corretamente a transmissão de dados de um computador central aos robôs em campo. E importante acrescentar, que ao longo do trabalho são apresentados todos os tópicos necessários à compreensão do problema em questão e as soluções encontradas.', 
  resumo_en: '--'},

 {numero: 607, ano: 2008, dia: '28/08', autor: 'Marcus Vinícius Rodrigues Lima', orientador: [204], linha: 1, arquivo: '2008-Marcus.pdf', 
  titulo: 'Uma Abordagem ao Roteamento de Veículos Utilizando Múltiplos Critérios', 
  resumo_pt: 'O futebol de robôs se mostrou uma excelente plataforma experimental para desenvolvimento de aplicações voltadas a computação e engenharias. Possibilita a aplicação e validação imediata de técnicas de várias áreas do conhecimento humano. Sobretudo, para implantar um ambiente robótico para este fim, diversos são os sistemas fundamentais ao funcionamento de uma equipe de futebol de robôs para atuar especificamente na Small Size League da RoboCup, desde a implementação de softwares complexos até construção física de robôs. Nesse sentido, o presente trabalho estuda os principais times desenvolvidos e suas técnicas empregadas. Além disso, foram implementados todos os requisitos necessários para o pleno funcionamento de uma equipe da F180 da RoboCup, incluindo a construção de circuitos eletrônicos, do corpo dos robôs, o desenvolvimento de um sistema para planejamento e tomada de decisão para controle dos jogadores (robôs), uma máquina de conhecimento que atua como técnico detectando como deve ser a atuação do time em campo, e um sistema de comunicação robusto, necessário para estabelecer corretamente a transmissão de dados de um computador central aos robôs em campo. E importante acrescentar, que ao longo do trabalho são apresentados todos os tópicos necessários à compreensão do problema em questão e as soluções encontradas.', 
  resumo_en: '--'},

 {numero: 606, ano: 2008, dia: '08/08', autor: 'Daniel Gomes', orientador: [3], linha: 1, arquivo: '2008-DanielGomes.pdf', 
  titulo: 'Análise de Resultados Obtidos pela Heurística Espectral para o Problema Largura de Banda em Caterpillars', 
  resumo_pt: 'Neste trabalho serão apresentados resultados experimentais obtidos com a heurística espectral para resolver o problema de largura de banda (bandwidth) restrito à classe particular de grafos conhecida na literatura como caterpillars. Caterpillars formam uma subfamília das árvores. O problema de largura de banda é NP-completo para grafos em geral. Porém, existem algoritmos polinomiais para resolver o problema de determinar a largura de banda em caterpillars proposto por Miller e outro por Assman para uma extensão da família de caterpillars. Juvan e Mohar definiram um procedimento aproximado para determinar a largura de banda de um grafo geral usando conceitos da teoria espectral, mais especificamente o vetor de Fiedler. Mostraremos resultados computacionais obtidos a partir das implementações dos algoritmos propostos por Miller e por Juvan e Mohar para casos particulares de caterpillar. A partir das conclusões obtidas dos resultados de comparação, será proposto a melhoria do algoritmo aproximado de Juvan e Mohar (Heurística 1) para caterpillars e para alguns supergrafos de caterpillars.', 
  resumo_en: '--'},

 {numero: 605, ano: 2008, dia: '08/08', autor: 'Marcos Gomes Pinto Ferreira', orientador: [205], linha: 2, arquivo: '2008-Marcos.pdf', 
  titulo: 'Uma Abordagem de Acesso em Redes Mesh Baseada em Conceito de Reputação', 
  resumo_pt: 'As redes mesh se originaram das redes sem fio ad-hoc com mecanismos mais robustos de segurança e garantia de qualidade de serviço. Os terminais podem funcionar como roteadores, encaminhando pacotes advindos de seus terminais vizinhos, estabelecendo uma comunicação entre outros nós, mesmo que estes nós não sejam diretamente alcançáveis. A comunicação, nestes casos, se dá através de múltiplos saltos, utilizando nós intermediários da rede. Por todas estas características, as redes mesh passaram a despertar um maior interesse, pois a tecnologia possui diversos benefícios em comparação com a topologia de redes com infra-estrutura. Entre os benefícios pode-se destacar: escalabilidade, menor custo de construção, compatibilidade com outras tecnologias, qualidade de serviço, robustez, reconstrução de rotas e crescimento orgânico. A proposta deste trabalho é avaliar o impacto da cooperação entre os nós em uma rede mesh de arquitetura híbrida. A cooperação entre os nós de uma rede mesh, na estrutura do backbone sem fio, é compulsória e construída quando na concepção da rede, porém, avaliamos além da estrutura do backbone, onde os nós podem não querer, de forma espontânea, disponibilizar seus recursos computacionais e de comunicação. Outra proposta é avaliar, de acordo com o nível de cooperação, o efeito da diferenciação de acesso ao meio com base na manipulação da janela de contenção e do mecanismo de backoff. Os resultados alcançados neste trabalho demonstram que a cooperação traz reais benefícios para a rede como um todo e ainda mostram que a manipulação do algoritmo de backoff é um mecanismo útil para garantir a diferenciação de prioridade no acesso ao meio e com isto, proporcionar uma melhoria na quantidade total de pacotes transmitidos pela rede.', 
  resumo_en: '--'},

 {numero: 604, ano: 2008, dia: '30/07', autor: 'Raphaela Baptista Fonseca', orientador: [8], linha: 3, arquivo: '2008-Raphaela.pdf', 
  titulo: 'Uma Estratégia de Apoio à Seleção de Algoritmos de Clusterização de Dados', 
  resumo_pt: 'A clusterização de dados, foco da presente dissertação, é uma tarefa de KDD que se caracteriza por ser um processo de otimização que pode apresentar uma diversidade de soluções possíveis. A busca por boas soluções nesse espaço caracteriza-se como um problema NP-completo. Diante disso, uma pergunta natural em uma aplicação envolvendo a tarefa de clusterização de dados refere-se à escolha entre inúmeros métodos de clusterização de dados disponíveis, de qual ou quais métodos seriam os mais recomendados para o problema que esteja sendo analisado. Uma alternativa para a escolha de métodos de mineração de dados seria a experimentação individual dos métodos disponíveis. Tal abordagem mostra-se, muitas vezes, inviável na prática, considerando o grande número de métodos a serem experimentados. Uma alternativa de cunho prático mais viável sugere a ordenação dos métodos de mineração de dados com base no desempenho destes métodos em experiências similares realizadas anteriormente. Assim sendo, a abordagem proposta pela presente dissertação utiliza conhecimento experimental sobre o desempenho dos métodos de clusterização de dados em situações anteriores de forma a propor ordenações entre estes métodos segundo seu potencial de utilização em novas situações.', 
  resumo_en: '--'},

 {numero: 603, ano: 2008, dia: '25/07', autor: 'Bruno de Souza Pinto Marques Correa', orientador: [205], linha: 2, arquivo: '2008-Bruno.pdf', 
  titulo: 'Adicionando Mobilidade ao Processamento de Imagens Médicas', 
  resumo_pt: 'A cada dia cresce a utilização de sistemas de informação dentro das instituições de saúde. Sistemas como PACS (Picture Archiving and Communication Systems) são cada dia mais utilizados. Atualmente, dispositivos móveis estão sendo incorporados aos PACS, aumentando assim a mobilidade dos profissionais de saúde. A maioria dos trabalhos na área de telessaúde que discutem a utilização de dispositivos móveis em ambientes médicos apresentam os dispositivos apenas visualizando imagens médicas, manipulando informações administrativas, laudos e históricos de pacientes. Neste trabalho, contrastando com outros trabalhos de telessaúde, propomos o processamento e a análise (ao menos preliminar) das imagens médicas utilizando dispositivos móveis, aumentando assim a mobilidade e tornando as aplicações mais eficientes no auxílio à diagnósticos e tratamentos médicos. Para adicionar mobilidade ao processamento de imagens médicas, construímos um sistema distribuído para superar as limitações de processamento e armazenamento dos dispositivos móveis. O sistema utiliza a tecnologia dos Web Services para a comunicação entre os dispositivos móveis e o servidor. Como uma prova de conceito, adicionamos mobilidade a três aplicações médicas: um simulador para implante dentário, um sistema de análise de imagens médicas que utiliza um algoritmo de CAD (Computer Aided Diagnosis) para detectar candidatos a hemorragias intra craniais e uma ferramenta para segmentação de imagens e reconstrução de modelos 3D.', 
  resumo_en: '--'},

 {numero: 602, ano: 2008, dia: '21/07', autor: 'Seimou Hamilton Oshiro', orientador: [13], linha: 3, arquivo: '2008-Seimou.pdf', 
  titulo: 'Um Ambiente Inteligente para Recuperação de Imagens', 
  resumo_pt: 'Com o crescimento no volume de informações, devido aos constantes avanços na área da Tecnologia de Informação, a Recuperação de Informação(RI) vem crescendo muito. RI visa desenvolver mecanismos computacionais que localizem e recuperem informações. Este presente trabalho vem abordar a Recuperação de Imagens, que se mostra de grande importância na atualidade, devido aos grandes acervos de imagens nas mais diferentes áreas e aplicações. Serão abordadas a recuperação de imagens baseada em sentenças(que faz a recuperação por meio de uma sentença em linguagem natural), recuperação de imagens baseada em conteúdo(que faz a recuperação por meio de características de imagens) e é proposta uma combinação dessas abordagens. Um protótipo foi desenvolvido para atender o IST-RIO, unidade de ensino superior da FAETEC, com intuito de preservar a história da instituição e ser acoplado ao sistema de gestão de conhecimento desenvolvido pelo instituto. Os resultados obtidos com este protótipo foram avaliados pela sua efetividade, ou seja, por resultados relevantes. Ao fim, foram apresentadas conclusões sobre as abordagens de recuperação de imagens avaliadas e propostas sugestões de trabalhos futuros.', 
  resumo_en: '--'},

 {numero: 601, ano: 2008, dia: '10/07', autor: 'José Guilherme Monteiro de Menezes', orientador: [8], linha: 3, arquivo: '2008-Jose.pdf', 
  titulo: 'Gerência Distribuída de Dados em Workflows de Bioinformática', 
  resumo_pt: 'Workflows científicos têm sido utilizados para a realização de experimentos in silico. Esses experimentos são caracterizados por serem executados com o apoio de computadores. Em suas execuções, uma sequência de programas computacionais é processada e os dados de saída de um programa são compostos como dados de entrada no programa seguinte. No sentido de prover workflows científicos mais interoperáveis e flexíveis, a tecnologia de serviços Web vem sendo adotada pela comunidade científica como um facilitador para a disponibilização e acesso a programas científicos. Isso possibilitou que programas científicos utilizados em ambientes distintos pudessem ser integrados para comporem workflows científicos. Porém, a natureza distribuída dos serviços Web resgatou algumas questões referentes à gerência dos dados gerados e processados durante a execução dos experimentos - dados intermediários. Onde e como disponibilizar esses dados de modo que fiquem acessíveis para análises e reutilizações em futuras execuções dos experimentos foram algumas questões levantadas com o uso de serviços Web na composição de workflows científicos. Muitas propostas para o gerenciamento de workflows científicos centralizam o armazenamento de todos os dados intermediários num único servidor. Essa abordagem pode impactar negativamente a execução do workflow, devido ao custo de repetidas transferências de dados para o sítio de armazenamento central. Este trabalho apresenta uma arquitetura denominada D-BioFlow para a gerência distribuída de dados em workflows de Bioinformática. A partir desta arquitetura foi implementado o protótipo e-Bioflow e foram realizados alguns testes para validação da abordagem proposta. A concepção desta arquitetura baseou-se fortemente no domínio da Bioinformática, porém acredita-se que possa ser aplicada também a outros domínios científicos. Um exemplo de uso do protótipo implementado foi realizado tomando-se por base um workflow real de Bioinformática chamado STINGRAY, que vem sendo utilizado pelo grupo de pesquisa BioWebDB da Fundação Oswaldo Cruz (FIOCRUZ) há alguns anos.', 
  resumo_en: '--'},

 {numero: 600, ano: 2008, dia: '30/06', autor: 'Daniel Pordeus Menezes', orientador: [202], linha: 2, arquivo: '2008-DanielMenezes.pdf', 
  titulo: 'Adicionando Segurança ao Algoritmo de Compressão Block-Sorting', 
  resumo_pt: 'Criptografia e compressão de dados são funcionalidades essenciais quando dados digitais são armazenados ou transmitidos através de canais inseguros. Geralmente, duas operações sequenciais são aplicadas: primeiro, compressão de dados para economizar espaço de armazenamento e reduzir custos de transmissão, segundo, criptografia de dados para prover confidencialidade. Essa solução funciona bem para a maioria das aplicações, mas é necessário executar duas operações de grande esforço computacional. Neste trabalho são propostas modificações no algoritmo de compressão Block-Sorting para que este realize compressão e criptografia de dados, simultaneamente. A contribuição deste trabalho é a criação de um novo algoritmo cripto-compressor, o BZip2s, que é uma versão do BZip2, encontrado em distribuições Linux. O BZip2s apresenta vantagens em relação a outros algoritmos cripto-compressores por obter um resultado melhor que a criptografia de Huffman original, além de possuir diversas possibilidades de configuração, permitindo variar entre uma melhor compressão, uma maior segurança ou um equilíbrio entre ambos. Diversos testes foram realizados, assim como resultados comparativos entre os métodos compressores originais e o BZip2s.', 
  resumo_en: '--'},

 {numero: 599, ano: 2008, dia: '23/06', autor: 'Herminio Camargo de Souza Junior', orientador: [8], linha: 3, arquivo: '2008-Herminio.pdf', 
  titulo: 'Ontologias Emergentes: uma Abordagem para Construção de Ontologias a Partir de Mapeamentos Ponto-a-Ponto', 
  resumo_pt: 'O aumento exponencial de informações vem exigindo das organizações de grande porte soluções inovadoras para a integração destas informações. Muitas destas abordagens envolvem a utilização de ontologias, com o objetivo de permitir uma visão global da organização. Porém a construção de uma ontologia é uma tarefa complexa e custosa. As organizações de grande porte geralmente configuram-se como ambientes autônomos, onde há um alto grau de heterogeneidade, e as informações são representadas das mais diversas formas. Arquiteturas ponto-a-ponto (P2P) oferecem uma solução interessante para estes ambientes na medida em que mantêm a autonomia, mas facilitam a integração entre os diversos pontos da organização. As abordagens P2P apontam para o uso de mapeamentos entre ontologias, entretanto, nenhuma das abordagens propostas oferece a possibilidade de geração de uma ontologia que represente a organização como um todo, ou seja, uma visão global da mesma. Inspirado nos chamados sistemas emergentes, este trabalho propõe uma abordagem denominada Ontologias Emergentes (OE) para a geração de uma ontologia da organização. Nesta abordagem, considera-se que a informação destas organizações estejam distribuídas em vários pontos, e que em cada ponto esta informação é representada através de ontologias. Além disso, à medida que estes pontos precisam trocar informações, mapeamentos P2P vão surgindo entre as ontologias. Neste contexto, foi definido um conjunto de heurísticas que são aplicadas ao conjunto de mapeamentos P2P para a geração da OE, representando a visão global da organização. Assim, dentre as principais contribuições deste trabalho destacam-se: a proposta de extensão de uma arquitetura P2P, acrescentando-se a funcionalidade necessária para a geração da Ontologia Emergente; a definição e formalização das heurísticas, viabilizando e facilitando, conseqüentemente, a implementação de um protótipo; e o protótipo OntoEmerge propriamente dito. Para avaliar a usabilidade do protótipo foram apresentados um exemplo de uso e realizados alguns testes. No exemplo de uso foram utilizadas 4 ontologias de aplicação, baseadas em sistemas de gerência de documentos. Para os testes foram utilizadas 31 ontologias da aréa bibliografica, obtendo resultados favoráveis à adoção da abordagem de Ontologias Emergentes.', 
  resumo_en: '--'},

 {numero: 598, ano: 2008, dia: '20/06', autor: 'Nuno Caminada Franklin de Oliveira e Silva', orientador: [1], linha: 1, arquivo: '2008-Nuno.pdf', 
  titulo: 'Identificação Automática de Expressões Cristalizadas Preposicionais em Corpora da Língua Portuguesa', 
  resumo_pt: 'Este é um trabalho de lexicografia computacional. Seu objetivo geral é o desenvolvimento de um método de identificação automática de expressões fixas, que, para os objetivos aqui propostos, confunde-se com o conceito de colocações da língua, ou seja, expressões multivocabulares que ocorrem com uma freqüência maior do que a estimada numa distribuição estatística hipoteticamente calculada. Como resultados de pesquisa, este trabalho propõe três contribuições distintas: a criação de um corpus de 30 milhões de palavras contendo textos contemporâneos oriundos da internet, a elaboração de um método de identificação de expressões candidatas à classificação como multivocábulos formados por sintagmas preposicionados, e a compilação de listas desses sintagmas preposicionados formalizados com vistas a sua incorporação a dicionário eletrônico. O método de identificação utiliza quatro fórmulas estatísticas classicamente utilizadas nesta tarefa, o Teste T, o Log Likelihood e o Mutual Information, buscando assim explorar os pontos fortes de cada um, melhorando o processo de identificação.', 
  resumo_en: '--'},

 {numero: 597, ano: 2008, dia: '12/05', autor: 'Fernanda Aparecida Lachtim', orientador: [206], linha: 3, arquivo: '2008-Fernanda.pdf', 
  titulo: 'Organização e Instanciação Automática de Conteúdos em Portais Semânticos', 
  resumo_pt: 'As tecnologias da Web Semântica(WS) podem transformar simples portais da Web em aprimorados portais semânticos, facilitando consideravelmente a organização, recuperação e compartilhamento de informações, dentre outras atividades. Portais Semânticos são caracterizados por armazenarem e estruturarem conteúdos informacionais segundo ontologias específicas de domínio. Esses conteúdos são representados através de linguagens ontológicas (RDF, OWL, entre outras), dotadas da capacidade de inferir novos conhecimentos no portal. No entanto, a alimentação destes portais é na maior parte das vezes feita de maneira manual, o que pode tornar-se um problema considerando-se que um portal deve estar sempre atualizado. Neste trabalho é proposta uma arquitetura para recuperar informações da Web com base em ontologias de domínio, e a partir daí integrá-las, estruturá-las, organizá-las e instanciá-las num portal semântico, permitindo também inferir e agregar novas informações a este ambiente. Para atingir este objetivo, o trabalho investiga ferramentas de interoperabilidade entre ontologias e propõe a extensão do mecanismo CMS (CROSI Mapping System), utilizado para a realização do cálculo de similaridade entre entidades de ontologias e para possibilitar a instanciação dinâmica da ontologia base de um portal. Essa extensão foi implementada em um protótipo que é usado para alimentar o Portal Semântico Educacional (POSEDU). Por fim, este trabalho ainda contribui com o desenvolvimento de uma ontologia no domínio educacional, usada como ontologia base para o portal POSEDU.', 
  resumo_en: '--'},

 {numero: 596, ano: 2008, dia: '08/05', autor: 'Rafael Lima de Carvalho', orientador: [9], linha: 2, arquivo: '2008-RafaelCarvalho.pdf', 
  titulo: 'Sistema de Identificação para a Casa Inteligente Utilizando Som', 
  resumo_pt: 'O presente trabalho lida com o problema de identificação de indivíduos a partir do som dos passos. A abordagem utilizada foi estudar características aplicadas a outros sistemas de identificação e realizar uma avaliação de suas propriedades discriminatórias, quando aplicadas ao sinal acústico do caminhar. As características foram divididas em dois grupos: objetivas e subjetivas. As objetivas se baseiam nos aspectos físicos da onda. As subjetivas são parâmetros processados levando-se em consideração o modelo acústico do sistema auditivo humano, ou seja, como o som é sentido pelo ouvido. Os parâmetros considerados geraram um volume considerável de dados, fazendo-se necessária a aplicação de métodos para eleição de atributos que melhor representem o padrão do caminhar. A fase de identificação é composta pela avaliação de dois algoritmos: uma versão modificada do K-Means e k vizinhos mais próximos - KNN, utilizando validação cruzada com k conjuntos e leave-one-out, como metodologias de partição de dados, respectivamente. O critério de Fisher foi utilizado como método de seleção de atributos. Em ambos, foram testadas diversas configurações utilizando o critério de seleção, com variadas proporções de dados. Para avaliar o sistema, foi realizado um experimento real através de gravações do caminhar de um conjunto de indivíduos, os quais utilizaram seis tipos de calçados diferentes no total. Uma análise foi feita através da separação do conjunto total em seis subgrupos característicos: COTURNO, TENIS, SAPATO, UMPORCALCADO, MESMOCALCADO e IGNORECALCADO. A metodologia de testes consistiu em aplicar cada característica, singularmente, nos algoritmos de classificação. Em seguida, as características são selecionadas e combinadas de acordo com a quantidade de acertos obtidos na etapa anterior. Essa metodologia resultou em taxas de identificação que vão de 33,8% a 97,5%.', 
  resumo_en: '--'},

 {numero: 595, ano: 2008, dia: '03/04', autor: 'André Oliveira Castelucio', orientador: [12], linha: 2, arquivo: '2008-Andre.pdf', 
  titulo: 'Uma Rede Sobreposta no Nível de Sistemas Autônomos para Rastreamento de Tráfego em Redes IP', 
  resumo_pt: 'Ataques distribuídos de negação de serviço (DDoS) atualmente representam uma grande ameaça à operação adequada de serviços na Internet. Nesta direção é proposto um sistema que cria uma rede sobreposta para o rastreamento de tráfego em redes IP a ser implementada no nível de Sistemas Autônomos (SAs) para lidar com essa ameaça. Este sistema de rastreamento de tráfego IP no nível de SAs contrasta com os trabalhos anteriores, pois ele não requer conhecimento prévio da topologia da rede enquanto permite o rastreamento de um único pacote bem como uma instalação parcial e incremental. A implementação do sistema de rastreamento é validada através de simulações, mostrando que a possibilidade de instalação parcial ofertada pelo nosso sistema provê resultados relevantes de rastreamento IP, tornando-o viável para redes de larga escala como a Internet.', 
  resumo_en: '--'},

 {numero: 594, ano: 2008, dia: '12/02', autor: 'Marlos de Mendonça Corrêa', orientador: [9], linha: 2, arquivo: '2008-Marlos.pdf', 
  titulo: 'Geração de Movimentos em Trajetórias Ajustáveis para Veículos Autônomos Aéreos não Tripulados', 
  resumo_pt: 'Entre os diversos aspectos envolvidos na construção e operação de veículos autônomos não tripulados, um que ocupa papel fundamental é o planejamento de trajetórias. O presente trabalho estuda detidamente esse problema quando a trajetória está sujeita às restrições cinemáticas e dinâmicas inerentes ao veículo. Além disso, o trabalho trata especificamente de dirigíveis, os quais são sujeitos à uma restrição em relação a carga paga que podem transportar. Tal fato limita o hardware embarcado e estabelece como requisitos essenciais do algoritmo de planejamento de trajetória o baixo consumo de recursos computacionais. Um método, então, foi escolhido e adaptado para geração de trajetórias adequadas às restrições mencionadas. O método de campo potencial virtual, escolhido por sua simplicidade e adequação ao planejamento on-line, foi adaptado e testado, mostrando desempenho e consumo adequados aos requisitos estabelecidos. Para simplificar a geração de trajetórias, o problema de planejamento foi reduzido ao planejamento de trajetória para uma partícula. Essa redução do corpo sólido do robô a uma partícula foi realizada por um processo chamado construção do espaço de con- figurações do robô, o qual envolve cálculos geométricos cuja complexidade está ligada diretamente ao tipo de operação e à dimensão do ambiente real mapeado. Assim, o processo de construção do espaço de configurações foi planejado e implementado mantendo-se em vista o baixo consumo de recursos e o pequeno tempo de resposta. Em particular nesse caso, os testes realizados mostraram excelente desempenho para o processo, confirmando todo o conjunto - construção do espaço de configurações e planejamento de trajetórias - como um sistema adequado ao uso embarcado. A pesquisa identificou, também, a inexistência de métodos padronizados e universais para avaliação da qualidade das trajetórias geradas. Uma forma de mensurar esta qualidade se faz necessária, para permitir uma discussão mais objetiva com relação à eficiência dos métodos de planejamento. Em virtude disso, propôs-se um conjunto de critérios para permitir a avaliação em questão. Tais critérios, conforme concluiu-se no estudo realizado, são dependentes da aplicação, do veículo utilizado e do tipo de ambiente de emprego. Contudo, foram estabelecidos de forma a serem adequados ao problema em questão. Pela natureza do método utilizado, a influência do par métrica-função potencial no processo também foi estudada. Constatou-se que individualmente a influência se faz sentir como um melhor ou pior ajuste da trajetória, contudo o par pode impor alterações a ponto de tornar incapaz o planejamento de um caminho, mesmo que ele exista. Contudo, esse estudo forneceu indícios de que o uso de métricas mais elaboradas pode, talvez, permitir a codificação das restrições na estrutura do espaço pesquisado, o que facilita o processo de planejamento. De forma a permitir os estudos supracitados, foi implementado um simulador utilizandose orientação a objetos e linguagem C++. Esse simulador, ao permitir o ajuste de diversos parâmetros, a escolha de diversas combinações métrica-função potencial e a elaboração de ambientes bidimensionais, constitui-se em uma ferramenta útil e versátil na exploração do método proposto.', 
  resumo_en: '--'},

 {numero: 593, ano: 2008, dia: '31/01', autor: 'Periceles José Vieira Vianna', orientador: [12], linha: 2, arquivo: '2008-Periceles.pdf', 
  titulo: 'Aplicação da Tecnologia de Roteamento de Estado de Enlace em Projetos de Sistemas Autônomos Auto-Balanceados para Engenharia de Tráfego', 
  resumo_pt: 'Este trabalho propõe uma metodologia para projetos de Sistemas Autônomos AutoBalanceados utilizando protocolos de roteamento de estado de enlace. Sistemas Autônomos Auto-Balanceados são aqueles que buscam de forma contínua melhorar o balanceamento da carga na rede. O adequado balanceamento da carga evita que enlaces fiquem pouco utilizados em detrimento de outros, super utilizados ou mesmo congestionados. Atualmente, a monitoração do nível de carga nos enlaces e otimização da distribuição da carga é uma das tarefas de um administrador de rede. A metodologia ora proposta torna o sistema autônomo capaz de operar esta tarefa sem interferência do administrador da rede, automatizando o processo. Para atingir este objetivo, esta arquitetura que funciona sobre protocolos de roteamento de estado de enlace, captura a topologia da rede através da base de dados do protocolo, verifica as condições de ocupação dos enlaces e, finalmente, configura os pesos dos enlaces de forma a melhor distribuir a carga na rede.', 
  resumo_en: '--'},

 {numero: 592, ano: 2008, dia: '18/01', autor: 'Emanuel José Pacheco Freire', orientador: [12], linha: 2, arquivo: '2008-Emanuel.pdf', 
  titulo: 'Detecção de Anomalias em Tráfego HTTP', 
  resumo_pt: 'O presente trabalho apresenta uma metodologia para a detecção de fluxos anômalos em tráfego HTTP. Considera-se como anomalias o trânsito de algum outro protocolo pelas portas reservadas para HTTP. A metodologia foi avaliada com dados reais coletados a partir de um provedor de acesso Internet para o caso de fluxos de voz sobre IP (VoIP) presentes em tráfego HTTP. Os resultados experimentais mostraram uma boa eficiência do algoritmo para detecção VoIP, obtendo uma detecção de 90% dos fluxos possíveis de serem identificados com aproximadamente 2% de falsos positivos ou uma detecção de 100% de fluxos VoIP com uma taxa de falsos positivos de até 5%. Também foi avaliada nossa proposta em uma simulação de detecção em tempo real.', 
  resumo_en: '--'},

 {numero: 591, ano: 2007, dia: '21/12', autor: 'João Alberto Neves dos Santos Filho', orientador: [11], linha: 3, arquivo: '2007-Joao.pdf', 
  titulo: 'AspectualJade: uma Plataforma para o Desenvolvimento de Sistemas Multi-Agentes com Modularização de Características Transversais', 
  resumo_pt: 'Com o aumento da complexidade de sistemas, cada vez torna-se mais necessária a presença de entidades que possuam a capacidade de autonomia, ou seja, a capacidade de tomar decisões sem a interação humana ou de outros sistemas previamente desenvolvidos para este fim. Para atender esta demanda, Sistemas Multi-agentes disponibilizam agentes, que são entidades computacionais com comportamentos autônonomos. A idéia principal de agentes em um sistema multi-agente é que um comportamento global inteligente possa ser alcançado a partir dos comportamentos individuais dos mesmos. Funcionalidades de sistemas estão sujeitos à composição explícita no decorrer de seus respectivos desenvolvimentos, revelando código espalhado e entrelaçado, principalmente entre requisitos funcionais e não-funcionais. O paradigma de orientação a aspectos procura evitar este tipo de problema, oferecendo uma entidade chamada aspecto, de maneira que a composição entre módulos seja realizada fora do contexto da aplicação. Tais módulos são conhecidos como caracerísticas transversais. Este trabalho tem como objetivo apresentar a plataforma AspectualJADE para promover a modularização de características transversais em sistemas multi-agentes, através do paradigma de orientação a aspectos. A plataforma AspectualJADE propõe uma integração entre o framework para desenvolvimento de agentes, o JADE, e a linguagem para desenvolvimento de aspectos, o AspectJ. Além disso, provê um modelo de composição baseado nas abstrações, como agentes, aspectos, planos e ações; e nos relacionamentos horizontais e verticais propostos pelo framework ACROSS. Uma prova de conceito foi realizada para mostrar o modelo de composição apresentado pela plataforma AspectualJADE.', 
  resumo_en: '--'},

 {numero: 590, ano: 2007, dia: '05/12', autor: 'Pier-Giovanni Taranti', orientador: [11], linha: 3, arquivo: '2007-Pier.pdf', 
  titulo: 'Dominium: uma Abordagem para Regular SMA em Ambientes Dinâmicos e Geo-Referenciados', 
  resumo_pt: 'Em um sistema multiagente (SMA) não é possível prever, em tempo de projeto, o conjunto de todas as interações entre agentes ou entre estes e o ambiente. Isto fica ainda mais difícil, pois o ambiente pode possuir características de dinâmica próprias, ou seja, pode alterar seu estado no decorrer do tempo, sendo necessário aos agentes perceber estas mudanças. Além disso, o ambiente pode representar o mundo físico, contendo, portanto, representação geográfica virtual de objetos ou regiões que afetem o processo decisório do agente. No entanto, para que seja possível construir SMA fidedignos é necessário limitar a incerteza causada pela autonomia dos agentes à níveis aceitáveis e conhecidos. SMA regulados são sistemas nos quais existem normas que limitam as ações dos agentes, orientando suas decisões. Existem diversas abordagens para regular SMA, mas estas não consideram, especificamente, a existência de SMA onde o ambiente é dinâmico e geo-referenciado. Este trabalho apresenta uma arquitetura para regular SMA em ambientes dinâmicos e geo-referenciados, chamada Dominium. A arquitetura é composta por: um modelo conceitual capaz de representar o domínio do sistema; um componente de regulação que tem por fim informar os agentes da aplicação a que normas eles estão sujeitos, verificar se os mesmos as estão cumprindo e executar medidas previstas para transgressão de normas (enforcement); bibliotecas de verificações e conseqüências utilizadas nas tarefas de verificação de normas e enforcement; e um banco de dados com suporte geográfico (GisBD).', 
  resumo_en: '--'},

 {numero: 589, ano: 2007, dia: '10/08', autor: 'Regina Helena Farias de Almeida', orientador: [207], linha: 3, arquivo: '2007-Regina.pdf', 
  titulo: 'Processo de Gerência de Assets no Desenvolvimento de Software Baseado em Reuso: uma Abordagem no Contexto do Ministério da Defesa e de seus Comandos Subordinados', 
  resumo_pt: 'No mundo globalizado de hoje, a necessidade de se prover sistemas para o gerenciamento do grande volume de informações gerado a cada dia tornam imperativa a busca contínua por novas práticas e formas de desenvolvimento de software para a manipulação dessas informações. Isso está vinculado também à necessidade de se conseguir ganhos de qualidade, produtividade e redução de custos em tais desenvolvimentos, pois esses são fatores fundamentais no sucesso do negócio de empresas ligadas à tecnologia da informação. Nesse cenário, o reuso dos artefatos de software vem a exercer um papel importante na concretização desses objetivos. Mas como garantir que esses artefatos serão reutilizados? O grande gargalo da reutilização de software é a falta de um processo que garanta a pratica do reuso. Por isso, a utilização de um processo de gerência dos artefatos reusáveis é um dos principais fatores que asseguram a qualidade, armazenamento, busca e recuperação dos artefatos e o sucesso do reuso dentro de uma organização. Este trabalho está baseado nas definições da norma IEEE Std 1517 (1999) para o processo de gerência de assets. O processo estabelecido abrange a utilização do meta-modelo SPEM na elaboração do processo, o esquema de classificação em facetas segundo Prieto-Diaz (1993), a estrutura de armazenamento segundo a RAS (2005), documentação segundo Sametinger (1996) e o cálculo de métricas segundo Ezran (2002). Além disso, foi especificado um controle efetivo de usuários ao repositório e um controle das diferentes versões dos assets armazenados Por fim, é apresentado um estudo de caso aplicando o processo de gerência aos assets gerados no projeto piloto Controlador Tático, desenvolvido com base no Processo de Desenvolvimento de Software Baseado em Reuso no Contexto do Ministério da Defesa e de seus Comandos Subordinados de GURGEL (2004) e nos artefatos gerados no desenvolvimento do Simulador do Treinador de Ataque e no Sistema de Apoio à Decisão Logística. Este estudo de caso mostra que a utilização de um processo de gerência de assets proporciona o efetivo sucesso da reutilização dentro da sistemática de desenvolvimento de sistemas baseado em reuso.', 
  resumo_en: '--'},

 {numero: 588, ano: 2007, dia: '16/07', autor: 'Ricardo Maroquio Bernardo', orientador: [9], linha: 2, arquivo: '2007-Ricardo.pdf', 
  titulo: 'Simublimp – Uma Contribuição ao Desenvolvimento de Algoritmos Inteligentes para uma Equipe de Dirigíveis Robóticos Autônomos', 
  resumo_pt: 'Esta dissertação trata do desenvolvimento de um simulador para uma frota de dirigíveis robóticos autônomos não-tripulados, e é parte do projeto VANT (Veículo Aéreo NãoTripulado) do Instituto Militar de Engenharia. O simulador desenvolvido é capaz de exibir cenários tridimensionais com ambientes e dirigíveis virtuais, sendo que o comportamento dos dirigíveis obedece a um modelo matemático que determina as reações do dirigível às forças dinâmicas, aerodinâmicas e deliberadas que atuam sobre o veículo. Um detalhamento de tal modelo é apresentado, bem como os passos para sua implementação. Outra característica do simulador é a interface de comunicação via soquetes, que possibilita a interação com sistemas externos, os quais podem consultar informações dos sensores embarcados simulados (como câmera, GPS, inclinômetro etc.) e comandar os atuadores (como os propulsores). Entre as aplicações potenciais para VANT estão: a vigilância, a inspeção e o reconhecimento de ambientes. Uma das vantagens oferecidas é o fato de o veículo atuar em altitudes elevadas, proporcionando cobertura privilegiada a alguns sensores, como a câmera. Dentre os tipos de VANT, o dirigível destaca-se como uma opção de baixo custo aquisitivo e operacional, possuindo, ainda, vantagens como: dispensar pista de pouso, capacidade de pairar no ar, navegar em baixa velocidade, entre outras. Ainda, por ser não-tripulado, é possível utilizar dirigíveis de menor porte, facilitando a logística operacional e reduzindo as conseqüências de possíveis incidentes. Sobremaneira, o uso desses veículos torna-se mais interessante quando possuem algum grau de autonomia, sendo capazes de realizar tarefas com o mínimo de intervenção humana. Entretanto, a conquista da autonomia requer o desenvolvimento de algoritmos inteligentes capazes de executar as tarefas inerentes ao problema, como navegação, pouso, decolagem, desvio de obstáculos, reconhecimento de objetos etc. Complementarmente ao simulador, são apresentados dois sistemas que o estendem com algoritmos inteligentes para determinação de caminhos entre dois pontos. Um deles baseia-se no algoritmo clássico A* e o outro, no algoritmo Q-Learning, sendo que o sistema baseado no Q-Learning é capaz de determinar trajetórias não só com distâncias mínimas, mas também com o mínimo de mudanças de direção. Os sistemas interagem com o simulador via soquetes, usando um protocolo pré-definido. A renderização das imagens 3D é feita utilizando a biblioteca OpenGL. São apresentados testes de desempenho que avaliam a utilização via soquetes e a renderização das imagens tridimensionais, e os resultados obtidos confirmam a viabilidade de se utilizar o simulador para o fim proposto.', 
  resumo_en: '--'},

 {numero: 587, ano: 2007, dia: '02/07', autor: 'Rafael Dias Ribeiro', orientador: [12], linha: 2, arquivo: '2007-Rafael.pdf', 
  titulo: 'Mecanismos para Controle de Tráfego UDP Através de Política de Preços Baseada na Utilidade', 
  resumo_pt: 'Este trabalho estuda os problemas decorrentes da coexistência dos fluxos TCP e UDP em redes de dados, tendo em vista o observado aumento do volume de tráfego transportado via UDP nos últimos anos. Esta proposta baseia-se em regular os fluxos UDP, que não são cooperativos, em situação de congestionamento, através de penalização via tarifação de acordo com o impacto causado no tráfego TCP. Para isso desenvolveu-se uma política dinâmica de preços utilizando conceitos microeconômicos baseados nas funçóes utilidades dos fluxos TCP e UDP. Os fluxos UDP são tarifados de acordo com o prejuízo causado aos fluxos TCP em enlaces congestionados. Este trabalho propóe algoritmos de controle de admissão e roteamento de forma a regularem o tráfego UDP minimizando o estrangulamento do TCP e garantindo também a QoS necessária aos fluxos UDP. Resultados da simulação mostram as vantagens da proposta quando comparada com o cenário atual da Internet.', 
  resumo_en: '--'},

 {numero: 586, ano: 2007, dia: '29/06', autor: 'Kele Teixeira Belloze', orientador: [8], linha: 3, arquivo: '2007-Kele.pdf', 
  titulo: 'Uma Extensão do Processo de Anotação Genômica para Ampliar o Uso e a Evolução Colaborativa de Ontologias no Domínio da Biologia Molecular', 
  resumo_pt: 'A anotação genômica é uma das tarefas mais importantes na área de pesquisa genômica. Uma anotação é o registro do significado biológico dos genes identificados nas seqüências genômicas. Contudo, em geral a anotação é feita através de um vocabulário do próprio anotador ou do grupo de pesquisa. Isto pode dificultar a troca de informações entre pesquisadores do mesmo grupo, entre projetos parceiros e pesquisadores interessados na pesquisa em questão, e consequentemente prejudicar a evolução da pesquisa. Para resolver este problema, alguns grupos de pesquisa genômica têm feito o uso da anotação baseada em ontologia. Embora o uso de ontologias seja crescente na comunidade de Bioinformática, estas ainda não acompanham a contento as descobertas genômicas. Tanto as falhas quanto os sucessos no uso destas ontologias são identificados no contexto dos projetos de pesquisa genômica. Entretanto, falhas e sucessos raramente são reportados aos desenvolvedores/curadores das ontologias. Esta dissertação propõe uma extensão do processo de anotação genômica para capturar o raciocínio feito sobre a anotação baseada em ontologia. Por exemplo, as ações envolvidas na confirmação dos termos das ontologias, revelando as razões pela qual um termo foi identificado como adequado ou não, ou os problemas existentes, ou ainda possíveis sugestões para quando um termo não foi encontrado. O registro deste raciocínio contribui para a ampliação do uso e evolução da ontologia, e consequentemente, ampliando-se o uso de ontologias, facilita-se as colaborações inter e intra projetos. Um protótipo denominado GARSA Notes, o qual possui as funcionalidades necessárias para apoiar a proposta de extensão foi desenvolvido e anexado ao GARSA, um sistema de anotação em uso por um grupo de pesquisa da FIOCRUZ.', 
  resumo_en: '--'},

 {numero: 585, ano: 2007, dia: '26/06', autor: 'Fernando Antônio Diniz Corrêa', orientador: [11], linha: 3, arquivo: '2007-Fernando.pdf', 
  titulo: 'Avaliação de Interoperabilidade na Adoção de Códigos Abertos', 
  resumo_pt: 'Um processo de desenvolvimento de software utiliza diversas ferramentas. O problema é que, nem sempre, há uma integração entre elas, dificultando a criação dos artefatos e a seqüência das atividades que fazem parte do processo, impedindo o uso da informação, fundamental para o sucesso da sua aplicação, servindo de base para a construção do software. Interoperabilidade é um fator importante a ser considerado para uma efetiva seleção de ferramentas que apóiam um processo de desenvolvimento de software. Ferramentas de código aberto, pela própria natureza, buscam seguir padrões, incentivando a interoperabilidade. No entanto, a quantidade de ferramentas de código aberto cada vez cresce mais, oferecendo um leque muito grande de opções e dificultando a escolha de um conjunto coerente de ferramentas interoperáveis para o suporte ao processo. Este trabalho apresenta a proposta de um modelo baseado em critérios de interoperabilidade e qualidade, para a seleção de ferramentas que ofereçam suporte ao processo. Sua estrutura é formada por checklists que permitem a coleta das funcionalidades desejadas pelo usuário, dando sustentação às atividades do processo de desenvolvimento. O modelo possibilita o uso de padrões abertos, apoiado pela arquitetura e-PING, além de utilizar funcionalidades de processo de software do CMMI e MPS.br. Além disso, o uso de matrizes de interoperabilidade, desenvolvidas para o mecanismo de avaliação do modelo, orientam na identificação das ferramentas, de acordo com o seu uso, pelas diversas fases do processo. Com este trabalho, busca-se fazer uso dos atributos de interoperabilidade para auxiliar a gerência de projeto na escolha de ferramentas que consigam interoperar, atendendo melhor os requisitos esperados, evitando a impedância da informação. O modelo proposto é aplicado em dois estudos de caso, utilizados na validação dos conceitos, que permitiram oferecer maior maturidade a sua estrutura e o entendimento do usuário quanto à utilização, mostrando os resultados obtidos.', 
  resumo_en: '--'},

 {numero: 584, ano: 2007, dia: '06/06', autor: 'Miriam Oliveira dos Santos', orientador: [8], linha: 3, arquivo: '2007-Miriam.pdf', 
  titulo: 'Armazenamento e Recuperação de Documentos XML Heterogêneos: Aplicando Técnicas de KDD para Apoiar o Projeto Físico em SGBDs XML Nativos', 
  resumo_pt: 'Os avanços tecnológicos têm contribuído para um expressivo aumento do volume e da diversidade de informações que circulam atualmente pela Web. Muitas dessas informações encontram-se organizadas em documentos XML e provêm de várias fontes. O gerenciamento de conteúdo envolvendo documentos XML heterogêneos carece de mecanismos que orientem o processo de armazenamento desses documentos, de forma a facilitar sua posterior recuperação. Assim sendo, este trabalho apresenta uma estratégia baseada em princípios de Descoberta de Conhecimento e Mineração de Dados para orientar o processo de armazenamento de documentos XML heterogêneos em SGBD’s XML Nativos. Avalia-se o potencial da estratégia proposta, através de um estudo de caso utilizando o SGBD XML Nativo Berkeley DB XML.', 
  resumo_en: '--'},

 {numero: 583, ano: 2007, dia: '28/05', autor: 'Liliana Paola Mamani Sánchez', orientador: [208], linha: 1, arquivo: '2007-Liliana.pdf', 
  titulo: 'Atribuição de Papéis Semânticos a Argumentos de Nominalizações: um Método Semi-Automático', 
  resumo_pt: 'Dada a crescente importância da informação de tipo semântico para os sistemas de processamento automático da linguagem, as pesquisas nas áreas de lingüística computacional e processamento de linguagem natural estão se desenvolvendo ao redor de temas como a etiquetagem de papéis semânticos e estruturas argumento-predicado, entre outros. Sistemas de pergunta-reposta, tradução automática, geração da linguagem, são alguns dos sistemas de processamento da linguagem que se veriam beneficiados com o crescimento de recursos e ferramentas que trabalhem com informação semântica. Um tipo de informação semântica é a dos papéis semânticos que desempenham os constituintes de uma predicação, cujo predicador pode não apenas ser um verbo, mas também outros elementos como nomes ou adjetivos. Este trabalho apresenta um método semi-automático básico de etiquetagem de papéis semânticos em sentenças que contém predicações cujo predicador é uma nominalização deverbal. A maior parte dos trabalhos que aborda esta tarefa utilizam técnicas de aprendizado de máquina, que requerem corpora etiquetados de grande porte. A inexistência de um corpus em língua portuguesa anotado com informação de papéis semânticos motivou a utilização de duas estratégias: aproveitamento da correspondência entre a estrutura argumental de um verbo e a estrutura argumental da nominalização desse verbo; e o uso da Web como corpus para inferir probabilisticamente a formação dos complementos aos quais pode ser atribuído um papel semântico e a posição preferencial dos complementos dentro da estrutura sintática do verbo base da nominalização. Pretende-se que a ferramenta em que foi implementado o método proposto seja utilizada como sistema baseline para a etiquetagem de papéis semânticos. O método foi avaliado tanto com relação à corretude quanto à qualidade das estruturas argumentais envolvidas. Os resultados são encorajadores considerando o uso da Web como corpus.', 
  resumo_en: '--'},

 {numero: 582, ano: 2007, dia: '05/05', autor: 'Fábio Silveira Vidal', orientador: [9], linha: 2, arquivo: '2007-FabioVidal.pdf', 
  titulo: 'Sistema de Navegação para Dirigíveis Aéreos Não-Tripulados Baseado em Imagens', 
  resumo_pt: 'O uso de VANT (Veículos Aéreos Não-Tripulados) é conveniente para auxiliar a execução de diversas atividades como vigilância, inspeção e reconhecimento de ambientes, devido à posição em que os mesmos atuam e pelo fato de poderem ser de menor porte por não possuírem tripulantes. Sobretudo, a utilização destes veículos torna-se mais interessante se os mesmos possuírem autonomia, ou seja, se forem capazes de realizar tarefas a eles delegadas com o mínimo de intervenção humana. Um dos requisitos para que um VANT seja autônomo é que o mesmo possua um sistema de navegação próprio, o qual possibilite determinar por onde o veículo deverá seguir a fim de alcançar uma determinada posição, previamente estabelecida. Dentre os tipos de VANT, podemos destacar o dirigível como uma opção de baixo custo aquisitivo e operacional, possuindo, ainda, vantagens como dispensar pista de pouso e capacidade de pairar no ar, entre outras. Portanto, é proposto, neste trabalho, o desenvolvimento de um sistema de navegação para dirigíveis aéreos não-tripulados em ambientes fechados baseado em imagens, o qual é feito, essencialmente, em três passos: aquisição das imagens a partir das câmeras embarcadas; mapeamento do ambiente com o uso de visão estereoscópica; e, planejamento de trajetória. Ao longo da dissertação, serão apresentados os conceitos fundamentais da problemática abordada e dos métodos empregados para o desenvolvimento da solução proposta.', 
  resumo_en: '--'},

 {numero: 581, ano: 2007, dia: '15/03', autor: 'Monael Pinheiro Ribeiro', orientador: [9], linha: 2, arquivo: '2007-Monael.pdf', 
  titulo: 'Camin – Cadeira Móvel Inteligente: um Sistema de Navegação Utilizando uma Adaptação do Método dos Campos Potenciais Artificiais', 
  resumo_pt: 'O estudo dessa dissertação versa justamente no ramo das tecnologias assistivas, empregando o conhecimento da robótica e ciência da computação na elaboração de um sistema de controle e navegação para uma cadeira de rodas móvel semi-autônoma. Atualmente existem cerca de 200 mil cidadãos, apenas no Brasil, com severas limitações físicas, estes necessitam de algum tipo de aparato assistivo para auxiliá-los no ato de caminhar. Ao vislumbrar o auxílio a essas pessoas, a tecnologia dispõe de um ramo recente chamado Tecnologias Assistivas, que aplicam os avanços tecnológicos no desenvolvimento de dispositivos facilitadores do cotidiano de pacientes com algum tipo de deficiência física. O presente estudo, em seu principal objetivo, sugere, simula e experimenta um sistema de navegação para uma cadeira de rodas móvel inteligente empregando a heurística dos campos potenciais artificiais para sua navegação. Além disso, uma ferramenta clássica da teoria dos grafos que determina caminhos mais curtos - chamada algoritmo de Dijkstra - é utilizada sobre um grafo construído a partir de checkpoints estrategicamente posicionados na planta arquitetônica da edificação que a cadeira de rodas navegará, sugerindo assim, uma adaptação da heurística clássica. Ademais, emprega-se a lógica nebulosa na concepção de um controlador na resolução do problema localizado e no desvio de obstáculos dinâmicos no ambiente, garantindo, desse modo, que a cadeira robótica alcance de forma precisa e segura seu objetivo final.', 
  resumo_en: '--'},

 {numero: 580, ano: 2007, dia: '02/03', autor: 'Flávio Barbieri Gonzaga', orientador: [12], linha: 2, arquivo: '2007-Flavio.pdf', 
  titulo: 'Serviços Proporcionais Diferenciados Baseados em Métrica Única Combinada', 
  resumo_pt: 'Este trabalho é dividido em duas etapas. Na primeira é proposto o uso de uma métrica única combinada (MUC) para a implementação dos Serviços Proporcionais Diferenciados (Proportional Differentiated Services - PDS). A MUC é baseada na relação entre vazão e atraso observadas nas classes de serviço, proporcionando uma implementação de PDS mais consistente do que aquelas baseadas apenas no atraso. Um novo algortimo de escalonamento é proposto para trabalhar diretamente com MUC. Resultados de simulação mostram as vantagens da proposta quando comparada com escalonadores comumente empregados no modelo PDS, tais como Waiting Time Priority (WTP) e Proportional Average Delay (PAD). Na segunda etapa é apresentada uma forma de se oferecer níveis de garantia da QoS à aplicações com características de Hard QoS juntamente com o modelo PDS. Simulações foram realizadas considerando uma classe com conexões VoIP atuando em conjunto com o tráfego comum nas demais classes. Na classe VoIP, mecanismos de controle de admissão foram implementados, visando manter os níveis de qualidade exigidos nesse tipo de tráfego.', 
  resumo_en: '--'},

 {numero: 579, ano: 2007, dia: '27/02', autor: 'William Augusto Rodrigues de Souza', orientador: [5], linha: 2, arquivo: '2007-William.pdf', 
  titulo: 'Identificação de Padrões em Criptogramas Usando Técnicas de Classificação de Textos', 
  resumo_pt: 'O desenvolvimento de produtos criptográficos requer meios para se avaliar a confiabilidade destes produtos. Desta forma, pode ser definido um conjunto de requisitos a serem atendidos para que um algoritmo criptográfico seja considerado confiável. Um desses requisitos poderia ser a inexistência de padrões nos criptogramas gerados por esses algoritmos. No trabalho de Carvalho (2006), foram detectados padrões em criptogramas gerados com os algoritmos DES e AES, com chaves de 64 e 128 bits, respectivamente, no modo de operação ECB. Estes padrões se caracterizaram pelo agrupamento dos criptogramas cifrados com a mesma chave. Neste contexto, a presente dissertação tem o objetivo de identificar padrões em criptogramas gerados pelos algoritmos DES, com chaves de 64 bits, AES, com chaves de 128, 192 e 256 bits, e RSA, com chaves de 64, 128, 256, 512 e 1024 bits, por meio de técnicas de Recuperação de Informações e de Inteligência Artificial. Além disso, o modo de operação CBC é avaliado.', 
  resumo_en: '--'},

 {numero: 578, ano: 2007, dia: '07/02', autor: 'Sérgio dos Santos Cardoso Silva', orientador: [12], linha: 2, arquivo: '2007-Sergio.pdf', 
  titulo: 'Escalonador Multi-Critério para Sistemas de Mensagens Militares em Redes sem Fio de Baixo Desempenho', 
  resumo_pt: 'O objetivo deste trabalho é avaliar o emprego de métodos multi-critério para escalonar mensagens militares em redes HF. Devido às características de uma rede HF, tais como, baixa largura de banda, alta latência, alta variabilidade do canal e alta taxa de erros, faz-se necessário à priorização das mensagens de acordo com o valor militar do seu conteúdo. Atualmente o operador do sistema classifica a mensagem utilizando um único critério, que é a precedência, dentro de quatro níveis pré-definidos: Urgentíssima, Urgente, Prioridade e Rotina. Para o método atual, utiliza-se um algoritmo de fila de prioridades, com cada precedência alocada a uma fila, e o escalonador sempre irá atender a fila de maior prioridade. Mensagens que possuem baixo valor militar irão aguardar no buffer até que não exista nenhuma mensagem de maior valor. Desta forma, dependendo da carga do sistema, as mensagens de menor valor podem ser atrasadas de tal forma que a validade de seu conteúdo pode expirar, acarretando em perdas elevadas que em alguns casos podem atingir mais de 50% das mensagens. A presente dissertação propõe o uso de outros critérios de forma a quantificar o valor da prioridade final de uma mensagem, tais como: prazo de validade, precedência e nível hierárquico do emissor. Estes critérios foram selecionados com o objetivo de estimar a prioridade final e permitir que mensagens de menor precedência possam ser escalonadas em determinados momentos, mesmo na presença de mensagens de maior precedência. Dois algoritmos de escalonamento são propostos: ordenação lexicográfica e método da função valor. Resultados de simulação demonstram as vantagens do emprego dos algoritmos propostos em comparação com os métodos tradicionais em diversos cenários.', 
  resumo_en: '--'},

 {numero: 577, ano: 2007, dia: '23/01', autor: 'Vitor Henrique Pereira Draeger', orientador: [5], linha: 2, arquivo: '2007-Vitor.pdf', 
  titulo: 'Extração de Informações em Patentes', 
  resumo_pt: 'Uma patente é um título de propriedade temporário sobre um invento (produto ou processo) caracterizada pela apresentação de uma nova forma de fazer algo ou pela exposição de uma nova solução para um problema. A patente garante aos seus proprietários o direito exclusivo da utilização do conhecimento, impedindo que terceiros usufruam da invenção sem a devida autorização durante a vigência do documento. Em um mundo em que o conhecimento e a agilidade são fundamentais, a Extração de Informação (EI) é uma área que vem sendo crescentemente explorada, pois define processos automatizados para a retirada de informações úteis de um grande volume de texto. Buscar conhecimento em uma base de patentes é uma tarefa interessante, já que estes documentos apresentam o que há de mais moderno em termo de desenvolvimentos científicos e tecnológicos. Com as informações extraídas pode-se, por exemplo, construir uma base de conhecimento técnico-científico e fazer prospecção tecnológica. Esta dissertação objetiva desenvolver um sistema computacional de EI baseado na identificação de sintagmas nominais e na inferência de relações entre estas entidades através da aplicação de regras que descrevem padrões linguísticos a serem buscados nos textos. Assim, ao contrário dos sistemas de EI convencionais que extraem informações com base em um tipo específico (nome de pessoa, datas, etc), o sistema proposto é genérico, embora, neste trabalho, o foco seja sua aplicação na extração de informação em um corpus de patente. Para avaliar os resultados produzidos pelo sistema, desenvolveu-se uma metodologia específica, uma vez que as convencionais requerem o uso de um corpus anotado, o que não ocorre com o corpus de patente elaborado. Esta metodologia, em vez de avaliar a correção da informação extraída, comparando-a com as anotações manualmente realizadas, usa como critério a utilidade desta informação, de acordo com o julgamento de avaliadores instruídos para a execução desta tarefa. Além da apresentação e análise dos resultados da avaliação, são discutidas as limitações do trabalho e sugeridas algumas melhorias que podem ser futuramente realizadas no sistema e na avaliação, bem como são apontados novos horizontes de pesquisa.', 
  resumo_en: '--'},

 {numero: 576, ano: 2007, dia: '11/01', autor: 'Marcelo Luz Sande e Oliveira', orientador: [11], linha: 3, arquivo: '2007-Marcelo.pdf', 
  titulo: 'Modelagem de Arquitetura de Software Orientada a Aspectos com UML 2.0', 
  resumo_pt: 'Atualmente, a definição da arquitetura de software possui um papel fundamental no processo de desenvolvimento. A arquitetura de um software é a descrição dos subsistemas e componentes e suas relações, fornecendo uma visão geral do software e facilitando a comunicação com os interessados. O propósito básico da definição de arquitetura é projetar uma estrutura para o software para servir de ponto de partida para a fase detalhada do projeto. As linguagens de descrição arquitetural (ADLs) permitem uma padronização e alguma formalização na descrição da arquitetura. No entanto, as ADLs não são amplamente empregadas devido a falta de ferramentas que dêem suporte ao seu uso e a falta de regras de rastreabilidade para as outras fases. Uma forma de contornar este problema seria utilizar a UML 2.0, considerada um padrão de facto para modelagem de software. Embora a UML 2.0 tenha apresentado uma evolução em relação à UML 1.4, principalmente no que diz respeito à modelagem arquitetura, ela ainda encontra-se distante da representação proporcionada pelas ADLs. Outro ponto que vem ganhando destaque é o desenvolvimento de software orientado a aspectos (DSOA). O objetivo do DSOA é modularizar os interesses transversais que encontram-se espalhados e entrelaçados pelo sistema. Inicialmente concebido focando a fase de implementação, atualmente o DSOA está presente em todas as fases do desenvolvimento, inclusive na arquitetura. Embora tenham surgido diversas soluções para a representação de aspectos em nível arquitetural (AO-ADLs), essas propostas sofrem da mesma crítica das ADLs regulares. Neste sentido, este trabalho propõe uma extensão da UML 2.0 para que seu pacote arquitetural incorpore os conceitos das ADLs textuais e represente relacionamentos transversais entre componentes. Além disso, este trabalho apresenta um conjunto de regras de transformação para uma linguagem de projeto detalhado orientada a aspectos. Desta forma, busca-se oferecer ao arquiteto modelar a arquitetura de software com UML 2.0, incluindo interações transversais e permitir que o projeto de arquitetura do sistema seja reforçado durante a implementação.', 
  resumo_en: '--'},

 {numero: 575, ano: 2007, dia: '00/00', autor: 'Fábio Rachid da Rocha', orientador: [11], linha: 3, arquivo: '2007-FabioRocha.pdf', 
  titulo: 'Instanciação e Execução de Modelos de Processo de Software no Eclipse Process Framework', 
  resumo_pt: 'Com o objetivo de aumentar a qualidade dos produtos de software, é importante definir e controlar o processo de desenvolvimento. Um Ambiente de Desenvolvimento de Software orientado ao processo (ADS) deve contemplar as fases de modelagem, instanciação e execução de forma integrada. Assim, um ADS fornece um mecanismo de controle que possibilite ao gerente estar ciente do estado e das ocorrências do projeto, além de contribuir para a atividade contínua de melhoria de processo. Este trabalho pesquisa uma proposta de modelo e mecanismos que contribuam para o controle do processo de desenvolvimento de software, envolvendo a definição, instanciação e execução integrada de processos. O ponto inicial da proposta é a ferramenta de modelagem Eclipse Process Framework (EPF, 2006), que permite a definição de modelos de desenvolvimento. Foram criados mecanismos para a instanciação e execução desse modelo, que trabalham de forma integrada à ferramenta de modelagem e propõem uma maneira de controlar o processo que ofereça o conjunto de informações consideradas relevantes para os atores e gerentes de projeto. O mecanismo de instanciação foi implementado como um plugin para o Eclipse, que utiliza como entrada o produto gerado pela ferramenta de modelagem e possibilita a definição de um conjunto de informações que transformem o modelo abstrato de um processo de software em um modelo executável. Neste conjunto de informações estão os prazos para as tarefas e as ferramentas que devem ser utilizadas durante sua realização, além da definição de atores e associação dos papéis aos quais os atores fazem parte. O mecanismo de execução foi implementado como um sistema Web que utiliza como entrada o produto gerado pela ferramenta de instanciação. O mecanismo oferece informações aos atores, os guiando ao longo do projeto. Deve ser utilizado como repositório de documentos, centralizando o arquivamento dos mesmos e facilitando o acesso por parte de qualquer um dos participantes do projeto. Permite o registro de ocorrências para cada tarefa e modificações dinâmicas ao longo da execução do processo.', 
  resumo_en: '--'},

 {numero: 574, ano: 2006, dia: '20/12', autor: 'Fabio Lopes Licht', orientador: [209], linha: 100, arquivo: '2006-Fabio.pdf', 
  titulo: 'Fornecimento Automatizado de Certificados de Curta Duração para Dispositivos Móveis em Grades Computacionais', 
  resumo_pt: 'Este trabalho apresenta a proposta de extensão de um serviço existente de fornecimento de certificados confiáveis de curta duração, com o intuito de autorizar nós móveis a ingressarem temporariamente em grades computacionais. Dentre as atividades deste trabalho, inclui-se a utilização de chaves públicas, privadas, fornecimento de certificados confiáveis e dinâmicos além de criptografia e utilização de redes móveis. O uso destes padrões em conjunto com a ferramenta MyProxy, responsável por fornecimento de certificados dinâmicos de curta duração, irá propiciar maior agilidade na inclusão de novos usuários e a sua extensão para inclusão de nós móveis que quiserem associar-se à grade computacional, modificando um padrão atualmente manual de inclusão de novos nós, em um sistema automático e funcional. Fortalecendo o uso da grade por dispositivos móveis e por usuários temporários, sem abrir mão da segurança.', 
  resumo_en: '--'},

 {numero: 573, ano: 2006, dia: '15/12', autor: 'Janilma Affife Raffid de Villara Peres', orientador: [11], linha: 100, arquivo: '2006-Janilma.pdf', 
  titulo: 'Integração e Composição de Características Transversais em Sistemas Multiagentes', 
  resumo_pt: 'O princípio da separação de características é um conceito bem estabelecido na Engenharia de Software, significando que cada característica do sistema deve ser tratada como uma unidade conceitual isolada. A Orientação a Aspectos surgiu como um paradigma para promover a modularização efetiva das características que estão espalhadas e entrelaçadas em um sistema, i.e., as chamadas caracterísiticas transversais. Embora na sua origem seja um paradigma complementar à Orientação a Objetos, a Orientação a Aspectos foi estendida e passou a cobrir outras abordagens, como os Sistemas Multiagentes. Os Sistemas Multiagentes são compostos por entidades autônomas, os agentes de software, que executam ações para atingir seus objetivos. Os agentes possuem algumas propriedades internas, como a autonomia, interatividade, pró-atividade, adaptação, aprendizado, colaboração, mobilidade e ser situado. Muitas características transversais podem estar presentes na modelagem dos Sistemas Multiagentes, sendo conseqüências de seus requisitos funcionais, não-funcionais e das propriedades internas dos agentes. No entanto, as linguagens e metodologias de modelagem para Sistemas Multiagentes existentes não oferecem suporte à modularização das características transversais. Este trabalho objetiva propor um framework de modelagem que modularize características transversais de amplo escopo em Sistemas Multiagentes pela aplicação de conceitos modelagem orientada a metas. Este framework, chamado ACROSS, utiliza as abstrações convencionais do paradigma dos agentes de software, como agentes, objetivos de agentes, planos e ações e traz novas abstrações como aspectos e objetivos de aspecto para modularizar elementos que possuem impacto transversal na modelagem dos agentes. O framework também apresenta relacionamentos que definem, respectivamente, a hierarquia entre essas abstrações e como a transversalidade ocorre em cada nível. Uma notação baseada em XML para descrever os modelos gerados foi proposta. Além disso, desenvolveu-se um estudo de caso de um sistema de Comando e Controle para a Artilharia de Campanha e sugeriu-se formas de mapear os modelos em código, considerando as tecnologias de implementação JADE, para Sistemas Multiagentes e AspectJ, para aspectos. Por fim, conclui-se sobre a aplicabilidade e limitações do ACROSS e propõe-se algumas orientações para pesquisa e desenvolvimentos futuros.', 
  resumo_en: '--'},

 {numero: 572, ano: 2006, dia: '08/08', autor: 'Welsing Moreira Pereira', orientador: [4], linha: 100, arquivo: '2006-Welsing.pdf', 
  titulo: 'Emprego de Protocolos Multicast na Camada de Aplicação no Suporte a Ambientes Virtuais Colaborativos', 
  resumo_pt: 'Ambientes Virtuais Colaborativos (AVCs) são sistemas onde um grupo de usuários podem interagir entre si e com o próprio sistema, permitindo assim usuários colaborarem. Muitas arquiteturas AVC avançadas contam fortemente com o uso do multicast na camada de rede no suporte a comunicação. No entanto, multicast na camada de rede não está prontamente disponível na Internet. Multicast na camada da aplicação (ALM – Application Layer Multicast) tem recentemente surgido como uma boa alternativa ao multicast na camada de rede. Neste trabalho, nós avaliamos diversos algoritmos ALM a partir das necessidades de rede impostas por aplicações AVC.', 
  resumo_en: '--'},

 {numero: 571, ano: 2006, dia: '07/08', autor: 'Fabricio Guedes Bissoli', orientador: [205], linha: 100, arquivo: '2006-FabricioBissoli.pdf', 
  titulo: 'Gerência de Memória Cooperativa em Proxies para Distribuição de Vídeo sob Demanda', 
  resumo_pt: 'No momento em que a popularização de vídeo em DVDs e transmissões digitais para TV tornam-se realidade, uma outra tecnologia se aproxima do sucesso: o vídeo digital transmitido sob demanda, ou simplesmente VoD (Video on Demand). Sistemas que envolvem VoD necessitam de uma infraestrutura especial em relação à transmissão live (ao vivo, como ocorre nas TVs digitais) devido a um alto tráfego ocasionado por diversos pedidos simultâneos ao servidor. Por essa razão várias pesquisas buscam soluções para a redução de carga no servidor e largura de banda utilizada por clientes na dorsal da rede. Nesse trabalho é apresentada a proposta de um protocolo de gerência de memória cooperativa em proxies entitulado ODPC (On Demand Proxy Collaboration). Seu objetivo é gerenciar o buffer de dois ou mais proxies de maneira a formar um sistema colaborativo entre os mesmos.', 
  resumo_en: '--'},

 {numero: 570, ano: 2006, dia: '04/08', autor: 'João Paulo de Brito Gonçalves', orientador: [14], linha: 100, arquivo: '2006-Joao.pdf', 
  titulo: 'Diferenciação de Serviços em Ambientes Virtuais Colaborativos em Larga Escala', 
  resumo_pt: 'Os ambientes virtuais colaborativos (AVCs) são aqueles que possibilitam compartilhamento do mundo virtual por usuários que não precisam estar fisicamente no mesmo local. Para a comunicação entre usuários, utiliza-se uma rede de comunicação. Ambientes Virtuais Colaborativos baseados na Internet apresentam potenciais problemas de desempenho, já que a Internet não provê garantias quanto à largura de banda e atraso. Como aplicações multimídia, eles necessitam de certas garantias mínimas para executarem com um desempenho adequado. O presente trabalho apresenta as etapas de desenvolvimento de um modelo de simulação para um protocolo de comunicação para AVCs. Para aumentar a confiabilidade deste protocolo, ele foi combinado com técnicas de qualidade de serviço e posteriormente modelado em um simulador, para que seu desempenho fosse validado através de simulações. Estudos sobre plataformas para AVCs existentes na literatura relacionada são apresentados bem como os resultados obtidos com as simulações executadas. O principal objetivo foi avaliar o desempenho da arquitetura na transmissão de mensagens em relação a atrasos e perda de pacotes.', 
  resumo_en: '--'},

 {numero: 569, ano: 2006, dia: '28/07', autor: 'Fabrício Nogueira da Silva', orientador: [8], linha: 100, arquivo: '2006-FabricioSilva.pdf', 
  titulo: 'In services: um Sistema para Gerenciamento de Dados Intermediários em Workflows Científicos na Bioinformática', 
  resumo_pt: 'Workflows científicos têm sido utilizados para a gerência de experimentos in silico. Esses experimentos são caracterizados por serem executados e analisados através de computadores. Em suas execuções, uma seqüência de programas computacionais é processada e os dados de saída de um programa são compostos como dados de entrada no programa seguinte. Na área de bioinformática, workflows científicos são comumente definidos utilizando-se linguagens de script. Porém, essa abordagem de definição, apesar de permitir a automatização da execução do experimento, possui algumas deficiências com relação a flexibilidade para atender diferentes necessidades dos cientistas sobre o workflow. Além disso, conta ainda com a deficiência de interoperabilidade entre os passos do workflow ou entre outros workflows. Para superar essas deficiências, a tecnologia de serviços Web vem sendo adotada pela comunidade científica como um facilitador para a disponibiliza- ção e acesso a programas científicos. Isso possibilitou que programas científicos utilizados em ambientes distintos pudessem ser integrados para comporem workflows científicos. Porém, a natureza distribuída dos serviços Web resgatou algumas questões referentes à gerência dos dados gerados e processados durante a execução dos experimentos - dados intermediários. Onde e como disponibilizar esses dados de modo que fiquem acessíveis para análises e reutilizações em futuras execuções dos experimentos foram algumas dessas questões levantadas com o uso de serviços Web na composição de workflows científicos. Uma outra questão diz respeito a como realizar a automatização de filtragem de dados durante as execuções dos workflows. Neste trabalho, foi proposto um sistema cuja arquitetura visa solucionar essas questões de gerência de dados intermediários em workflows científicos na bioinformática compostos por serviços Web - sistema In Services. Um workflow real chamado GARSA, em uso por um grupo de pesquisa (BioWebDB) da Fundação Oswaldo Cruz, foi utilizado como estudo de caso para validar o sistema proposto. Um prtótipo desse sistema foi implementado facilitando a geração de filtros a serem aplicados sobre dados intermediários de workflows científicos de bioinformática.', 
  resumo_en: '--'},

 {numero: 568, ano: 2006, dia: '28/07', autor: 'Walmir Oliveira Couto', orientador: [1], linha: 100, arquivo: '2006-Walmir.pdf', 
  titulo: 'Uso de Padrões de Código no Ensino de Java', 
  resumo_pt: 'Este trabalho apresenta uma avaliação da aplicabilidade prática de uma ferramenta de detecção de padrões de codificação para a linguagem Java. Iniciamos este trabalho instanciando um conjunto de 39 padrões sintáticos que foram incorporados à ferramenta. Em seguida, utilizando técnicas de experimentação de software, utilizamos esta ferramenta em ambiente acadêmico com os alunos de graduação em Engenharia de Computação do IME objetivando: avaliar a aplicação prática da ferramenta e dos anti-padrões codificados (comparando os programas escritos pelos alunos que utilizaram a ferramenta, com daqueles que não usaram a ferramenta); e analisar o impacto no ensino (o que os alunos aprenderam utilizando a ferramenta? Quais os ensinamentos que esta experiência trouxe para o alunado). Chegamos a conclusão que a ferramenta contribui para a obtenção de melhores programas, e que ela poderia ser utilizada como instrumento de apoio na aquisição do conhecimento sobre padrões de codificação; ajudando os alunos a adquirir o hábito de programar se preocupando com a qualidade do seu trabalho.', 
  resumo_en: '--'},

 {numero: 567, ano: 2006, dia: '24/07', autor: 'Ana Carolina Brito de Almeida', orientador: [8], linha: 100, arquivo: '2006-Ana.pdf', 
  titulo: 'Bioanot: um Sistema Multi-Agentes para Notificação de (Re)Anotações de Sequências em Banco de Dados Genômicos', 
  resumo_pt: 'A Bioinformática é uma área que está evoluindo rapidamente. Os projetos de pesquisa desta área, através do uso de ferramentas computacionais, geram um grande volume de dados. Como a maioria destes projetos colabora entre si, é fundamental que haja uma troca destes dados entre eles, a fim de que cada um tenha acesso à informação dos demais. Uma destas informações é a anotação genômica, que é a identificação de genes e sua função. Estes dados sobre anotações de seqüências estão em constante atualização. Como os projetos de pesquisa colaboram e estudam organismos correlatos, há a necessidade de se notificar biólogos sobre mudanças nesta anotação. Esta dissertação introduz o BioANot, um sistema multi-agentes para auxiliar os pesquisadores na troca de anotações genômicas. O BioANot auxilia no processo de anotação e oferece um mecanismo de notificação automática sobre (re) anotações de seqüências. As principais contribuições do BioANot são: a especificação de um mecanismo pró-ativo para manter os biólogos atualizados sobre anotações de seqüências de seu interesse e a melhoria na qualidade de anotação genômica, evitando a propagação de erros entre projetos colaborativos. Assim, por meio deste mecanismo, espera-se que cada projeto mantenha as anotações atualizadas sobre as seqüências que podem levar a descobertas sobre organismos em estudo.', 
  resumo_en: '--'},

 {numero: 566, ano: 2006, dia: '22/06', autor: 'Diogo Oliveira Paiva Mattos', orientador: [206], linha: 100, arquivo: '2006-Diogo.pdf', 
  titulo: 'Rosa+: uma Extensão do Modelo Rosa com Suporte a Regras e Inferência', 
  resumo_pt: 'ROSA é um repositório de Objetos de Aprendizagem (Learning Objects - LOs) com acesso semântico, que permite a criação, armazenamento, reuso e gerenciamento de LOs. Um LO é uma coleção de material reutilizável, utilizado para dar suporte ao aprendizado através de informações em seu conjunto de metadados. Conhecimento no ROSA é representado num modelo próprio, estendido a partir do RDF, onde LOs são acessados de forma contextualizada, determinada por associações semânticas entre eles. Este artigo apresenta o ROSA+, uma extensão do modelo ROSA, que visa deduzir conhecimento semântico através de propriedades de relacionamentos e de regras. Baseado na linguagem de ontologia OWL (Ontology Web Language), e na de regras SWRL (Semantic Web Rule Language), o ROSA+ realiza inferências sobre uma base de dados descrita em OWL, recuperando conhecimento não explicitado por sua representação ontológica.', 
  resumo_en: '--'},

 {numero: 565, ano: 2006, dia: '28/04', autor: 'Sérgio Augusto Freitas Filho', orientador: [1], linha: 100, arquivo: '2006-Sergio.pdf', 
  titulo: 'Detecção de Padrões de Código em WMLScript', 
  resumo_pt: 'Este trabalho apresenta uma ferramenta de detecção de padrões de codificação para a linguagem WMLScript. Em nossa pesquisa não encontramos outra ferramenta equivalente para esta linguagem. Em comparação com ferramentas similares para outras linguagens a ferramenta é mais flexível e permite a criação de novos padrões de forma fácil e intuitiva. A ferramenta baseou-se em um trabalho análogo desenvolvido neste instituto para a linguagem Java.  Ilustramos o uso da ferramenta através da implementação de um conjunto de padrões de codificação para a linguagem WMLScript. Não temos conhecimento de um padrão de codificação publicado para a linguagem. Assim, apresentamos como uma contribuição adicional um conjunto de padrões de código para WMLScript. Finalmente, para validarmos a funcionalidade da ferramenta e a utilidade dos padrões propostos, foi feito um estudo de caso aplicando-se os padrões a dezesseis programas de domínio público.', 
  resumo_en: '--'},

 {numero: 564, ano: 2006, dia: '05/04', autor: 'Vitor Guerra Rolla', orientador: [12], linha: 100, arquivo: '2006-Vitor.pdf', 
  titulo: 'Engenharia de Tráfego em Redes IP', 
  resumo_pt: 'Este trabalho apresenta uma abordagem alternativa para o problema do roteamento ótimo de menor caminho e propõe um conjunto de novos algoritmos para a resolução do problema. Tais algoritmos constituem um pacote (caixa de ferramentas) para o apoio às operações de engenharia de tráfego em redes IP. Em termos gerais, uma solução para o roteamento ótimo de menor caminho determina quais são os pesos ideais para os enlaces da rede de forma que protocolos de roteamento tradicionais (p.ex. OSPF) possam otimizar a distribuição do tráfego na rede. Foram considerados os objetivos principais da engenharia de tráfego: balanceamento de carga, otimização do uso dos recursos da rede e tolerância a demandas crescentes de tráfego. Resultados numéricos mostraram um desempenho bastante significativo para os algoritmos propostos.', 
  resumo_en: '--'},

 {numero: 563, ano: 2006, dia: '03/03', autor: 'Carlos Alberto Padilha Pinheiro', orientador: [210], linha: 100, arquivo: '2006-CarlosPinheiro.pdf', 
  titulo: 'Veículos Aéreos Não Tripulados para Monitoramento de Ambientes Desestruturados e Comunicação de Dados', 
  resumo_pt: 'Veículos aéreos autônomos não tripulados, tomados individualmente ou organizados em frota, têm ampla aplicação potencial e grande complexidade. O paradigma de sistemas multiagentes é um dos instrumentos para o seu tratamento, pois lida com complexidade e com autonomia de agentes. Além disto, sistemas multiagentes podem modelar sistemas multirobôs, e a frota de veículos aéreos é um sistema multirobô. Tal modelagem facilita a condução de testes de hipóteses e de soluções de projeto do sistema multirobô representado, tanto pela redução acentuada dos custos dos procedimentos, quanto pela presteza com que os testes são preparados e os respectivos resultados obtidos. Testes efetuados com os próprios robôs são demorados, caros e arriscados. Os que usam modelos físicos de robôs são geralmente de custo também elevado. Os sistemas multiagentes constituídos por agentes de software são, assim, alternativa competitiva como instrumento de projeto e desenvolvimento de sistemas multirobôs. Esta dissertação trata da modelagem de um simulador de frota de veículos aéreos autônomos não tripulados. A frota deve operar em ambiente desestruturado e oferecer serviços de telecomunicações e de monitoramento do ambiente. O modelo do simulador visa a construção de um sistema multiagente composto de agentes de software. Além da modelagem em si, a dissertação trata da avaliação do modelo produzido, mostrando o desenvolvimento de subsistemas do simulador a partir do modelo.', 
  resumo_en: '--'},

 {numero: 562, ano: 2006, dia: '22/02', autor: 'Alexandre Tadeu Rossinni da Silva', orientador: [9], linha: 100, arquivo: '2006-Alexandre.pdf', 
  titulo: 'Comportamento Social Cooperativo na Realização de Tarefas em Ambientes Dinâmicos e Competitivos', 
  resumo_pt: 'Ao longo da dissertação são apresentados os fundamentos necessários para o trabalho cooperativo. Nesse sentido, foi feita uma discussão filosófica sobre o comportamento social, dando ênfase nas relações entre indivíduos e ambiente. Entretanto, para o comportamento social surgir, é necessário um mecanismo democrático, que trate os indivíduos em igualdade de condição, já que todo indivíduo é importante para o ambiente. Contudo, entender o processo de tomada de decisão é fundamental para a obtenção de bons resultados. Para isso, a Teoria dos Jogos foi utilizada a fim de compreender o processo de tomada de decisão em um ambiente dinâmico, onde duas sociedades de robôs disputam um mesmo objetivo. Para a validação da proposta, a aplicação escolhida foi o futebol de robôs (RoboCup Small Size League f-180 com seus parâmetros e suas regras), por ser um desafio padrão da área de robótica. Na RoboCup f-180, um computador deve processar imagens capturadas por uma câmera, localizada acima do campo, e, a partir das informações extraídas das imagens, definir as ações cooperativas a serem executadas pela equipe. E importante acrescentar que o futebol de salão (futsal) foi a principal inspiração para o desenvolvimento da solução para a aplicação escolhida. Por fim, foram realizados testes para validar a solução.', 
  resumo_en: '--'},

 {numero: 561, ano: 2006, dia: '22/02', autor: 'Carlos André Batista de Carvalho', orientador: [5], linha: 100, arquivo: '2006-CarlosCarvalho.pdf', 
  titulo: 'O Uso de Técnicas de Recuperação de Informações em Criptoanálise', 
  resumo_pt: 'Este trabalho apresenta uma aplicação inovadora de técnicas de recuperação de informações em criptoanálise. Utilizando técnicas de agrupamento foi desenvolvido um novo procedimento para a determinação do período da chave no processo de criptoanálise polialfabética. Esse conhecimento foi, então, utilizado para concretizar o objetivo principal da dissertação: provar, experimentalmente, uma fraqueza nas cifras de bloco. Os padrões lingüísticos das mensagens são, de certo modo, propagados para os criptogramas. Entretanto, esses padrões são modificados em função da chave, que pode ser considerada como uma propriedade lingüística que determina o vocabulário da nova linguagem. O grande diferencial deste estudo é proporcionado pelo uso de técnicas de agrupamento, que permitem a identificação desses padrões por meio da separação de criptogramas de acordo com a chave usada na cifragem. No procedimento proposto para cifras polialfabéticas, o método Kasiski é integrado às técnicas de agrupamento, gerando resultados mais eficientes que os métodos tradicionais. O princípio fundamental das cifras de bloco é a obtenção de criptogramas com uma distribuição tão aleatórias de símbolos que não se consiga identificar uma correlação entre os dados de entrada e de saída. Entretanto, em cifras operadas no modo ECB, a realização com sucesso do agrupamento prova a existência dessa correlação. Assim, torna-se altamente não-recomendável o uso desse modo de operação.', 
  resumo_en: '--'},

 {numero: 560, ano: 2006, dia: '20/02', autor: 'Mauro Monteiro Silva', orientador: [11], linha: 100, arquivo: '2006-Mauro.pdf', 
  titulo: 'Plataforma Orientada a Serviços para o Desenvolvimento de Agentes Pró-Ativos', 
  resumo_pt: 'O desenvolvimento de Sistemas Multi-Agentes é uma abordagem nova que utiliza a abstração de agentes para modelagem de sistemas distribuídos e complexos. Os agentes são definidos como entidades que são capazes de ações autônomas em busca dos seus objetivos de projeto. Contudo, muitas das plataformas existentes não utilizam o conceito de objetivo como elemento de primeira ordem (elemento chave) na implementação de Sistemas Multi-Agentes, não permitem configurar o grau de pró-atividade dos agentes e criam um forte acoplamento entre os agentes e suas ações. A falta de uma representação para esses conceitos acaba por transferir a responsabilidade da implementação deles para o desenvolvedor, contribuindo para que estes conceitos fiquem misturados e espalhados juntos com a criação das ações executadas pelos agentes. Desta forma, fica difícil identificá-los na implementação dos agentes. Além disso, existe o forte acoplamento entre o agente e suas ações, o que dificulta o reuso e a manutenção das ações, dificultando a evolução dos agentes. Para resolver estes problemas, este trabalho propõe uma plataforma para o desenvolvimento de agentes baseada nos objetivos dos agentes, que foram identificados nas fases de análise. A plataforma oferece uma associação direta destes objetivos aos agentes e um mecanismo que permite ao desenvolvedor, configurar o grau de pró-atividade que o agente deve apresentar. Esta plataforma introduz alguns conceitos definidos na Arquitetura Orientada a Serviços (SOA) no desenvolvimento de Sistemas Multi-Agentes. Por meio dessa integração, busca-se uma redução no acoplamento existente entre o agente e suas ações, procurando facilitar o desenvolvimento das ações e promover o seu reuso.', 
  resumo_en: '--'},

 {numero: 559, ano: 2006, dia: '17/02', autor: 'Leonardo Lima dos Santos', orientador: [11], linha: 100, arquivo: '2006-Leonardo.pdf', 
  titulo: 'Geração da Modelagem de Sistemas Multi-Agentes a Partir de Cenários', 
  resumo_pt: 'A Engenharia de Software Orientada a Agentes é uma abordagem recente, entretanto, o paradigma de agentes vem sendo utilizado com sucesso em aplicações industriais, tais como telecomunicações e comércio eletrônico. Por ser uma área que ainda não está madura, carece de metodologias de especificação, técnicas, e ferramentas, que ofereçam suporte ao desenvolvimento de sistemas orientados a agentes. O grande interesse nesta área levou à elaboração tanto de metodologias de desenvolvimento quanto de linguagem de modelagens de SMA. Entretanto, estas metodologias e linguagens não se preocupam explicitamente com a elicitação de requisitos e a sua transformação em modelos de análise de SMA, o que gera uma lacuna entre eles e os artefatos produzidos ao longo da modelagem do SMA. Para tratar do problema acima exposto, nesta dissertação, apresentamos um conjunto de diretrizes para a geração dos artefatos de modelagem de SMA, que emprega a técnica de cenários em conjunto com o Léxico Ampliado de Linguagem (LAL) como fonte de informação. Especificamente, esta dissertação descreve como a lacuna entre a elicitação de requisitos e a análise orientada a agentes pode ser diminuída. O método proposto guia a construção dos artefatos de modelagem de SMA, permitindo um rastreamento inicial dos elementos presentes nos diagramas, pelos rastros que são registrados ao longo da aplicação do método. Nosso trabalho está focado na geração dos diagramas de ANote que especificam a estrutura de um SMA: diagramas de objetivos, ambiente e agentes. Finalmente, a aplicação das diretrizes é ilustrada por um estudo experimental: Meeting Scheduler.', 
  resumo_en: '--'},

 {numero: 558, ano: 2006, dia: '17/02', autor: 'Simone Kimihe Kawasaki de Oliveira', orientador: [3], linha: 100, arquivo: '2006-Simone.pdf', 
  titulo: 'Algoritmos Dedicados para Cálculo de Vertex Separation / Layout Ótimo e Edge Search Number / Plano de Busca Ótimo em Árvores Binárias Cheias', 
  resumo_pt: 'Um layout linear L é a numeração dos vértices de um grafo. Dado um layout linear L, o vertex separation de G = (V, E) com relação a L, vsL(G), é a máxima quantidade de vértices que encontram-se à esquerda de um vértice qualquer, e que possuem alguma aresta incidente à direita deste vértice. O vertex separation de G, vs(G), consiste em encontrar o menor valor possível de vsL(G) considerando todos os layouts L de G. O layout que efetiviza o mínimo é chamado de layout ótimo. O edge search number de um grafo G, esn(G), envolve encontrar a menor quantidade de guardas necessária para capturar um fugitivo móvel que encontra-se escondido em uma aresta de G. Um plano de busca ótimo é uma seqüência de movimentações de guardas que utiliza esn(G) guardas para capturar o fugitivo. Vertex separation, layout ótimo, edge search number e plano de busca ótimo são problemas NP-completos em grafos. No entanto, todos estes problemas possuem algoritmos de tempo polinomial para árvores. O principais resultados deste trabalho são quatro algoritmos que foram adaptados a partir dos seus algoritmos lineares em árvores para o caso particular das árvores binárias cheias. Esta dissertação também apresenta e prova dois novos teoremas. O primeiro deles mostra que o vertex separation de uma árvore binária cheia Th depende de sua altura h, ou seja, vs(Th) = (h+1)/2. Para as árvores binárias cheias, o segundo teorema melhora o resultado para grafos obtido por Ellis em 1994, onde vs(G) <= esn(G) <= vs(G) + 2. Este novo teorema determina que o edge search number de uma árvore binária cheia Th é igual ao seu valor de vertex separation mais 1, mais precisamente, esn(Th) = vs(Th) + 1.', 
  resumo_en: '--'},

 {numero: 557, ano: 2006, dia: '01/02', autor: 'Carlos Cesar Gomes São Braz', orientador: [204], linha: 100, arquivo: '', 
  titulo: 'Algoritmos de Roteamento em Malhas Rodoviárias com Sistemas de Informações Geográficas', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 556, ano: 2005, dia: '25/11', autor: 'Fernando Apolinário Pereira', orientador: [9], linha: 100, arquivo: '2005-Fernando.pdf', 
  titulo: 'Múltiplos Robôs Móveis Autônomos em uma Estrutura Cooperativa', 
  resumo_pt: 'A utilização de sistemas robóticos na realização de tarefas repetitivas já está bastante explorada na indústria. Porém, em ambientes desestruturados ou semi-estruturados, dos quais temos informações limitadas quanto à disposição estática ou dinâmica dos objetos, a utilização de novas abordagens, modelos e sistemas de controle faz-se necessária. Neste trabalho é proposto um sistema composto por dois robôs móveis autônomos em uma tarefa cooperativa, utilizando-se uma arquitetura líder-seguidor. Um dos robôs (líder) utiliza uma estratégia global (campo potencial artificial) e local (lógica fuzzy) para o seu planejamento de trajetória, do qual apenas os pontos de partida e de chegada são conhecidos. O robô seguidor acompanha fidedignamente a movimentação do líder e auxilia na solução de mínimos locais. A tarefa consiste no transporte livre de obstáculos de uma barra pelo conjunto líder-seguidor. Os protótipos experimentais dos robôs e simulações foram implementadas, a fim de validar o sistema proposto.', 
  resumo_en: '--'},

 {numero: 555, ano: 2005, dia: '00/10', autor: 'Evandro da Cruz Goulart', orientador: [7], linha: 100, arquivo: '2005-Evandro.pdf', 
  titulo: 'Concepção e Implementação de um Ambiente de Aula na Web', 
  resumo_pt: 'A prática do ensino a distância está sendo cada vez mais utilizada e apresentando melhores resultados. Através do uso da rede mundial de computadores, o aluno, munido de um computador, pode, mesmo estando separado no espaço e possivelmente no tempo também de seus colegas e professores, ter as suas atividades monitoradas, interagir e promover discussões com os colegas de um curso, utilizar diferentes recursos multimídias no seu aprendizado, entre outras possibilidades. Estas e muitas outras vantagens da modalidade do EAD, mediado por computador, promovem a cada dia o surgimento de novos ambientes virtuais, sejam estes comerciais ou acadêmicos. Neste trabalho, é descrita a concepção de uma ambiente de aula na Web, que tem como objetivo principal facilitar a elaboração de conteúdos didáticos pelo professor. O material didático gerado pode conter diferentes recursos como sons, filmes, animações, etc, tornando assim a aula muito mais interessante de ser assistida pelo aluno. Além de possuir ferramentas, que facilitem o professor na produção de novos materiais, outras funcionalidades como upload e download estão disponíveis para o professor. Para assistir este conteúdo, o aluno faz uso de um ambiente de aula assíncrono, composto de ferramentas que promovem o acompanhamento e a interação entre alunos e professores.', 
  resumo_en: '--'},

 {numero: 554, ano: 2005, dia: '12/09', autor: 'Fábio de Oliveira Fagundes', orientador: [209], linha: 100, arquivo: '2005-FabioFagundes.pdf', 
  titulo: 'Sistema de Monitoramento Passivo de Segurança de Grid Computacional', 
  resumo_pt: 'Grids computacionais ou ambientes distribuídos em larga escala têm o potencial de se tornarem plataformas poderosas utilizadas pela comunidade de computação distribuída, tanto científica quanto comercial, para a execução de aplicações de grande importância e alto teor computacional. Estes ambientes computacionais podem estar constituídos por computadores geograficamente dispersos e sob diferentes domínios administrativos, em um ambiente tão vasto, segurança certamente é uma preocupação. Um dos problemas envolvendo a adoção e a implementação de políticas ou normas de segurança para sistemas de informação consiste em verificar se as regras efetivamente implementadas estão em conformidade com aquelas definidas nos respectivos padrões adotados. Este trabalho estende e adapta o modelo de verificação automática de aderência proposto por (GONÇALVES, 2005) para as particularidades de grids computacionais. O Sistema de Monitoramento Passivo de Segurança de Grid Computacional - SiMPaSeG é então apresentado como uma solução que aproveitase do ambiente de execução remota habilitado pelo Middleware da grid - o Globus Toolkit - para verificar se os controles adotados estão efetivamente implementados e se são eficazes. Esses controles são recomendações sobre medidas e procedimentos que podem ser adotados de modo a salvaguardar a informação. A Norma Internacional de Segurança da Informação ISO/IEC 17799:2005 é a mais importante e internacionalmente adotado padrão de segurança.', 
  resumo_en: '--'},

 {numero: 553, ano: 2005, dia: '12/09', autor: 'Márcio Leyes Dutra', orientador: [211], linha: 100, arquivo: '2005-Marcio.pdf', 
  titulo: 'Otimização Paralela de Consultas com Funções de Usuários em Ambiente de Grid', 
  resumo_pt: 'O Grid é um ambiente computacional que vem despertando grande interesse na área de computação distribuída, pois oferece uma infra-estrutura para agregar uma grande quantidade de recursos computacionais, que podem estar dispersos geograficamente por instituições diferentes. Um ponto importante desta infra-estrutura é a forma de gerenciar estes recursos através de distintos níveis hierárquicos de usuários, com políticas de compartilhamento e gerenciamento dos usuários e recursos, dando origem ao conceito de organização virtual. Esse ambiente computacional favorece as aplicações que se beneficiam de algum tipo de paralelismo ou aquelas com grande demanda computacional. A utilização e a ampliação da Internet vêm aumentar largamente esse potencial. O nosso estudo de caso se concentra num problema de cálculo da trajetória de partículas sobre um fluxo sangüíneo em uma artéria. Esta é uma aplicação de visualização cientifica, que exige um grande esforço computacional. Isso motivou o desenvolvimento do middleware CoDIMS-G que oferece uma infra-estrutura para suportar a computação em Grid e conseqüentemente a redução dos tempos de execução. Esse middleware é construído sobre a arquitetura CoDIMS, e que se baseia em componentes para as principais funções de um sistema integrador de dados, onde cada um desses componentes é desenvolvido especificamente para cada aplicação. Nesse contexto, o objetivo desta dissertação é a construção do componente otimizador de consultas para o middleware CoDIMS-G, buscando proporcionar a máxima exploração dos recursos em um mínimo de tempo. Isso é realizado através de uma estratégia de otimização de consultas distribuídas e paralelas para o ambiente de Grid. Essa estratégia utiliza um algoritmo proposto chamado G2N, que tem a finalidade de selecionar os nós do Grid para paralelizar as funções de usuários ou programas baseando-se em informações históricas.', 
  resumo_en: '--'},

 {numero: 552, ano: 2005, dia: '24/08', autor: 'Fábio Suim Chagas', orientador: [9], linha: 100, arquivo: '2005-FabioChagas.pdf', 
  titulo: 'Manipulador Bilateral com Realimentação Háptica', 
  resumo_pt: 'Esta dissertação trata do desenvolvimento de um sistema de telemanipulação com realimentação háptica, visto que este tema tem despertado grande interesse na comunidade científica nas útimas décadas devido à sua interdisciplinaridade e à sua vasta gama de aplicações. O desenvolvimento do sistema foi elaborado em várias etapas a saber: (i) a primeira, foi a elaboração do projeto do manipulador que consistiu na obtenção das equações cinemáticas e dinâmicas; (ii) a segunda, consistiu na elaboração da arquitetura de controle para o sistema; (iii) a terceira, foi a elaboração de um protocolo de comunicação desenvolvido utilizando-se programação de sockets; (iv) a quarta, consistiu na elaboração de um simulador que foi implementado na liguagem JAVA combinada com VRML (Virtual Reality Modeling Language) através da Java EAI (External Authoring Interface); (v) a quinta, foi o desenvolvimento de uma placa de captura e escrita de dados, utilizada como interface para interligar dois manipuladores reais de um grau de liberdade à fim de validar o sistema robótico proposto.', 
  resumo_en: '--'},

 {numero: 551, ano: 2005, dia: '22/08', autor: 'Melina de Vasconcelos Alberio', orientador: [4], linha: 100, arquivo: '2005-Melina.pdf', 
  titulo: 'Acontece – um Ambiente Virtual Colaborativo para Treinamento Cirúrgico', 
  resumo_pt: 'A Realidade Virtual é uma técnica da Ciência da Computação que visa simular atividades em um mundo sintético, como se fossem desenvolvidas no mundo real, estimulando os sentidos humanos. Ambientes Virtuais Colaborativos são sistemas de Realidade Virtual distribuídos em rede, cuja finalidade é integrar alguns usuários que desejem colaborar entre si através do mundo sintético para resolução de um problema proposto pelo sistema. Ambientes Virtuais aplicados à Medicina são utilizados para fins de diagnose, educação tanto de médicos quanto de seus pacientes, simulação de procedimentos invasivos, dentre outros. Os simuladores cirúrgicos são utilizados para o planejamento pré-operativo, treinamento ou assistência em cirurgia. Esta dissertação apresenta o ACOnTECe (um Ambiente Virtual COlaborativo para TrEinamento Cirúrgico), um ambiente virtual não-imersivo para o estudo da anatomia cardíaca e treinamento de transplante de coração, levando em consideração os pré-requisitos necessários para que o ambiente virtual seja realista e atrativo aos usuários conectados remotamente ao ambiente.', 
  resumo_en: '--'},

 {numero: 550, ano: 2005, dia: '14/07', autor: 'Eduardo de Almeida Cadorin', orientador: [4], linha: 100, arquivo: '2005-EduardoCadorin.pdf', 
  titulo: 'ACAmPE: Ambiente Virtual Colaborativo para a Área do Petróleo', 
  resumo_pt: 'A realidade virtual está cada vez mais presente e, em certas atividades, está se tornando essencial. O uso de ambientes virtuais, para se efetuar treinamentos, vem aumentando consideravelmente e está se tornando uma peça chave neste processo. Permitir que situações de risco e de emergência sejam treinadas, unir funcionários geograficamente distantes permitindo que eles interajam no mundo virtual, são apenas algumas das novas facilidades que esta tecnologia oferece. Nesta dissertação é apresentado um ambiente virtual colaborativo, para uso em treinamentos na área de petróleo. O protótipo apresentado neste trabalho visa oferecer um ambiente de treinamento para funcionários que, trabalham ou vão trabalhar, em uma plataforma de petróleo. O mundo virtual tratado é um modelo, em escala, de uma plataforma de petróleo real, obtendo-se assim um alto nível de detalhamento e realismo. É apresentado um estudo sobre ambientes virtuais colaborativos, voltados para treinamentos, bem como uma análise comparativa entre eles e o protótipo deste trabalho.', 
  resumo_en: '--'},

 {numero: 549, ano: 2005, dia: '14/07', autor: 'Eduardo Soares Albuquerque', orientador: [1], linha: 100, arquivo: '2005-EduardoAlbuquerque.pdf', 
  titulo: 'Detecção de Padrões de Código Java na Plataforma Eclipse', 
  resumo_pt: 'Este trabalho apresenta uma ferramenta para detecção desses padrões integrada à Plataforma Eclipse. A vantagem da ferramenta apresentada é implementar uma nova linguagem de descrição de padrões, definida neste trabalho, que permite que padrões sejam escritos de forma declarativa. Isso torna a ferramenta mais flexível que as ferramentas existentes. Nesta linguagem, os padrões são representados na forma de árvores, que são buscados pela ferramenta na AST do programa fonte. O algoritmo desenvolvido para fazer o casamento utiliza uma estratégia de clonagem de padrões, a fim de realizar o casamento de todas as instâncias dos padrões selecionados para detecção, percorrendo a AST do programa uma única vez. É apresentado ainda um estudo de caso realizado com a utilização da ferramenta sobre um sistema comercial, que mostra a flexibilidade, facilidade de uso e a eficiência da ferramenta em um projeto real.', 
  resumo_en: '--'},

 {numero: 548, ano: 2005, dia: '09/06', autor: 'Gabriel André Duquesnois Dubois Brito', orientador: [206], linha: 100, arquivo: '2005-Gabriel.pdf', 
  titulo: 'Integração de Objetos de Aprendizagem no Sistema ROSA – P2P', 
  resumo_pt: 'ROSA é um sistema de e-learning que permite a criação, armazenamento, re-uso e gerenciamento de objetos de aprendizagem – Learning Objects (LOs). Um LO, caracterizado por um conjunto de descritores de metadados, é uma coleção de material reutilizável usado para suportar aprendizagem, educação e/ou treinamento. Entretanto, o sistema ROSA ainda é um sistema centralizado, não permitindo a integração de LOs criados em outras instituições. Esta dissertação apresenta a evolução do ROSA em um sistema peer-to-peer (P2P), denominado ROSA - P2P, capaz de realizar a integração de LOs oriundos de diferentes peers. O ambiente P2P provê a interoperabilidade entre os peers ROSA - P2P, incluindo estratégias específicas para: conexão/desconexão de peers à rede P2P; definição e eleição de super-peers; balanceamento e redistribuição de peers no sistema; e alguns aspectos de tolerância à falhas. O sistema de integração de dados possui procedimentos de reenvio e reescrita de consultas baseados no seu significado semântico, assim como mecanismos para a correta integração dos respectivos resultados. Vocabulários controlados são utilizados para suportar estes processos, permitindo dentre outras coisas, a correta interpretação semântica dos dados e resolução de possíveis conflitos semânticos.', 
  resumo_en: '--'},

 {numero: 547, ano: 2005, dia: '19/05', autor: 'Paulo Gustavo Portella Ziemer', orientador: [14], linha: 100, arquivo: '2005-Paulo.pdf', 
  titulo: 'Extensão e Avaliação do Protocolo RSVPv2 em Redes Móveis Infra-Estruturadas', 
  resumo_pt: 'Os principais elementos de um sistema de comunicação capaz de oferecer qualidade de serviço, na Internet, são: o escalonador, o controle de admissão e o protocolo de sinalização de reserva de recursos. O protocolo RSVP foi definido, pela IETF, como padrão para sinalização de qualidade de serviço na Internet. No entanto, este protocolo apresenta problemas de escalabilidade e seu funcionamento não é adequado para redes móveis. Para minimizar estes problemas e oferecer suporte à mobilidade, (BRUNNER et al., 2003) propuseram o protocolo RSVPv2. Este é orientado ao emissor e utiliza apenas uma mensagem de reserva. Embora os autores tenham indicado a utilização de um identificador único de reserva, para que o protocolo de sinalização trabalhe corretamente em redes móveis, ainda se faz necessário que diversos outros aspectos, relacionados com a criação, manutenção e remoção das reservas em redes móveis infra-estruturadas, sejam especificados. Sendo assim, este trabalho apresenta as extensões e a avaliação do protocolo RSVPv2 em ambientes de redes móveis infra-estruturadas. As extensões, propostas neste trabalho, são as seguintes: emprego da mensagem de criação de reserva, para que a reserva seja rapidamente criada após o acontecimento de um handoff; utilização da mensagem de remoção de reserva, enviada do roteador de cruzamento, para que a reserva, do caminho antigo, seja rapidamente eliminada; utilização da mensagem de anúncio de agente de mobilidade com finalidade de transporte de informações de reconhecimento da criação da reserva, quando o nó móvel se encontra distante de seu agente domiciliar. O protocolo RSVPv2 e as extensões propostas foram avaliados através do simulador de redes NS-2. A análise, das simulações realizadas, mostrou que a utilização do identificador único de reserva juntamente do emprego da mensagem específica de criação da reserva, após um handoff é capaz de reduzir o atraso do estabelecimento da reserva. A utilização da mensagem de remoção explícita de reserva pode aumentar o número de sessões aceitas pela rede em, aproximadamente, 33 %. Simulações quanto à utilização da mensagem de anúncio de agente de mobilidade estendida com informações de reconhecimento foram realizadas e indicaram redução no atraso do estabelecimento da reserva.', 
  resumo_en: '--'},

 {numero: 546, ano: 2005, dia: '24/02', autor: 'Cícero Nogueira dos Santos', orientador: [208], linha: 100, arquivo: '2005-Cicero.pdf', 
  titulo: 'Aprendizado de Máquinas na Identificação de Sintagmas Nominais: o Caso do Português Brasileiro', 
  resumo_pt: 'Nos últimos anos, tem-se observado um retorno aos métodos empíricos de Processamento de Linguagem Natural, em que a aquisição do conhecimento pela máquina é majoritariamente realizada com base nos dados. Desde a última década, várias técnicas de Aprendizado de Máquina (AM) têm sido utilizadas para a identificação automática de sintagmas nominais (SNs). A identificação de SNs em textos tem aplicações em diversos problemas como: recuperação e extração de informações, análise sintática, resolução de co-referência, identificação de relações semânticas, entre outros. A maior parte dos trabalhos publicados na área têm o inglês como língua alvo; trabalhos sobre o uso de técnicas de AM para a identificação de SNs do português brasileiro não foram encontrados. Nesse trabalho, a identificação automática de SNs do português brasileiro é tratada como uma tarefa de classificação a ser automaticamente aprendida com o uso da técnica de AM chamada Aprendizado Baseado em Transformações (do inglês Transformation Based Learning – TBL). Nesta abordagem, o aprendizado é guiado por um corpus de treino que contém exemplos corretamente classificados. A própria classificação dos exemplos foi derivada automaticamente a partir de um corpus preexistente. O conhecimento lingüístico gerado por essa técnica consiste de uma lista ordenada de regras de transformação, que pode ser utilizada para a classificação de novos textos. Para a redução dos erros de classificação de preposições foi proposto um novo tipo de molde de regras, cuja a unidade básica, aqui chamada de Termo Atômico, possui uma janela de contexto de tamanho variável e um teste que precede a captura dos valores que compõem as regras. Nesse trabalho também é mostrado que o uso de uma classificação inicial mais precisa para o português e o uso de um lematizador de verbos contribuem para a redução do tempo de treinamento, e, ainda, trazem alguns benefícios para a eficácia das regras aprendidas. Na identificação de SNs do português brasileiro com o uso da ferramenta TBL desenvolvida foi obtido, no melhor caso, precisão de 86,6% e abrangência de 85,9%.', 
  resumo_en: '--'},

 {numero: 545, ano: 2005, dia: '23/02', autor: 'Mauricio Costa Reis', orientador: [13], linha: 100, arquivo: '2005-Mauricio.pdf', 
  titulo: 'Descoberta de Conhecimento em Documentos XML', 
  resumo_pt: 'Esta dissertação propõe novos métodos para implementação das tarefas de associação e classificação em documentos XML. Associação e classificação são duas importantes tarefas de KDD (Knowledge Discovery in Databases), originalmente definidas para aplicação em bancos de dados relacionais. XML é um padrão de representação para armazenamento e intercâmbio de dados, que vem tendo grande disseminação pela sua aplicação na Internet e se caracteriza por ser uma estrutura de dados semi-estruturados hierarquizada. Estudos de descoberta de conhecimento em dados semi-estruturados se encontram em estágio incipente e normalmente envolvem técnicas adaptadas de KDD para dados estruturados, cujos estudos se encontram em estágio mais avançado. As técnicas tradicionais de associação - baseados no algoritmo Apriori - envolvem a geração de conjuntos de itens candidatos e apresentam baixo desempenho devido ao grande número de iterações necessárias para obtenção dos resultados desejados. Os métodos propostos, além de mais adequados para tratamento de dados semi-estruturados (como XML), não fazem a geração de conjuntos de itens candidatos e, assim, mostra um desempenho superior quando comparado com a implementação do algoritmo Apriori.', 
  resumo_en: '--'},

 {numero: 544, ano: 2005, dia: '21/02', autor: 'Francisco Handrick Tomaz da Costa', orientador: [211], linha: 100, arquivo: '2005-Francisco.pdf', 
  titulo: 'ROSAI – uma Proposta de Representação do Modelo ROSA em Linguagem Lógica', 
  resumo_pt: 'O ROSA é um sistema para recuperação de objetos de aprendizado baseado na descrição semântica de seus conteúdos. Este trabalho tem por objetivo estender o modelo de dados ROSA, a partir de sua representação como uma base de conhecimento, na quais assertivas são definidas como fatos e restrições e regras são expressas através de fórmulas lógicas. As consultas presentes na estensão do modelo serão expressões lógicas com variáveis livres. Sua avaliação sobre a base de conhecimento permite a inferência de novos fatos, segundo a premissa do mundo aberto. A base de conhecimento é implementada em uma máquina Prolog que tratará fatos e regras, correspondendo ao domínio de aplicações suportadas pelo ROSA.', 
  resumo_en: '--'},

 {numero: 543, ano: 2005, dia: '14/02', autor: 'Sandro Santos de Lima', orientador: [9], linha: 100, arquivo: '2005-Sandro.pdf', 
  titulo: 'Análise e Desenvolvimento de um Ambiente de Simulação para Aplicações Domóticas', 
  resumo_pt: 'Dá-se o nome de domótica à área da Robótica que trata do emprego da Tecnologia da Informação no sistema de controle de residências. Domótica vem da contração da palavra latina domus, cujo significado é casa, com a palavra robótica. A casa inteligente proposta neste trabalho busca satisfazer quatro necessidades básicas dos ocupantes de uma edificação: energia, comunicações, conforto e segurança. Isto é possível graças a ação de uma série de sistemas interoperáveis que são capazes de realizar as seguintes tarefas: identificar os moradores; aprender seus hábitos; adaptar e monitorar as condições internas dos cômodos para que estes permaneçam de acordo com as preferências de seus ocupantes; e proteger a casa contra invasores, incêndios e gases tóxicos. A modelagem orientada a objetos, utilizada na descrição dos serviços da casa, permite a inclusão de novos serviços, bastando para isto implementar os sistemas necessários ao seu tratamento. A ação personalizada da casa é possível graças a utilização do sensor de passos, elemento que possibilita a identificação dos indivíduos a partir de características peculiares presentes em seu caminhar. Optou-se pela utilização do sensor de passos porque considera-se que em uma residência, o sistema de identificação deve ser não invasivo, ou seja, o morador deve ser identificado sem que para isto tenha que utilizar qualquer dispositivo, ou então, interagir de maneira consciente com o sistema de identificação. Além disso, como pode ser instalado em toda a residência, o sistema de identificação será capaz de saber a posição exata de todos os ocupantes da casa. Para o tratamento dos dados fornecidos pelo sensor, algoritmos específicos são empregados para a determinação dos parâmetros do caminhar do indivíduo. Uma rede neural artificial executa a tarefa de reconhecimento dos indivíduos, através da classificação dos seus padrões de caminhar. O sistema de identificação é capaz de reconhecer indivívuos previamente identificados, mesmo quando estes apresentam variação em seu padrão de caminhar; e devido ao processo de aprendizado não supervisionado da rede neural, o sistema é capaz de reconhecer novos moradores sem a necessidade de treinamento da rede. Para validar a proposta da casa inteligente, desenvolveu-se o sistema adaptativo de iluminação, que trabalhando cooperativamente com o sistema de identificação, efetua o controle personalizado da luminosidade de um ambiente. Com os resultados obtidos, o sistema mostrou-se bastante confiável, sendo capaz de garantir a ação personalizada da casa, além de proporcionar uma substancial redução do consumo de energia elétrica.', 
  resumo_en: '--'},

 {numero: 542, ano: 2005, dia: '14/02', autor: 'Wagner Tanaka Botelho', orientador: [9], linha: 100, arquivo: '2005-Wagner.pdf', 
  titulo: 'Um Sistema de Identificação e Adaptação Pervasivo para a Casa Inteligente Utilizando Sistemas Multi-Agentes', 
  resumo_pt: 'A área de Inteligência Artificial Distribuída (IAD) é uma dentre diversas áreas, desenvolvidas nos últimos anos, pela constante busca por sistemas autônomos inteligentes. Surgiu, no início da década de 70, da junção entre as áreas de IA (Inteligência Artificial) e sistemas distribuídos. A casa inteligente proposta nesse trabalho tem como objetivo principal a sua adptação com um mínimo de interferência dos moradores. Os moradores não precisarão digitar códigos em painéis de controle, serem observados por câmeras de vídeo presentes na casa, ou ficarem acionando dispositivos por meio de voz. A casa inteligente é um sistema composto por uma série de agentes que interagem e cooperam entre si, com o objetivo de fazer com que a mesma cumpra de maneira autônoma, as tarefas que nas casas convencionais ficam a cargo do morador. O cômodo é adaptado de acordo com as preferências de temperatura ou luminosidade, podendo-se monitorar o consumo de energia; e medidas de segurança serão tomadas quando um invasor entrar na casa. A arquitetura proposta neste trabalho possui duas camadas. Na primeira camada, o agente é responsável por informar a temperatura, a luminosidade atual e a entrada ou saída de um ocupante no cômodo. Além disso, qualquer alteração na temperatura ou luminosidade no cômodo é solicitada para este agente. Na segunda camada, encontram-se dois grupos de agentes. No primeiro grupo, os agentes recebem e informam as modificações necessárias nos cômodos. No segundo grupo, o agente é responsável por atualizar o modo de segurança e acionar a polícia ou firma de segurança quando um invasor entrar na casa. Uma característica importante nesta arquitetura é a possibilidade de acrescentar um agente sem a necessidade de modificar a arquitetura. Assim, o sistema é aplicável a diferentes configurações de instalações que podem ser além de casas, escritórios, hospitais, hotéis, etc. Neste projeto, combina-se uma metodologia orientada a agentes chamada MaSE com UML (Unified Modeling Language), juntamente com o software AgentTool, que é utilizado no desenvolvimento das etapas definidas na metodologia. A linguagem de programação Java, a linguagem de comunicação entre os agentes KQML, o banco de dados MySQL e o SACI, que é a ferramenta responsável pela comunicação entre os agentes, foram as tecnologias escolhidas para a implementação do sistema.', 
  resumo_en: '--'},

 {numero: 541, ano: 2004, dia: '13/09', autor: 'Ermírio de Siqueira Coutinho', orientador: [211], linha: 100, arquivo: '2004-Ermirio.pdf', 
  titulo: 'Armazenamento e Recuperação de Objetos em Ambientes Virtuais Colaborativas para SIG - 3D', 
  resumo_pt: 'SIG (Sistemas de Informações Geográficas) são sistemas computacionais especializados em análises sobre dados que possuam uma coordenada associada. Pode-se esquematizar um SIG como possuindo 3 camadas principais, essenciais e interdependentes que coexistem: uma de armazenamento dos dados (camada de persistência dos dados, constituída pelo Sistema Gerenciador de Banco de Dados – SGBD), uma responsável por executar a análise espacial desses dados e uma que permita a visualização (geralmente gráfica) dos resultados. A maioria dos SIG possui uma interface para visualização gráfica dos resultados em 2 dimensões. Atualmente em algumas aplicações específicas é fundamental que a visualização seja feita em 3 dimensões. Aliando-se conhecimentos das áreas de Realidade Virtual e Ambientes Virtuais Colaborativos surgem os SIG tridimensionais (SIG-3D). Tendo em vista o desenvolvimento futuro de um SIG-3D que permita executar-se análises espaciais sobre feições visualizadas em três dimensões, esta dissertação objetiva: (a) fazer um estudo sobre armazenamento de objetos para SIG-3D em um SGBD; (b) propor um modelo de dados genérico, independente de feições temáticas e (c) realizar o tratamento de oclusão a nível de armazenamento. A fim de tornar o SIG-3D compatível com outros sistemas, procurou-se, sempre que possível, adequar-se este trabalho aos padrões vigentes. Nesse sentido, adotou-se o padrão OpenGIS como referência para o desenvolvimento desta dissertação, sugerindo um modelo conceitual de dados mínimo, comum a todos os SIG-3D. Um dos principais problemas na geração (restituição) de cenas tridimensionais em ambientes densos de objetos, tal como ocorre em simulações de ambientes urbanos em um SIG-3D, é o tratamento do alto número de dados descartados por terem sua visualização obstruída em relação ao observador pela existência de algum elemento de proporções suficientes entre eles. Deve-se evitar que esses dados descartados sejam recuperados do SGBD desnecessariamente para sua remessa à camada de visualização, a fim de otimizar o sistema e permitir a fluidez da simulação, com a finalidade de aumentar a sensação de imersão no ambiente. Para atingir esse objetivo, esta dissertação apresenta um algoritmo de oclusão para simulação em ambientes densos com novo enfoque, que armazena seus resultados na camada de persistência e pretende assim reduzir consideravelmente o número de objetos (feições) recuperados desnecessariamente do SGBD para o módulo de visualização durante a fase de restituição da cena tridimensional.', 
  resumo_en: '--'},

 {numero: 540, ano: 2004, dia: '06/08', autor: 'Kleyna Moore Almeida', orientador: [207], linha: 100, arquivo: '2004-Kleyna.pdf', 
  titulo: 'Garantia da Qualidade de Software no Desenvolvmento Baseado em Reuso: uma Abordagem no Contexto do Ministério da Defesa e de seus Comandos Subordinados', 
  resumo_pt: 'A garantia da qualidade de software no desenvolvimento baseado em reuso é um dos principais fatores para assegurar e controlar a qualidade do processo de desenvolvimento de software e dos artefatos gerados, visando posterior reutilização. Um Processo de Garantia de Qualidade de Software é definido por meio de um conjunto de atividades sistemáticas para assegurar a adequação ao reuso de artefatos, visando obter maior produtividade e melhoria da qualidade de um processo de desenvolvimento de software baseado em reuso de assets (artefatos criados com o propósito explícito de serem posteriormente reutilizados). Este processo promove o desenvolvimento de projetos e de assets de forma eficaz com base em aspectos qualitativos e quantitativos. Os indicadores quantitativos asseguram um melhor planejamento e controle de projetos e de assets, do reuso no desenvolvimento de aplicações, utilizando métricas e modelos de software, indicadores de qualidade, produtividade e reusabilidade. Este trabalho está baseado nas definições de padrões de processos mais utilizados atualmente pelas industrias e organizações, tais como, CMM, CMMI, ISO 9000, ISO 12207 e ISO 15504, além da Norma IEEE1517. Os processos estabelecidos neste trabalho abrangem diferentes níveis de maturidade do CMM, tais como, pontos de revisão e checklists nas diversas fases do processo de desenvolvimento de software (Nível 3 - Definido), e também a definição de um processo de medição (Nível 4 - Gerenciado). As verificações e revisões empregam técnicas de leitura baseada em perspectiva e técnicas de leitura horizontal e vertical respectivamente. A implantação do processo desenvolvido neste trabalho possui quatro fases: implantação de um processo de garantia de qualidade no processo de desenvolvimento de assets; definição dos modelos de qualidade dos artefatos, dos objetivos da avaliação e de indicadores; aplicação de técnicas de qualidade; e utilização dos indicadores para promover a melhoria do processo. Além disso, é apresentado um estudo de caso aplicando o processo de garantia de software estabelecido em um projeto piloto, Controlador Tático, desenvolvido com base no processo de Desenvolvimento de Software Baseado em Reuso no Contexto do Ministério da Defesa e de seus Comandos Subordinados de GURGEL (2004). Este estudo de caso aponta que a estratégia utilizada pode proporcionar a melhora da qualidade do modelo conceitual final assim como no processo de produção de software para reuso mais produtivo.', 
  resumo_en: '--'},

 {numero: 539, ano: 2004, dia: '18/06', autor: 'Silviane Gomes Rodrigues', orientador: [4], linha: 100, arquivo: '2004-Silviane.pdf', 
  titulo: 'Advice – um Ambiente Virtual Colaborativo para o Ensino a Distância', 
  resumo_pt: 'Os ambientes Virtuais Colaborativos (AVCs) estão evoluindo significativamente como um novo meio de acesso a informações e de comunicação que podem ser utilizados como uma opção dentre as outras tecnologias computacionais existentes para telecolaboração. Esses ambientes vêm crescendo principalmente nas áreas de treinamento e educação. Pode-se definir um AVC como um sistema de Realidade Virtual (RV) que utiliza o sistema de comunicação para prover suporte a atividades colaborativas, possibilitando usuários, mesmo que estejam separadas geograficamente, interagirem entre si e realizarem atividades colaborativas através do mundo virtual como se estivessem juntas no mundo real. O presente trabalho apresenta um Ambiente VIrtual Colaborativo para o Ensino a Distância - ADVICE. O protótipo aqui apresentado busca facilitar a colaboração entre alunos e professores geograficamente dispersos, utilizando a RV como uma ferramenta auxiliar na criação de interfaces amigáveis e interessantes para os usuários. Além da interface, foram estudadas também melhores formas de comunicação entre os integrantes do ambiente. Estudos sobre AVCs existentes na literatura são apresentados, bem como uma análise comparativa entre estes e ADVICE.', 
  resumo_en: '--'},

 {numero: 538, ano: 2004, dia: '18/05', autor: 'Fábio Jose Coutinho da Silva', orientador: [211], linha: 100, arquivo: '2004-Fabio.pdf', 
  titulo: 'Processamanto de Consultas sobre o Modelo de Dados ROSA', 
  resumo_pt: 'Sistemas de gerência de conteúdo de aprendizagem (Learning Content Management Systems - LCMS) exercem um importante papel no desenvolvimento do Ensino a Distância (EAD). ROSA (Repository of Objects with Semantic Access) constitui um LCMS que disponibiliza a criação, armazenamento, reutilização e gerência de objetos de aprendizagem (Learning Object - LO). Em ROSA, a estrutura de um curso e seus itens de conteúdo são organizados conforme um Mapa Conceitual. Este último exprime os relacionamentos semânticos definidos entre os LOs, através de associações rotuladas, tais como: pré-requisito, é_base_para, fundamenta, etc. Este trabalho apresenta o modelo de dados ROSA, e tem como foco o processamento de consultas sobre esse modelo. Isso inclui a especificação de uma álgebra de consulta, a Álgebra ROSA, com a definição de operações sobre os metadados dos LOs e operações de navegação, através de seus relacionamentos. Também é definido um conjunto de regras de equivalência entre essas operações, e por fim, a implementação de uma máquina de execução de consultas que executa os operadores da álgebra ROSA. O trabalho sugere, ainda que direcionado para o domínio de ensino à distância, um formalismo e sua implementação através de técnicas de banco de dados, como contribuição à área de Web Semântica.', 
  resumo_en: '--'},

 {numero: 537, ano: 2004, dia: '06/05', autor: 'Adriana Pereira Fernandez', orientador: [206], linha: 100, arquivo: '2004-Adriana.pdf', 
  titulo: 'Representação e Acesso a Objetos de Aprendizagem no Sistema ROSA: uma Abordagem em Topic MAPs', 
  resumo_pt: 'Muitos sistemas para e-learning estão surgindo nos dias de hoje, devido não somente às metodologias pedagógicas aplicadas, mas também pela evolução da Web e às tecnologias aí contidas. Sabe-se que a Web é um imenso repositório de informações que caracterizado por ser um ambiente heterogêneo, autônomo e distribuído, traz uma gama de problemas já conhecidos, os quais se propagam aos ambientes nela inseridos. Assim, domínios de conhecimento, tais como e-learning, necessitam ter seu universo organizado e representado, pois estes são alguns dos principais fatores que contribuem para a qualidade e estrutura do conteúdo, essenciais à sua recuperação. Este trabalho apresenta um sistema para e-learning denominado ROSA (Repository of Objects with Semantic Access for e-Learning), que disponibiliza a criação, armazenamento, reutilização e gerência de objetos de aprendizagem (OA). Um OA é uma coleção de material reutilizável utilizado para dar suporte a e-learning, i.e., uma entidade digital ou não, podendo ser usada para ensino, educação ou treinamento. O sistema ROSA representa o principal foco de estudo, a partir do qual são investigados os benefícios do formalismo Topic Maps (TM) para expressar um mapa de objetos de aprendizagem. Topic Maps são vistos como um paradigma para aprimorar o acesso à informação e a organização do conteúdo, o que motiva a usufruir e sua forma representacional. De modo a validar tais benefícios, foi realizada uma análise comparativa entre os modelos TM e ROSA, onde os construtores de um modelo foram mapeados em termos do outro. Isto permitiu a construção de alguns módulos destinados a utilizar a representação de TM para o ambiente em questão. O ambiente foi também enriquecido com a criação e visualização de mapas, também oriundos de TM, o que foi realizado por meio de uma interface gráfica.', 
  resumo_en: '--'},

 {numero: 536, ano: 2004, dia: '05/03', autor: 'Jorge de Albuquerque Lambert', orientador: [5], linha: 100, arquivo: '2004-Jorge.pdf', 
  titulo: 'Cifrador Simétrico de Blocos: Projeto e Avaliação', 
  resumo_pt: 'Neste trabalho é apresentado um estudo dos processos de seleção de padrões criptográficos realizados pelo Ministério do Comércio Norte Americano para os padrões de criptografia Data Encryption Standard (DES) e Advanced Encryption Standard (AES). São estudadas as exigências feitas aos algoritmos candidatos, bem como os aspectos positivos e negativos de três destes algoritmos: o DES, o AES (Rijndael) e o Serpent; Com base nos estudos supracitados, é apresentada uma filosofia para projeto de cifradores de blocos traduzida em cinco princípios básicos: segurança, eficiência, simplicidade, flexibilidade, credibilidade. É apresentada uma descrição do Rijndael baseada apenas em operações lógicas, de modo que o algoritmo pode ser estudado e implementado de maneira mais simples, sem a necessidade explícita da álgebra de polinômios em corpos finitos. É apresentada uma técnica para avaliação de transformações lineares sob a ótica do efeito avalanche – as matrizes de avalanche. É apresentada uma técnica para construção de transformações lineares para cifradores de blocos capaz de produzir a completude da função criptográfica em um número de iterações menor que o necessário para os algoritmos DES, Rijndael e Serpent. É apresentada uma técnica para construção e utilização de tabelas de substituição em cifradores tipo Feistel capaz de produzir boa resistência contra as criptoanálises diferencial, linear e criptoanálise por entradas invariantes. É apresentado o cifrador Alpha, um cifrador de blocos de 128 bites tipo Feistel modificado construído dentro da filosofia de projeto apresentada. O cifrador Alpha utiliza chaves a partir de 128 bites. São construídas as matrizes de correlação linear e matrizes de distribuição de diferenças para as tabelas de substituição apresentadas, com o objetivo de mostrar a sua resistência contra as criptoanálises linear e diferencial. São executados no cifrador Alpha os principais testes realizados pelo National Institute of Standards and Technology (NIST) e pelo New European Schemes for Signatures, Integrity and Encryption (NESSIE) em seus processos de seleção de cifras de blocos.', 
  resumo_en: '--'},

 {numero: 535, ano: 2004, dia: '19/02', autor: 'Wallace Anacleto Pinheiro', orientador: [206], linha: 100, arquivo: '2004-Wallace.pdf', 
  titulo: 'Busca em Portais Semânticos: uma Abordagem Baseada em Ontologias', 
  resumo_pt: 'Os portais fornecem pontos de acesso a informações personalizadas de seus usuários e têm se popularizado nos últimos anos na internet. Os portais semânticos surgiram como uma extensão natural dos portais e têm atraído a atenção de pesquisadores e empresas como uma forma inovadora de fornecer informações aos seus usuários. As ontologias são a base dos portais semânticos e uma de suas funções precípuas tem sido categorizar as informações contidas nos mesmos. Porém os usuários ainda sentem falta de aplicações práticas que demonstrem o uso das ontologias. Este trabalho perpassa por todos estes temas e demonstra como as ontologias podem contribuir de forma prática na solução de alguns dos problemas atuais, através de um dos componentes mais importantes do portal, o componente busca. Neste trabalho é criado um ambiente de um portal semântico, denominado PASS (Portal with Access to Semantic Search), dando-se enfoque a sua ferramenta de busca. Esse ambiente permite a criação, edição e armazenamento de ontologias. A ferramenta de busca utiliza os componentes das ontologias de domínio específico desse ambiente para realizar o processo de expansão dos termos de busca digitados pelo seu usuário. Na etapa final, essa ferramenta é avaliada, através da comparação com outras ferramentas de busca existentes na Web.', 
  resumo_en: '--'},

 {numero: 534, ano: 2004, dia: '16/02', autor: 'Celso Sooma Sasaqui', orientador: [212], linha: 100, arquivo: '2004-Celso.pdf', 
  titulo: 'Utilização Sequencial de Projeto de Experimentos e Inferência Bayesiana na Previsão da Demanda', 
  resumo_pt: 'A acirrada competição pelos mercados leva as organizações à exigência de redução dos seus custos, principalmente a partir da redução de seus estoques. Para isso é importante que existam formas eficientes de prever a demanda. Sendo assim, este trabalho parte da suposição que o conhecimento do comportamento do mercado torna a previsão da demanda mais eficiente. Esse trabalho propõe uma forma de previsão que combina o modelo causal com o qualitativo. Essa combinação se faz utilizando o Projeto de Experimentos para caracterizar a demanda, baseando-se nas informações dos especialistas, e estimar uma função de regressão que é utilizada como função de previsão da demanda. As estimativas dos coeficientes de regressão podem ter resultados com níveis de significância elevados e conseqüentemente um intervalo de confiança muito grande. Então existe a necessidade de melhorar as estimativas dos coeficientes para reduzir os intervalos de confiança. Para isso utiliza-se a Inferência Bayesiana para estimar os coeficientes de regressão usando as estimativas do Projeto de Experimentos como prioris.', 
  resumo_en: '--'},

 {numero: 533, ano: 2004, dia: '10/02', autor: 'Marcelo Gurgel de Souza', orientador: [207], linha: 100, arquivo: '2004-Marcelo.pdf', 
  titulo: 'Estudo do Desenvimento de Software Baseado em Reuso no Contexto do Ministério da Defesa e de seus Comandos Subordinados', 
  resumo_pt: 'O desenvolvimento de software baseado em reuso representa uma solução estratégica para organizações que façam uso ou desenvolvam um grande número de sistemas de software devido, principalmente, à redução dos custos a longo prazo e à melhoria da qualidade dos sistemas desenvolvidos. No entanto, apesar de já ser um conceito maduro e empregado em muitas grandes empresas, a sua implantação passa por alguns pontos chaves, tais como: a seleção da tecnologia de reuso a ser empregada, a necessidade de definição de um ciclo de vida do software baseado em reuso; a especificação de uma arquitetura de domínio que sirva de base para o desenvolvimento e montagem dos aplicativos; e, finalmente, a criação de um Programa de Reuso que traduza uma solução gerencial e organizacional de implantação das mudanças que se façam necessárias e na definição de pontos de controle de forma a gerenciar os riscos envolvidos neste processo. Este trabalho trata destes pontos chaves no contexto específico de uma organização hierarquizada de grande porte com núcleos de desenvolvimento descentralizados como o Ministério da Defesa.', 
  resumo_en: '--'},

 {numero: 532, ano: 2003, dia: '15/09', autor: 'Luiz Roberto Martins Bastos', orientador: [212], linha: 100, arquivo: '2003-Luiz.pdf', 
  titulo: 'Definição de Atributos Normativos para Avaliação de um Sistema de Educação a Distância', 
  resumo_pt: 'O crescente envolvimento da sociedade com o computador, a expansão da Web e a consequente globalização da informação levam a um desenvolvimento maior na educação a distância, evidenciada pela vasta literatura existente. Um exame dessa literatura revela que, até o momento, poucos esforços foram realizados para definir parâmetros que permitam medir o desempenho de sistemas de educação a distância (SEAD). O objetivo desta pesquisa é descrever sobre as fases do estudo para definir atributos normativos orientados para a avaliação de SEAD; detalha a evolução das atividades representadas por um modelo de arquitetura de sistema multiagente, onde cada agente recebe atribuições que são desempenhadas visando a estabilidade e funcionalidade do SEAD. Esta abordagem leva à identificação de diversos atributos observáveis identificados no corpo do conhecimento, definido como um conjunto de atributos normativos. Em seguida, para demonstrar como os atributos normativos podem contribuir para o aperfeiçoamento de um SEAD, estes são avaliados por especialistas por meio de questionário com emprego da técnica Delphi. O resultado é o levantamento de pontos fortes e oportunidades de melhoria, bem como a identificação dos modelos que melhor contêm os atributos normativos.', 
  resumo_en: '--'},

 {numero: 531, ano: 2003, dia: '11/09', autor: 'Jovania Menezes Dias', orientador: [7], linha: 100, arquivo: '2003-Jovania.pdf', 
  titulo: 'MApA: um Mecanismo de Avaliação da Aprendizagem do Aluno', 
  resumo_pt: 'Este trabalho apresenta uma pesquisa bibliográfica sobre abordagens de aprendizagem, abordagens de avaliação, e instrumentos que os ambientes de educação a distancia (EAD) na Web utilizam na avaliação da aprendizagem do aluno. A partir deste levantamento identificou-se características que devem ser observadas em ambientes EAD. Este trabalho visa a implementação de uma ferramenta para avaliação da aprendizagem do aluno em cursos a distancia baseados na Web, denominada MApA: Um Mecanismo de Avaliação da Aprendizagem do Aluno, que contém diferentes ferramentas que podem ser utilizadas em diversas abordagens de avaliação, satisfazendo tanto a abordagem tradicional quanto às novas tendências avaliativas que enfocam individualização da avaliação e negociação.', 
  resumo_en: '--'},

 {numero: 530, ano: 2003, dia: '28/08', autor: 'Abílio Fernades', orientador: [206], linha: 100, arquivo: '2003-Abilio.pdf', 
  titulo: 'Organização, Compartilhamento e Consultas a Objetos de Conhecimento Baseado em Ontoligias', 
  resumo_pt: 'É fato dizer que atualmente a Web é o maior repositório de dados de que se tem conhecimento e que ela está prestes a sofrer uma grande revolução. A visão da Web Semântica se apresenta como a principal proposta para melhorar a organização, exploração e intercâmbio de dados nesse ambiente complexo e heterogêneo. Por outro lado, as pessoas lidam diariamente com uma quantidade cada vez maior de informações. O conteúdo dessas informações se materializa nas máquinas dos usuários de computadores na forma de arquivos e links, que passam a constituir uma base de objetos manipuláveis de interesse desses indivíduos. Gerenciar tal base é importante não só para o uso pessoal, mas também para sua utilização na interação com outros indivíduos. Facilitar a organização, localização e intercâmbio eficiente de dados, quer seja na Web ou nas comunidades e grupos virtuais nos quais as pessoas se associam e interagem, constitui um desafio. O presente trabalho descreve o papel das ontologias e conceitos associados à Web Semântica como elemento de utilidade para organização e intercâmbio de dados. Apresenta uma proposta de organização dos arquivos e links que as pessoas utilizam em suas máquinas, numa abordagem baseada no uso de metadados. Enquadrando-se numa proposta colaborativa, auxilia o intercâmbio de conhecimento, implícito nesses objetos, sugerindo o reaproveitamento dos esforços individuais de descrição. Os metadados gerados são compartilhados numa infraestrutura comum construída para um determinado domínio de conhecimento, onde o intercâmbio se dá com a aplicação de técnicas de integração de ontologias. Como forma de validar essas propostas é apresentado o protótipo de uma ferramenta que procura simplificar o acesso do usuário comum a esses novos conceitos, que atualmente ainda são restritos aos pesquisadores e especialistas do assunto.', 
  resumo_en: '--'},

 {numero: 529, ano: 2003, dia: '01/08', autor: 'Iuri Locatelli Vieira', orientador: [3], linha: 100, arquivo: '2003-Iuri.pdf', 
  titulo: 'Grafos Periplanares Biconexos e Dual Geométrico Fraco', 
  resumo_pt: 'Nesta dissertação é estudada a classe de grafos periplanares biconexos e alguns problemas restritos a esta classe de grafos. Na literatura existente, os problemas de triangulação de peso mínimo e de base de ciclos minimal de grafos periplanares biconexos e o problema de representação planar de grafos periplanares maximais são resolvidos utilizando o dual geométrico fraco. Propomos nesta dissertação um algoritmo polinomial para determinar o dual geométrico fraco de um grafo periplanar biconexo. Posteriormente, utilizando o conceito de in-tree relaxada, é proposto um algoritmo polinomial para obter representações planares de um grafo periplanar biconexo, estendendo o algoritmo existente na literatura. Por último, são apresentadas propostas de implementação para todos os algoritmos inclusos nesta dissertação.', 
  resumo_en: '--'},

 {numero: 528, ano: 2003, dia: '10/07', autor: 'Marcelo Soares Loufti', orientador: [1], linha: 100, arquivo: '2003-Marcelo.pdf', 
  titulo: 'Desempenho da Linguagem Java em Dispositivos Embutidos', 
  resumo_pt: 'Este trabalho investiga as características da linguagem Java que degradam a performance de aplicações Java, especialmente em dispositivos embutidos. O estudo está focado em algumas propostas de máquinas virtuais para dispositivos embutidos: SimpleRTJ, KVM, Waba, SuperWaba e Spotless. Diversos dados comparativos são apresentados com o objetivo de confrontar as JVMs quanto à arquitetura, poder de abstração e performance. Os dados apresentados auxiliam na conduta do desenvolvimento de aplicações para estes dispositivos e no entendimento dos eventuais gargalos de desempenho de uma aplicação.  Finalmente, a dissertação apresenta possibilidades de implementar técnicas de otimização de código inexploradas na linguagem Java, diante a ausência de certas abstrações que impedem que tais otimizações sejam implementadas.', 
  resumo_en: '--'},

 {numero: 527, ano: 2003, dia: '07/07', autor: 'Daniel Scofield Saraiva Nogueira', orientador: [213], linha: 100, arquivo: '2003-Daniel.pdf', 
  titulo: 'Assistência Inteligente ao Planejamento de Ações em KDD', 
  resumo_pt: 'Essa dissertação tem como objeto de estudo o problema da Assistência Inteligente ao Planejamento de Ações de KDD (Knowledge Discovery in DataBases). Os objetivos compreendem a pesquisa, definição, modelagem, implementação e testes do módulo de Assistência ao Planejamento das Ações de KDD que auxiliará usuários a executar tarefas de KDD. De uma forma geral, um processo de KDD envolve pré-processamento de dados, escolha de um algoritmo de mineração de dados, aplicação deste algoritmo e pósprocessamento do resultado da mineração. Existem diversas possibilidades de escolha dentre as inúmeras ferramentas que podem ser aplicadas em cada uma dessas etapas e iterações não triviais entre elas. O encadeamento dessas ferramentas constitui a construção de um Plano de Ação. Tanto os usuários novatos quanto os especialistas em Mineração de Dados necessitam de assistência ao navegarem através do espaço de possibilidades do processo KDD. Nesse contexto, surge o conceito de Assistentes Inteligentes de Descoberta (AID). O trabalho foi desenvolvido de maneira a apresentar inicialmente conceitos, exemplos de aplicações e uma taxonomia das atividades realizadas na área de KDD. Por conseguinte, apresentação dos trabalhos mais influentes realizados sobre Seleção de Algoritmos e construção e ordenação de Planos de Ação. Outro ponto importante desse trabalho é o estudo de técnicas de Inteligência Artificial onde são apresentadas a Teoria de Planejamento (Planning) e a Técnica de Meta-aprendizado para solução do problema apresentado. Neste ponto, a interface do protótipo implementado em Delphi é apresentada. Por fim, são realizados testes e avaliações através de experimentos estatísticos que comprovam a eficiência da abordagem proposta: seleção e ordenação de algoritmos de mineração e construção dos planos de ação a partir desse conjunto ordenado de algoritmos.', 
  resumo_en: '--'},

 {numero: 526, ano: 2003, dia: '01/06', autor: 'Aurisan Souza de Santana', orientador: [206], linha: 100, arquivo: '2003-Aurisan.pdf', 
  titulo: 'Uso de Metadados no Suporte à Transformação e Linhagem de Dados em Ambientes de Warehousing', 
  resumo_pt: 'Sistemas de Data Warehouse são uma prática adotada no âmbito corporativo em função de tais sistemas responderem a questões estratégicas de negócio, que visam a tomada de decisão. Neste contexto, a gerência de metadados torna-se um fator crucial para o sucesso de projetos de Data Warehouse. Sua implementação, quando realizada no âmbito de um padrão de metadados, viabiliza a interoperabilidade de dados e metadados neste ambiente, caracterizado sobretudo pela forte heterogeneidade de fontes de dados. A implementação de mecanismos de linhagem de dados e metadados melhora a qualidade na geração da informação, pois é possível conhecer todo o processo de extração, transformação e carga dos dados. Este trabalho apresenta o conceito de linhagem de dados, formaliza o processo de linhagem de metadados propondo para isso algoritmos de linhagem de metadados e linhagem de dados para hipercubos, implementados sob um esquema estrela. Todo o trabalho é realizado no contexto de um padrão de metadados, o Common Warehouse Metamodel (CWM). De modo a consolidar este estudo foi desenvolvida uma ferramenta que implementa tais funcionalidades.', 
  resumo_en: '--'},

 {numero: 525, ano: 2003, dia: '21/05', autor: 'André Accioly Vieira', orientador: [214], linha: 100, arquivo: '2003-Andre.pdf', 
  titulo: 'Ontoextract: uma Ferramenta para Extração de Ontologias a Partir de Banco de Dados Relacionais', 
  resumo_pt: 'A Web Semântica, como enunciada na visão de Tim Berners-Lee, almeja desenvolver linguagens para expressar a informação de maneira a ser entendida por máquinas. Mecanismos de busca são pobres quando se trata de fazer inferências complexas e correlacionar assuntos aparentemente disjuntos. O desenvolvimento de ontologias visa prover o mecanismo que irá construir a parte semântica da Web Semântica. Entretanto, o desenvolvimento de ontologias é reconhecidamente complexo e consumidor de recursos. Ontologias e esquemas de bancos de dados são intimamente correlacionados e, na verdade, a diferença entre uns e outros está no propósito de sua utilização. Esquemas de bancos de dados são um manancial de informações, passíveis de serem transformadas em ontologias. Para tal, é necessário o desenvolvimento de ferramentas que possibilitem transformar esquemas tradicionais de bancos de dados em ontologias, reduzindo o custo e os recursos empregados para seu desenvolvimento. Este trabalho descreve o papel das ontologias na construção da Web Semântica e a oportunidade da construção de uma ferramenta para extração de ontologias a partir de bancos de dados relacionais, comparando, ao longo de seu desenvolvimento, diferentes linguagens de ontologias. Ao final, é apresentado um protótipo da ferramenta de extração, e sua aplicação a um estudo de caso, como prova de princípio.', 
  resumo_en: '--'},

 {numero: 524, ano: 2003, dia: '06/05', autor: 'Wagner Pinto Izzo', orientador: [215], linha: 100, arquivo: '2003-Wagner.pdf', 
  titulo: 'O Processo de Engenharia de Sistemas e sua Aplicação à Gestão do Desenvolvimento de Sistemas de Ensino a Distância', 
  resumo_pt: 'Nossa sociedade se encontra perplexa em face à influência crescente da informática no campo educacional. Uma grande variedade de cursos à distância é oferecida. Há até mesmo a idéia que desenvolver um curso na modalidade à distância é algo fácil: é só uma questão de estruturar um determinado conjunto de assuntos e construir um portal na Internet, habilitando algum tipo de interação amável entre estudantes e professores, por exemplo, e isso é tudo. Programas de Ensino à Distância (EAD) não são tão simples, pelo contrário, eles são bastante complexos. Esta complexidade vem, em primeiro lugar, com os conceitos embutidos no modelo de ensino/aprendizagem. A própria evolução da Tecnologia da Informação (TI) impõe uma avaliação constante dos requisitos de sistema e qualificação pessoal, com conseqüências em diversas áreas, tais como: estrutura organizacional, econômica, acadêmica, técnica e outras. Além disso, dificuldades associadas com a diversidade e quantidade de usuários do sistema (professores, estudantes, profissionais de pedagogia, coordenadores, administradores, etc) torna a atividade de desenvolver um sistema de EAD em um desafio administrativo enorme. Este presente trabalho mostra porque é necessário que o desenvolvimento de sistemas de EAD deva seguir um processo bem definido, estruturado e integrado de gestão, e também, porque o processo de Engenharia de Sistemas é adequado, ressaltando a importância da fase conceitual para a obtenção de uma administração apropriada do ciclo de vida do sistema, como um todo. Para tanto, relaciona os principais conceitos envolvidos na Engenharia de Sistemas, no Ensino à Distância e na aplicação do processo de Engenharia de Sistemas em sistemas de EAD. Um estudo de caso apresenta esta aplicação a um sistema de EAD de interesse do Exército Brasileiro. Por sua vez, a conclusão demonstra a complexidade dos sistemas de EAD atuais, respondendo às perguntas apresentadas no capítulo 1, confirmando a hipótese de que a aplicação da Engenharia de Sistemas é adequada à administração do desenvolvimento de sistemas de EAD e, identificando as adaptações necessárias ao processo para seu emprego específico neste tipo de sistema.', 
  resumo_en: '--'},

 {numero: 523, ano: 2003, dia: '15/04', autor: 'Flávio Augusto Coutinho Correia', orientador: [9], linha: 100, arquivo: '2003-Flavio.pdf', 
  titulo: 'Sistema de Tele-Operação para um Manipulador Mestre-Escravo 3 GDL com Realimentação Háptica', 
  resumo_pt: 'Há diversas situações nas quais a presença humana faz-se inadequada, ou mesmo, impraticável - seja pelo risco inerente envolvendo a sua integridade física, pelo custo elevado, pelo desconhecimento das reais necessidades para a manutenção da vida ou, ainda, por meras restrições de espaço para a execução apropriada de uma dada tarefa. Os sistemas robóticos de tele-presença representam uma promissora alternativa para auxiliar o homem nestas atividades. O objetivo principal deste trabalho é a construção de um simulador para um sistema de tele-operação para um manipulador mestre-escravo com 3(três) graus-de-liberdade (GDL) com realimentação háptica, o qual denominamos pelo seu acrônimo STMER . Este sistema é caracterizado por proporcionar ao usuário, a realimentação háptica proveniente de um sistema robótico escravo associado, que deve estar sincronizado com um sistema robótico mestre, com o qual o usuário estará interagindo. O conceito de percepção háptica relaciona-se com a habilidade de sensação de tato e força, semelhante à percepção dos dedos da mão de um ser humano. O sistema constitui-se principalmente de um protocolo de comunicação e dois subsistemas mestre e escravo, os quais são necessariamente idênticos. O protocolo de comunicação visa viabilizar a comunicação entre os subsistemas mestre e escravo, proporcionando ao usuário a possibilidade de receber a realimentação das sensações hápticas percebidas pelo manipulador do sub-sistema escravo. O sistema adota uma arquitetura que utiliza-se apenas de um cliente e um servidor, implementados com um algoritmo de sincronismo, a fim de proporcionar ao usuário o máximo possível de tempo-real. A arquitetura selecionada, dentre outras três elaboradas, foi denominada de CMSE; i.d., é constituída de um cliente mestre e um servidor escravo. Sua elaboração foi focada para atender à necessidade de comunicação do STMER em uma rede local; entretanto, da maneira como a arquitetura foi concebida, com apenas algumas alterações e ampliações da arquitetura, o sistema pode ser portado para outras redes. As características da troca de informação, através do protocolo e da arquitetura, baseiamse em dois fatores principais: (a) precisão e (b) tempo-real. Precisão no sincronismo entre o mestre e o escravo, e tempo-real para que uma tarefa, requisitada pelo usuário, não torne-se inviável. Mas estas características, a modularização do projeto e as inúmeras possibilidades dentro da área de tele-presença (que permitem que o projeto sirva de base para outras pesquisas) foram os principais desafios enfrentados neste trabalho.', 
  resumo_en: '--'},

 {numero: 522, ano: 2003, dia: '15/04', autor: 'Ricardo Eder', orientador: [9], linha: 100, arquivo: '2003-Ricardo.pdf', 
  titulo: 'Navegação em Ambientes Semiestruturados por um Sistema Robótico Autônomo em Tempo Real', 
  resumo_pt: 'Uma das áreas de pesquisa que mais tem crescido ultimamente é a área da robótica. Dentre as várias vertentes de pesquisa da área de robótica, uma que se destaca é a de sistemas robóticos autônomos. Um sistema robótico autônomo é formado por um conjunto de sistemas mecânicos, elétricos e, principalmente, computacionais, que unidos são capazes de realizar a tarefa de locomover um sistema robótico dentro de um espaço de configuração, sem a intervenção humana ou de qualquer outro sistema computacional. Um sistema robótico autônomo deve ser capaz de realizar o sensoreamento da área à sua volta, analisar os dados enviados ao seu sistema computacional e decidir qual é a melhor estratégia para conseguir completar a sua missão. Este trabalho apresenta um algoritmo de planejamento de trajetória que visa conduzir um sistema robótico, baseado na plataforma Khepera, através de um espaço de configuração dinâmico e desconhecido até a posição final desejada. A plataforma Khepera está se tornando cada vez mais um padrão nesse tipo de pesquisa, devido ao seu baixo custo e a sua facilidade de montagem e uso. Ela consiste num robô de pequenas dimensões (55mm de diâmetro e 30mm de altura) possuindo um conjunto de oito sensores de proximidade de infravermelho. Para realizar esta tarefa, o algoritmo de planejamento de trajetória foi baseado no método do campo potencial. Este método calcula a trajetória que o sistema robótico deve seguir através da resultante das forças que são aplicadas ao sistema robótico, com a seguinte distinção: os obstáculos são forças repulsivas e a posição final desejada é uma força atrativa. Um dos problemas deste método, o mínimo local, foi tratado realizando uma análise da vizinhança de cada uma das possíveis posições adjacentes à posição atual do sistema robótico. Este problema ocorre quando o algoritmo de planejamento de trajetória conduz o sistema robótico a um ponto, do qual o algoritmo não é mais capaz de movê-lo na direção da posição final desejada e, com isso, ficando num ciclo infinito. A utilização de um contador de visitas visa direcionar o sistema robótico para as áreas menos exploradas e, assim, impedir o algoritmo de entrar num ciclo, o que ocorre quando se encontra um ponto que é mínimo local. O algoritmo foi testado num simulador feito em C++ e os resultados estão no Capítulo 4. Os resultados foram satisfatórios e algumas sugestões de melhorias estão relacionadas no Capítulo 5.', 
  resumo_en: '--'},

 {numero: 521, ano: 2003, dia: '27/02', autor: 'Alexandre Laval Silva', orientador: [216], linha: 100, arquivo: '2003-Alexandre.pdf', 
  titulo: 'Modelo de Programação Linear em Dois Níveis para Otimização de Estoques de Sobressalentes', 
  resumo_pt: 'Em tempos de mercados competitivos, há uma busca por melhorias no desempenho operacional das empresas, aproveitando ao máximo os recursos disponíveis, de forma a minimizar os custos do processo logístico. Um aspecto fundamental da logística é a gestão de estoques de sobressalentes, cujo objetivo principal é conseguir um patamar satisfatório de nível de serviço com o mínimo custo. As cadeias de distribuição de sobressalentes normalmente estão escalonadas em vários níveis compostos por organizações distintas (depósitos e centros de manutenção ou assistência técnica). Na realidade, cada nível desta estrutura hierárquica tem objetivos próprios e controla algumas variáveis específicas, mas tem seu espaço de decisão total ou parcialmente determinado por outros níveis. Sendo assim, nem sempre as decisões ótimas de cada nível hierárquico conduzem ao ótimo global da cadeia de distribuição.Este trabalho propõe um Modelo de Programação Linear em Dois Níveis para Otimização de Estoques de Sobressalentes em uma cadeia de distribuição de sobressalentes constituída por centros de manutenção e um depósito central. Neste contexto, o primeiro nível de decisão (dos centros de manutenção) busca otimizar o número de pedidos atrasados e o desbalanceamento entre os níveis de serviço dos centros de manutenção e o segundo nível (o depósito central) busca minimizar os custos de transporte e de manutenção dos estoques na cadeia de distribuição de sobressalentes.', 
  resumo_en: '--'},

 {numero: 520, ano: 2003, dia: '27/02', autor: 'Ana Luiza Meyer Cardoso', orientador: [7], linha: 100, arquivo: '2003-Ana.pdf', 
  titulo: 'Desenvolvimento de um Modelo de Captura de Conteúdo para o Ensino a Distância Mediado Pedagogicamente', 
  resumo_pt: 'Um dos problemas mais graves da educação em nosso país consiste na existência de um discurso educativo não mediado pedagogicamente, tanto no contato entre professor e aluno, como no material didático utilizado. A Mediação Pedagógica é o tratamento de conteúdos e formas de expressão dos diferentes temas, a fim de tornar possível o ato educativo. Isso dentro do horizonte de uma educação concebida como participação, criatividade, expressividade e relacionamento (Gutierrez e Prieto, 1994). Gutierrez e Prieto (1994) estão convencidos do valor da mediação pedagógica para dar sentido à educação. A afirmação vale para todo o processo pedagógico, mas tem o seu maior grau de importância quando se trata de um Sistema de Ensino a Distância. No contato entre professor e aluno, a mediação pode surgir do trabalho na aula e depende quase sempre  da capacidade e da dedicação do docente. Num sistema a distância, os materiais encarnam esta dedicação. São eles que permitem ao estudante encontrar e concretizar o sentido do processo educativo (Gutierrez e Prieto, 1994). Assim, este trabalho se propõe a discutir os fundamentos teóricos existentes, relacionados ao ensino e ao aprendizado, apresentando um modelo conceitual considerado pelo autor como necessário à definição de um eficaz sistema de educação, principalmente quando for utilizado um sistema de Ensino à Distância.', 
  resumo_en: '--'},

 {numero: 519, ano: 2003, dia: '00/00', autor: 'Maximiliano Pinto Damas', orientador: [3], linha: 100, arquivo: '2003-Maximiliano.pdf', 
  titulo: 'Sobre a Solução Eficiente de Problemas em Grafos Utilizando Treewidth e Treewidth-Decomposition', 
  resumo_pt: 'Muitos problemas NP-difícies para grafos em geral podem ser resolvidos em tempo polinomial se restringirmos o trabalho a uma classe particular. Se o grafo apresenta estrutura de árvore (tree-like), isto em geral facilita a resolução de problemas. A tree-decomposition de um grafo G = (V,E) com largura k consiste em decompor os vértices do grafo em subconjuntos Xi contém V, 1 <= i <= p, sendo que cada um desses conjuntos representa um nó da árvore de decomposição e módulo(Xi) <= k, 1 <= i <= p. Se tivermos conhecimento da tree-decomposition de um grafo qualquer G, alguns problemas NP-difícies podem ser resolvidos de maneira eficiente utilizando a técnica de programação dinâmica. Entretanto, determinar a tree-decomposition de um grafo qualquer é um problema NPdifícil. Portanto, existem na literatura algoritmos aproximados e heurísticas para obter a treedecomposition. Outra abordagem possível é o estudo de classes particulares de grafos. Neste caso, os problemas NP-difícies restritos a estas classes particulares de grafos podem ser resolvidos em tempo polinomial. Existem situações práticas que podem ser modeladas como problemas em grafos onde é possível obter bons resultados utilizando a tree-decomposition. Dois exemplos desses problemas serão estudados nesta dissertação. O problema de solução eficiente de sistemas lineares simétricos esparsos de grande porte e o problema de alocação de registradores em programas estruturados. Mostraremos como utilizar a tree-decomposition do grafo associado à matriz de coeficientes, ainda que aproximada, facilita a solução do primeiro problema, e como o conceito de treewidth ajuda na otimização de códigos na compilação de programas escritos em linguagens estruturadas no segundo caso.', 
  resumo_en: '--'},

 {numero: 518, ano: 2002, dia: '30/07', autor: 'Glauco Fiorott Amorim', orientador: [217], linha: 100, arquivo: '2002-Glauco.pdf', 
  titulo: 'Análise de Desempenho de Protocolos de Roteamento com Diferenciação de Serviços em Redes de Comunicação Móvel Ad-hoc', 
  resumo_pt: 'Comunicações Celulares, Redes Locais Sem Fio e Serviços Via Satélite estão desfrutando de um rápido período de crescimento devido, principalmente, a dois fatores: o desenvolvimento de tecnologias que permitem uma ampla faixa de serviços e a necessidade dos usuários acessarem informações sem restrições de espaço e tempo. Essas tecnologias deram origem ao conceito de Redes Móveis. Neste contexto se destacam as Redes Móveis Ad Hoc devido à facilidade de instalação, baixo custo e variedade de aplicações. Ambas utilizam enlaces sem fio para proverem comunicação entre os terminais móveis que constituem estas redes. Pode-se utilizar ou não uma estrutura fixa. Um dos maiores desafios em Redes Ad Hoc está ligado à possibilidade de perda de comunicação, seja por interferências ou por mobilidade dos terminais. Portanto, a função de rotear mensagens é de fundamental importância e influencia diretamente o desempenho de toda a rede. Os protocolos de roteamento utilizam informações imprecisas sobre o estado dos enlaces da rede. Esta imprecisão pode afetar o rendimento do protocolo e conseqüentemente o desempenho da rede. Priorizar fluxos de pacotes de roteamento pode ser usado para possibilitar que os protocolos de roteamento utilizem informações mais recentes sobre as modificações ocorridas na rede. Outro aspecto importante e, da mesma forma, desafiador é fornecer Qualidade de Serviço (QoS) aos terminais móveis. Prover QoS em Redes Móveis é tão complexo quanto rotear mensagens, pois a mobilidade dos nós impõe várias restrições como baixa vazão, maior atraso entre a troca de mensagens, inexistência de uma entidade centralizadora etc. Para prover QoS, várias técnicas podem ser utilizadas, dentre elas Diferenciação de Serviços cuja aplicação tem crescido enormemente. Existem duas técnicas de Diferenciação de Serviços que destacam-se: Diferenciação de Serviços por variação do DIFS e Diferenciação de Serviços por Variação do Backoff. As técnicas de Diferenciação podem ser aplicadas para dar prioridades distintas a diferentes terminais ou a diferentes fluxos de mensagens. O objetivo dessa dissertação é investigar o impacto da utilização de Diferenciação de Serviços nos protocolos de roteamento. Para isso, foram usadas duas técnicas de diferenciação denominadas, Diferenciação de Serviços por Variação na Função de Backoff e Diferenciação de Serviços por Variação no DIFS aplicadas em dois protocolos de roteamento para redes Ad Hoc denominados Dynamic Source Routing Protocol – DSR e Destination-Sequenced Distance-Vector – DSDV. Os resultados mostram o desempenho dos protocolos originais e dos protocolos utilizando as técnicas de diferenciação. Além disso, é apresentada uma comparação entre os protocolos originais DSR e DSDV e uma comparação entre as técnicas de diferenciação empregadas.', 
  resumo_en: '--'},

 {numero: 517, ano: 2002, dia: '22/07', autor: 'Wagner Antonio Arbex', orientador: [218], linha: 100, arquivo: '2002-Wagner.pdf', 
  titulo: 'Aspectos de Visualização de Redes', 
  resumo_pt: 'Trabalhos na área de visualização e animação com o uso de ferramentas de informática começam a surgir no início dos anos 80, especificamente com trabalhos voltados para o ensino de informática. Com o passar dos anos são encontrados artigos, textos e ferramentas que envolvem visualização em engenharia de software e desenvolvimento de frameworks, análise, simulação e otimização de redes, tratamento e representação gráfica de grandes volumes de informações, de objetos abstratos e de relações entre objetos, projeto de web para sistemas distribuídos, validação de sistemas, ensino à distância, e ferramentas de auxílio à aprendizagem de algoritmos, grafos e linguagens de programação. O objeto desta dissertação é visualização de redes. Para tanto, encontra-se dividida em: organizar o material publicado sobre o tema; estudar a aplicabilidade dos aspectos relacionados à visualização em problemas reais modelados por redes; formalizar os procedimentos de visualização de redes e implementar o KitNet, um protótipo de ferramenta de visualização, ou melhor, um protótipo de software de visualização de redes.', 
  resumo_en: '--'},

 {numero: 516, ano: 2002, dia: '01/07', autor: 'Tânia Mara Lima da Fonseca', orientador: [206], linha: 100, arquivo: '', 
  titulo: 'Um Processo de Definição de Cenários Abstratos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 515, ano: 2002, dia: '01/07', autor: 'Sérgio Freitas Marreiros', orientador: [7], linha: 100, arquivo: '', 
  titulo: 'Arquitetura Baseada em Agentes Móveis para Elaboração de Simulados', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 514, ano: 2002, dia: '01/07', autor: 'Marcelo Buonocore Nunes', orientador: [219], linha: 100, arquivo: '', 
  titulo: 'Reconstrução da Estrutura Espacial Tridimensional de uma Cena a partir de Imagens Estéreo-Binoculares', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 513, ano: 2002, dia: '01/07', autor: 'Leonardo Malheiros Serrano de Cerqueira Leite', orientador: [9], linha: 100, arquivo: '', 
  titulo: 'Algoritmo para Controle do Sistema de Transmissão de um Automóvel: Inferência Direta sobre uma Massa de Dados', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 512, ano: 2002, dia: '01/07', autor: 'Juliana Kaercher', orientador: [7], linha: 100, arquivo: '', 
  titulo: 'Concepção de um Ambiente de Ensino à Distância na WEB', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 511, ano: 2002, dia: '01/07', autor: 'Claudio Nossar Paranhos Junior', orientador: [215], linha: 100, arquivo: '', 
  titulo: 'Parâmetros Básicos para Modelagem Conceitual de Sistemas de C4ISR: Uma Aplicação ao Processo de Engenharia de Sistemas', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 510, ano: 2002, dia: '27/06', autor: 'Myrna Cecília Martins dos Santos', orientador: [217], linha: 100, arquivo: '2002-Myrna.pdf', 
  titulo: 'Análise Formal de Protocolos de Autenticação para Redes Celulares', 
  resumo_pt: 'O crescimento extraordinário que tem ocorrido nesta década nas áreas de comunicação celular, redes locais sem fio e serviços via satélite permitirá que informações e recursos possam ser acessados e utilizados em qualquer lugar e em qualquer momento. Por isso, a preocupação com a segurança das informações torna-se cada vez mais importante. O livre acesso aos meios sem fio, expõe o conteúdo da comunicação sobre os enlaces entre uma unidade móvel e a rede cabeada ou mesmo entre unidades móveis, dando ao intruso a oportunidade de se passar por um assinante legítimo e obter informações sigilosas, como por exemplo, senhas e documentos importantes. Para garantir a segurança das informações vários protocolos foram desenvolvidos. Esses protocolos utilizam técnicas de criptografia para fornecerem: o sigilo do conteúdo das mensagens e da identidade do usuário/entidade, a autenticação dos participantes da comunicação, a integridade dos dados e o nãorepúdio.  O problema é que esses protocolos criptográficos estão sujeitos a erros no desenvolvimento, erros estes que podem ocorrer em qualquer uma das fases de seu projeto e, conseqüentemente, tornando-os vulneráveis a ataques. Foram criados então, métodos formais que têm a função de analisar e verificar se os objetivos propostos pelos autores dos protocolos foram alcançados. Este trabalho tem como finalidade mostrar a importância do emprego de métodos formais nos protocolos criptográficos. Para isso, foi realizada a análise de três protocolos de autenticação para o ambiente celular: GSM, CDPD e UMTS, utilizando uma das categorias de métodos formais, denominada lógica BAN.', 
  resumo_en: '--'},

 {numero: 509, ano: 2002, dia: '01/06', autor: 'Haroldo Lima Benício', orientador: [206], linha: 100, arquivo: '', 
  titulo: 'TOOHELP: um Ambiente para Aprendiagem de Tecnologias de Orientação a Objetos, Integrável a Plataformas de Educação a Distância', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 508, ano: 2002, dia: '01/06', autor: 'Alexandre Fitzner do Nascimento', orientador: [9], linha: 100, arquivo: '', 
  titulo: 'Sistema Dinâmico de Automação Residencial', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 507, ano: 2002, dia: '17/05', autor: 'Talles Marcelo Gonçalves de Andrade Barbosa', orientador: [7], linha: 100, arquivo: '2002-Talles.pdf', 
  titulo: 'QoSProxyServer: uma Abordagem de QoS para o Ensino a Distância', 
  resumo_pt: 'De forma geral, as aplicações desenvolvidas para Ensino a Distância via Web não estão habilitadas a operar com os diferentes níveis de necessidades de seus usuários sendo que, convencionalmente, não implementam nenhum mecanismo capaz de prover diferentes níveis de QoS (Quality of Service) ao nível da aplicação a fim de melhorar o desempenho do serviço oferecido para as conexões de usuários prioritários. Nesta dissertação, nós propusemos, implementamos e avaliamos um novo modelo para definir e distribuir diferentes prioridades às diferentes conexões de usuários, explorando características da tecnologia Java. Assim, será mostrado que é possível obter melhor aproveitamento dos recursos, melhorando o serviço prestado e sem a necessidade de intervenção na estrutura interna do sistema operacional subjacente.', 
  resumo_en: '--'},

 {numero: 506, ano: 2002, dia: '08/05', autor: 'Eugênio da Silva', orientador: [220], linha: 100, arquivo: '2002-Eugenio.pdf', 
  titulo: 'Reconhecimento Inteligente de Caracteres Manuscritos', 
  resumo_pt: 'Esta dissertação aborda o desenvolvimento de um protótipo de um modelo computacional, baseado em redes neurais, para o reconhecimento de caracteres manuscritos. Tal modelo deve fazer parte de um sistema mais abrangente que visa o processamento automático de formulários de concurso. O processo de desenvolvimento envolve a avaliação de alguns métodos de extração de características e também a concepção e implementação de várias estratégias para o reconhecimento tanto de dígitos quanto de letras manuscritos. Algumas das estratégias implicaram na utilização de mais de um método de extração de características e mais de uma rede neural, onde cada uma se especializou no reconhecimento de determinados caracteres. Os resultados de cada estratégia experimentada são avaliados individualmente e em seguida são comparados entre si. No caso das letras, é realizada também uma análise relativa dos resultados onde o desempenho no reconhecimento de cada letra é ponderado pela quantidade de vezes que aquela letra aparece nas palavras da língua portuguesa. Finalmente é realizada a avaliação do desempenho alcançado no reconhecimento de cada campo do formulário e também no reconhecimento de um formulário completo.', 
  resumo_en: '--'},

 {numero: 505, ano: 2002, dia: '01/05', autor: 'Verônica Aguiar da Silva', orientador: [206], linha: 100, arquivo: '', 
  titulo: 'Ferramenta de Consulta a Mapas de Sitios na Web', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 504, ano: 2002, dia: '01/05', autor: 'Sylvio Jorge de Souza Junior', orientador: [206], linha: 100, arquivo: '', 
  titulo: 'Framework para Sistemas de Comando e Controle: Definição a Partir do Domínio de Defesa Civil', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 503, ano: 2002, dia: '01/05', autor: 'Letícia Maria Gonçalves Furtado', orientador: [206], linha: 100, arquivo: '', 
  titulo: 'Tecnologia de Mediação para Transformação de Dados em Ambientes de Data Warehouse', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 502, ano: 2002, dia: '01/04', autor: 'Rodolfo Romualdo da Silva', orientador: [212], linha: 100, arquivo: '', 
  titulo: 'Análise Bayesiana da Confiabilidade de Itens Submetidos a Testes de Degradação', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 501, ano: 2002, dia: '01/04', autor: 'Jose Antonio de Sousa Fernandes', orientador: [9], linha: 100, arquivo: '', 
  titulo: 'Manipulador Virtual Teleoperado: Transmissão da Percepção Háptica à Distância', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 500, ano: 2002, dia: '01/03', autor: 'André Luiz Avelino Sobral', orientador: [7], linha: 100, arquivo: '', 
  titulo: 'Emprego de Agentes Móveis em Roteamento na Arquitetura TCP/IP', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 499, ano: 2001, dia: '01/12', autor: 'Thiago de Souza Rodrigues', orientador: [3], linha: 100, arquivo: '', 
  titulo: 'A st-numeração e suas Aplicações', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 498, ano: 2001, dia: '01/12', autor: 'Rodolfo Cardoso', orientador: [216], linha: 100, arquivo: '', 
  titulo: 'Impacto das Práticas-Chave de Melhoria da Gestão no Desempenho Organizacional: uma Metodologia de Avaliação', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 497, ano: 2001, dia: '01/12', autor: 'Paulo Renato da Costa Pereira', orientador: [9], linha: 100, arquivo: '', 
  titulo: 'Cooperação entre Robôs para Reconhecimento de Ambientes Desestruturados', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 496, ano: 2001, dia: '01/12', autor: 'Pauli Adriano de Almada Garcia', orientador: [212], linha: 100, arquivo: '', 
  titulo: 'Aplicação de Análise Envoltória de Dados (DEA) no processo de manutenção centrada em confiabilidade de RCM', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 495, ano: 2001, dia: '01/12', autor: 'Gilberto Ferreira da Silva', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Representação do Léxico para Reconhecimento da Similaridade de Palavras no Português', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 494, ano: 2001, dia: '01/12', autor: 'Glauter Fonseca Jannuzzi', orientador: [208], linha: 100, arquivo: '2001-Glauter.pdf', 
  titulo: 'Sistema Cliente para Comércio Eletrônico Business-to-Consumer Baseado em Agentes', 
  resumo_pt: 'O comércio eletrônico é uma das áreas da computação de maior crescimento nos dias de hoje. Vários investimentos estão sendo feitos e a cada dia surgem novas empresas voltadas para o mundo virtual. Essa realidade só é possível devido à tecnologia fornecida pela Internet, através de seus serviços mais utilizados, como a World Wide Web e o correio eletrônico. Com a grande popularidade da Web, o número de pessoas que adquirem computadores e se conectam à grande rede mundial cresce exponencialmente. Desta forma, o número de consumidores em potencial também aumenta e desperta um grande interesse de todos. A comodidade é o principal fator para incentivar usuários a realizarem negócios pela Internet. Nesta dissertação, foram estudados os conceitos envolvidos no comércio eletrônico business-to-business e business-to-consumer, além das técnicas utilizadas nos sistemas de recuperação de informação, sites de venda e sistemas de comparação de produtos na Web. O sistema proposto nesta dissertação tem como objetivo incentivar o comércio eletrônico business-to-consumer através do oferecimento de uma ferramenta de busca e comparação de produtos e sites de venda disponíveis na Web. O sistema implementado utiliza conceitos de recuperação da informação e de agentes de comércio eletrônico.', 
  resumo_en: '--'},

 {numero: 493, ano: 2001, dia: '01/11', autor: 'Carlos Eduardo Costa Vieira', orientador: [221], linha: 100, arquivo: '', 
  titulo: 'Metaheurísticas Aplicadas ao Problema de Alocação de Canal em Sistemas de Telecomunicações Móveis', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 492, ano: 2001, dia: '01/08', autor: 'Andrés Abelardo Level Montesinos', orientador: [221], linha: 100, arquivo: '', 
  titulo: 'Esquema de Gerenciamento de Chaves para Grupos Multicast com Estrutura Hierárquica', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 491, ano: 2001, dia: '28/05', autor: 'Genelice Paiva da Costa Pereira', orientador: [206], linha: 100, arquivo: '2001-Genelice.pdf', 
  titulo: 'Sistema de Gerenciamento de Documentos Eletrônicos e Coleções na Web', 
  resumo_pt: 'A Web é hoje considerada como a maior fonte de disseminação de informação nas principais áreas de conhecimento. O suporte ao acesso  a estas informações concentra-se sobretudo à ferramentas de pesquisa e folheio. Bibliotecas digitais e diretórios Web constituem importantes iniciativas no aperfeiçoamento de acesso à informação, criação e organização de coleções de documentos, de acordo com diferentes critérios. Ferramentas de pesquisa, por outro lado, disponibilizam os recursos de forma mais abrangente, utilizando serviços de indexação e coleta de documentos baseada em robôs. Contudo, as tecnologias aplicadas aos mecanismos de pesquisa na Web ainda oferecem pouco suporte ao gerenciamento de coleções de documentos, assim como os relacionamentos entre estes documentos não podem ser explicitamente identificados por seus formatos ou tipos. Este trabalho apresenta uma estrutura formal para a organização e descrição de coleções e documentos na Web. Esta é fundamentada em um modelo conceitual de metadados, que explora relacionamentos entre recursos de informação em diferentes níveis de granularidade. Para validar este modelo, um protótipo foi implementado utilizando duas tecnologias distintas: uma abordagem semi-estruturada, aplicando a arquitetura RDF (Resource Description Framework) em conjunto com a linguagem XML (eXtensible Markup Language) visando proporcionar alguma interoperabilidade semântica na Web;  e outra baseada em banco de dados Relacional-Objeto.', 
  resumo_en: '--'},

 {numero: 490, ano: 2001, dia: '19/02', autor: 'Cezar Simplicio Fernandes', orientador: [206], linha: 100, arquivo: '2001-Cezar.pdf', 
  titulo: 'Ferramenta para Estruturação e Visualização de Sítios na Web', 
  resumo_pt: 'Um dos principais problemas na Web está diretamente relacionado à vasta quantidade de informação disponível e nos seus limitados métodos de acesso. O processo de navegação utilizado para recuperar informações úteis é tão demorado que pode causar ao usuário uma sensação de desorientação. Grande parte das informações na Web é armazenada em documentos HTML, que oferecem poucas possibilidades para descrição e representação da estrutura de um documento, já que suas marcações focam apenas a forma apresentação da informação. Além disso, não permitem a representação semântica, um requisito necessário para expressar o relacionamento entre documentos. A World Wide Web Consortium (W3C) vem realizando um grande esforço no sentido de incrementar a semântica na Web. Um dos frutos desse trabalho é a XML (eXtended Markup Language), uma linguagem que, diferentemente da HTML, provê um conjunto extensível de marcações. Juntamente com a XSL (Extensible Sylesheet Language), permite separar o conteúdo da sua renderização. Outras iniciativas são o RDF (Resource Description Framework)  e o RDFS (Resource Description Framework Schema), poderosos mecanismos para a descrição do conhecimento, onde triplas (sujeito, predicado e objeto) denotam as relações entre pares de objetos. Muitas ferramentas e interfaces têm sido desenvolvidas com o objetivo de minimizar o problema da desorientação. Alguns destes auxílios permitem uma visão da topologia de um sistema de hipertexto, reduzindo a sua complexidade e permitindo ao usuário empregar um melhor conjunto de mecanismos de navegação, como o mapa do sítio, que é uma forma visual de descrever um sítio na Web. O presente trabalho apresenta um conjunto de ferramentas portáveis para auxiliar os usuários no processo de navegação. Nestas ferramentas os sítios na Web são descritos como um grafo e armazenados em documentos XML, segundo a arquitetura RDF.', 
  resumo_en: '--'},

 {numero: 489, ano: 2001, dia: '05/02', autor: 'Marcio de Carvalho Victorino', orientador: [206], linha: 100, arquivo: '2001-Marcio.pdf', 
  titulo: 'Uso da Tecnologia de Mediação em Extração de Dados e Metadados na Web para Sistemas de Apoio à Decisão Ambientais', 
  resumo_pt: 'Agências e organizações ambientais têm reconhecido que DWs orientados a assuntos ambientais podem causar profundo impacto na habilidade de proteger o meio ambiente. No entanto, a extração dos dados dos sistemas fonte para a carga da área de organização de dados desses DWs é muito mais complexa se comparada aos DWs convencionais. Em DWs ambientais é comum a extração ser baseada apenas em fontes externas, amplamente distribuídas, autônomas e heterogêneas. Visando minimizar esse problema, esse trabalho propõe o uso de uma arquitetura na qual o sistema de middleware Le Select e a tecnologia de DW são usados conjuntamente. O uso dessa arquitetura foi viabilizado através da implementação de uma ferramenta denominada LSExtract, que consiste de uma aplicação cliente Le Select, implementada em Java, com o objetivo de extrair os dados publicados pelo sistema Le Select e carregá-los, após sofrerem algumas transformações, na área de organização de dados de um DW ambiental.', 
  resumo_en: '--'},

 {numero: 488, ano: 2001, dia: '01/02', autor: 'Sérgio Luís Dutra de Lamare', orientador: [216], linha: 100, arquivo: '', 
  titulo: 'Avaliação da Implementação das Práticas-Chave de Melhoria da Gestão em Organizações Públicas: Aplicações a uma Instituição Pública de Ensino Superior', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 487, ano: 2000, dia: '01/12', autor: 'Vitor Moraes Ortuondo', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Representação Configurável de Conhecimento Visual', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 486, ano: 2000, dia: '01/12', autor: 'Marcelo Alves de Lima', orientador: [222], linha: 100, arquivo: '', 
  titulo: 'Modelo de Desativação de Viaturas: Aplicação ao Exército Brasileiro', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 485, ano: 2000, dia: '01/12', autor: 'Julio Cesar Silva Neves', orientador: [223], linha: 100, arquivo: '', 
  titulo: 'Aplicação de Análise Envoltória (DEA) para Avaliação de Fornecedores', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 484, ano: 2000, dia: '01/12', autor: 'Horus Armond da Fonseca', orientador: [218], linha: 100, arquivo: '', 
  titulo: 'k-Árvores: Caracterização, Propriedades e Aplicações', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 483, ano: 2000, dia: '01/12', autor: 'Franz Cristóbal Novillo Torrico', orientador: [206], linha: 100, arquivo: '', 
  titulo: 'Especificação de Regras de Negócio em Banco de Dados Relacional-Objeto', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 482, ano: 2000, dia: '01/12', autor: 'Fernanda Cristina Naliato', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Aplicação de Técnicas de Mineração de Dados: Estudo de Caso em Marketing Direto', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 481, ano: 2000, dia: '01/12', autor: 'Érica Ribeiro de Oliveira', orientador: [206], linha: 100, arquivo: '', 
  titulo: 'Interface Gráfica para Modelagem Conceitual num Banco de Dados Orientado a Objetos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 480, ano: 2000, dia: '01/12', autor: 'Eduardo Pinheiro do Amaral', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Geração de Regras Associativas Utilizando Intensivamente a Tecnologia de Bancos de Dados Relacionais', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 479, ano: 2000, dia: '01/12', autor: 'Cláudio Roberto Ferreira Costa', orientador: [224], linha: 100, arquivo: '', 
  titulo: 'Estudo de Robôs Cooperativos em Ambiente Simulado', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 478, ano: 2000, dia: '01/12', autor: 'Carlo Kleber da Silva Rodrigues', orientador: [218], linha: 100, arquivo: '', 
  titulo: 'O Problema do Caminho mais Rápido', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 477, ano: 2000, dia: '01/12', autor: 'Ademir Tomaz', orientador: [222], linha: 100, arquivo: '', 
  titulo: 'Boas Práticas Logísticas para a Manutenção de Morteiros 60 mm M949 AGR', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 476, ano: 2000, dia: '01/07', autor: 'Luís Alexandre Estêvão da Silva', orientador: [206], linha: 100, arquivo: '2000-Luis.pdf', 
  titulo: 'Geração Dinâmica de Interfaces de Bibliotecas Digitais em Metadados', 
  resumo_pt: 'Bibliotecas digitais estão sendo cada vez mais utilizadas como ferramenta de pesquisa por usuários mais experientes e que necessitam de informações mais especializadas em domínios específicos do conhecimento. Esta tendência em parte é justificada pela grande concentração de informação acumulada num mesmo site e pelos inúmeros serviços que uma biblioteca digital pode prestar, muitos dos quais assemelham-se aos de uma biblioteca tradicional. Porém, pelo fato de uma biblioteca digital ser disseminada num meio distribuído tal como a Internet, requer um conjunto complementar de ferramentas e serviços de forma a prover acesso eficiente e agradável a seus usuários. Este trabalho foi desenvolvido com o propósito de construir as ferramentas necessárias à melhoria da qualidade das pesquisas em uma biblioteca digital. Com a descrição mais apurada dos documentos eletrônicos através de um banco de metadados, é possível prover uma interface personalizada, tornando o manuseio por parte do usuário mais atrativo e natural. Este trabalho tem o suporte de um modelo baseado no estudo das categorias funcionais de metadados sobre documentos eletrônicos, visando ampliar as possibilidades de pesquisa em uma biblioteca digital. A partir desses metadados, o usuário dispõe de informações sobre os documentos do acervo em outras categorias de folheio, além daquela utilizada no momento, e da possibilidade de realizar pesquisas agregadas, que permitem a observação de informações complementares sobre um documento. Este modelo serviu como base para a implementação de um protótipo e aplicação em uma biblioteca digital na área da Ciência da Computação.', 
  resumo_en: '--'},

 {numero: 475, ano: 2000, dia: '01/06', autor: 'Alan Magno da Silva Gonzaga', orientador: [225], linha: 100, arquivo: '', 
  titulo: 'Arquiteturas para Gerência de Localização em Redes Sem Fio', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 474, ano: 2000, dia: '01/03', autor: 'Patrícia Fiuza de Castro', orientador: [208], linha: 100, arquivo: '2000-Patricia.pdf', 
  titulo: 'Categorização Automática de Textos', 
  resumo_pt: 'Com o crescente aumento da disponibilização de informação na forma textual, tais como, a possibilidade de armazenamento de textos na forma de documentos eletrônicos ou em campos texto dos bancos de dados e, principalmente , com o surgimento da World Wide Web, cresce também a necessidade de desenvolvimento de ferramentas cada vez mais poderosas para recuperação deste tipo de informação. Muitos recursos podem ser utilizados para aumentar a eficiência dos sistemas projetados com este objetivo. A categorização dos documentos é um exemplo de um destes recursos. A organização dos documentos de forma mais concisa, pode aumentar a eficiência nas consultas permitindo a busca em domínios apropriados, e facilitar a tarefa de browsing , caso o ambiente permita este tipo de consulta. O maior obstáculo para seu uso é a necessidade da presença de especialistas humanos que promovam esta categorização. O problema de categorização automática de documentos é bastante conhecido da área de Recuperação de Informação e, atualmente, resultados bastante positivos têm sido alcançados pela combinação das técnicas desta área e as desenvolvidas em Inteligência Artificial. Este trabalho foi desenvolvido com o propósito de descrever e implementar algumas destas técnicas, através do desenvolvimento de um protótipo cujo objetivo é promover a categorização automática de documentos. Todas as etapas deste processo são descritas e os resultados obtidos para a categorização em múltiplas classes, utilizando uma abordagem simbólica para o aprendizado, são apresentados.', 
  resumo_en: '--'},

 {numero: 473, ano: 2000, dia: '01/02', autor: 'Hélio Alvaro de Mello Perez', orientador: [206], linha: 100, arquivo: '2000-Helio.pdf', 
  titulo: 'Modelagem de Metadados para Suporte a Extração de Dados em Sistemas de Informações Ambientais', 
  resumo_pt: 'Informações ambientais são muito importantes para a criação de estratégias de proteção ambiental, sendo uma das grandes preocupações da maioria dos órgãos públicos e da sociedade no mundo. Essas informações são de natureza heterogênea e distribuída, e precisam ser integradas e transformadas em informação útil, levando ao surgimento dos Sistemas de Informações Ambientais (SIA). Uma das maiores dificuldades na criação de um SIA é que, apesar da grande quantidade de dados disponíveis, não existe um formato padrão de armazenamento, dificultando sua integração e utilização. Dentre os padrões de metadados específicos para a catalogação de dados ambientais mais conhecidos enquadram-se os padrões FGDC e UDK, que ainda possuem um baixo poder de representação da parte estrutural dos repositórios de dados. Este trabalho teve como objetivo melhorar esse poder de representação, a partir do desenvolvimento deuma arquitetura de três camadas para dar suporte ao acesso e extração de dados ambientais em repositórios heterogêneos e distribuídos; e de um meta-modelo baseado em metadados capaz de mapear a estrutura desses dados em um nível intermediário, permitindo aos usuários construírem programas que acessem a informação de forma direta, sem a necessidade da troca do local de armazenamento ou de mudanças na sua estrutura de armazenamento e padrão dos dados. De forma a validar esse modelo também foi construída uma ferramenta de gerência de metadados estruturais, oriundos de fontes heterogêneas. Esta ferramenta foi criada para o ambiente Web e testada com aplicações de sistemas ambientais reais.', 
  resumo_en: '--'},

 {numero: 472, ano: 1999, dia: '22/12', autor: 'Carlo Kleber da Silva Rodrigues', orientador: [218], linha: 100, arquivo: '1999-Carlo.pdf', 
  titulo: 'O Problema do Caminho Mais Rápido', 
  resumo_pt: 'O problema do caminho mais rápido consiste em determinar o caminho de menor tempo de transmissão entre uma origem e um destino em uma dada rede, considerando uma determinada quantidade de dados (carga, artigos, objetos, mercadorias, etc.) a ser transmitida. O estudo deste problema e das variações que naturalmente se sucederam desde a sua formalização é o objetivo desta Dissertação. Este estudo é conduzido através da apresentação e análise de alguns dos principais algoritmos propostos na literatura para a solução do problema, incluindo as suas variações. A metodologia de resolução do problema do caminho mais rápido é baseada na metodologia de resolução do clássico problema do caminho mais curto. Contudo, como diferenciação básica, tem-se que na primeira os caminhos são selecionados em função de duas métricas, enquanto que na segunda apenas uma métrica é utilizada. Esta diferenciação faz com que as modelagens de rede concebidas estejam mais próximas da realidade, garantindo soluções mais satisfatórias. A metodologia consiste basicamente ou na transformação, ou na decomposição da rede original em análise. Isto é feito para possibilitar que a partir da determinação de caminhos mais curtos possa-se identificar o caminho mais rápido procurado. Por fim, é mister ressaltar que, assim como o clássico problema do caminho mais curto, o problema do caminho mais rápido é também aplicável em diversas áreas de pesquisa que lidam com a necessidade da determinação de caminhos ótimos, como, por exemplo, as áreas de Redes de Computadores, Transportes, Telecomunicações e Engenharia Elétrica.', 
  resumo_en: '--'},

 {numero: 471, ano: 1999, dia: '01/12', autor: 'Ricardo Eiji Hamaoka', orientador: [222], linha: 100, arquivo: '', 
  titulo: 'Otimização de Sistemas Logísticos: Metodologia Aplicada à Unidade de Manutenção de Aviação do Exército Brasileiro', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 470, ano: 1999, dia: '01/12', autor: 'Luis Vandick Fajardo', orientador: [222], linha: 100, arquivo: '', 
  titulo: 'Metodologia para Implementação de um Sistema da Qualidade, Baseado na NBR ISO 9002, em Instituições de Ensino', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 469, ano: 1999, dia: '01/11', autor: 'Marcelo Rodrigues Pereira', orientador: [226], linha: 100, arquivo: '', 
  titulo: 'Metodologia de Avaliação de Implementação de Estratégias de Melhoria da Qualidade: Estudo de Caso na Indústria de Transporte Coletivo por Ônibus', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 468, ano: 1999, dia: '04/10', autor: 'Sérgio Luiz Curi Carazza', orientador: [208], linha: 100, arquivo: '1999-Sergio.pdf', 
  titulo: 'Sistema de Aquisição e Análise de Conhecimento', 
  resumo_pt: 'Este trabalho foi desenvolvido com o propósito de implementar um Sistema de Aquisição e Análise de Conhecimento através do desenvolvimento de técnicas automatizadas, de modo a eliminar os inúmeros problemas que surgem na interação entre especialistas e engenheiros de conhecimento, como os existentes nos Sistemas Especialistas. O objetivo básico do desenvolvimento de técnicas automáticas para aquisição de conhecimento, de modo a evitar distorções em relação ao conhecimento a ser representado, deve ser a preservação de conteúdo semântico deste mesmo conhecimento adquirido do próprio especialista. O desenvolvimento do sistema apoia-se em algumas noções mais básicas, tais como estruturação de conhecimento em planos hierárquicos e processamento da linguagem natural. Para o desenvolvimento do sistema, onde uma teoria deste tipo seja aplicável, foi necessário o uso de um provador automático de teoremas capaz de processar cláusulas genéricas, como mecanismo de inferência, e o uso de oráculos neste ambiente.', 
  resumo_en: '--'},

 {numero: 467, ano: 1999, dia: '09/08', autor: 'Cássia Maria Barreto', orientador: [206], linha: 100, arquivo: '1999-Cassia.pdf', 
  titulo: 'Modelo de Metadados para a Descrição de Documentos Eletrônicos na Web', 
  resumo_pt: 'O crescente status alcançado pela Web como ferramenta para publicação de recursos, realçado pelos avanços na área de recuperação de informações tem conduzido a uma sobrecarga de informações de difícil gerenciamento. Nesse contexto, a utilização de metadados fornecidos pelos próprios autores de documentos da Web tem sido considerado por diversos pesquisadores como a solução mais adequada para tornar mais precisa a descoberta de documentos eletrônicos, uma vez que as estratégias de indexação automática dos conteúdos analisados, empregadas pela maioria das ferramentas atuais de pesquisa, não têm sido suficiente para atender à demanda cada vez maior de informações. A iniciativa de metadados descrita neste trabalho é denominada Modelo de Objetos Digitais para Documentos Eletrônicos (MODDE). Seu objetivo é possibilitar a gerência de documentos da Web com base no seu conteúdo intelectual, além de auxiliar o usuário leigo na descrição adequada de suas propriedades segundo diversos padrões de metadados. O modelo proposto é baseado em projetos de metadados recentes, tais como a arquitetura Warwick, desenvolvida para o ambiente da Web, e no conceito de objeto digital proposto pela arquitetura de Kahn e Wilensky para a representação de recursos de informação gerenciados por Bibliotecas Digitais distribuídas. Sua estrutura hierárquica multi-camadas possibilita a organização coerente e padronizada do conteúdo do documento juntamente com as descrições associadas a esse conteúdo.', 
  resumo_en: '--'},

 {numero: 466, ano: 1999, dia: '01/08', autor: 'Yolanda Larraona Tavares', orientador: [206], linha: 100, arquivo: '1999-Yolanda.pdf', 
  titulo: 'Um Gerenciador de Meta-Esquemas no Suporte a Mediadores numa Arquitetura para Interoperabilidade entre Sistemas de Banco de Dados', 
  resumo_pt: 'Atualmente, o ambiente computacional das empresas é formado por uma variedade de computadores pertencentes a diferentes plataformas de hardware e software que armazenam dados relevantes ao desempenho das organizações, desencadeando a demanda por aplicações capazes de integrar esses dados que são criados e mantidos de forma independente. Nesta dissertação é apresentada uma arquitetura que tem como meta prover a interoperabilidade de SGBDs heterogêneos e distribuídos, disponibilizando aos usuários uma visão transparente e uniforme de todas as aplicações participantes. A arquitetura proposta é baseada na tecnologia de mediadores, sendo estes responsáveis pela gerência e integração dos dados armazenados nos SGBDs componentes da arquitetura. A interoperabilidade entre os componentes da arquitetura é garantida pela adesão às especificações CORBA, definida pelo OMG, enquanto que o mediador adota as especificações do ODMG para o modelo de dados, linguagem de consulta e definição de dados. Nesta arquitetura, a manipulação dos dados integrados através dos mediadores é facilitada devido à gerência de metadados provida pela arquitetura. Esses metadados são definidos segundo um modelo de objetos. Nesse modelo estão representadas informações acerca das fontes de dados participantes, conflitos e mapeamentos concernentes, assim como a localização dos componentes da arquitetura. Informações estatísticas também estão disponíveis, possibilitando a otimização de consultas globais pelos mediadores. Desta forma, um componente fundamental desta arquitetura é o gerente de metadados, que permite também aos usuários buscar e descobrir novas fontes de informação. A atenção especial à gerência de metadados facilitada pelo padrão ODMG e o uso de serviços de um gerente de objetos, trouxe inovações para arquiteturas de sistemas heterogêneos baseados em mediadores.', 
  resumo_en: '--'},

 {numero: 465, ano: 1999, dia: '01/08', autor: 'Andréa Gonçalves Preto', orientador: [214], linha: 100, arquivo: '', 
  titulo: 'META-SIG: Ambiente de Metadados para Aplicações de Sistemas de Informação Geográficos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 464, ano: 1999, dia: '01/07', autor: 'Maria do Socorro Alves Nunes', orientador: [227], linha: 100, arquivo: '', 
  titulo: 'Análise dos Modos e Efeitos de Falha (FMEA) de Produto com Utilização do Desdobramento da Função Qualidade (QFD)', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 463, ano: 1999, dia: '01/07', autor: 'Maria Cristina Honorato dos Santos', orientador: [227], linha: 100, arquivo: '', 
  titulo: 'Avalição de Métodos Indiretos para Obtenção dos Graus de Importância dos Requisitos do Cliente', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 462, ano: 1999, dia: '27/05', autor: 'Claudia Hazan', orientador: [222], linha: 100, arquivo: '1999-Claudia.pdf', 
  titulo: 'Metodologia para o Uso de Indicadores na Gerência de Projetos de Desenvolvimento de Software', 
  resumo_pt: 'Na década de 90, a Era da Qualidade da indústria de software, as organizações que desenvolvem software estão lidando com clientes sofisticados e exigentes que demandam alta qualidade, baixo custo e suporte total após a entrega do software. A Qualidade de Software pode ser definida como um conjunto de propriedades de software a serem satisfeitas em determinado grau, de modo a satisfazer as necessidades de seus usuários. A maioria das tecnologias de Engenharia de Software, como as metodologias de desenvolvimento de software e as técnicas de programação e testes, são inerentemente qualitativas. Mas os problemas do desenvolvimento do software não são exclusivamente qualitativos. Existem também vários problemas quantitativos, como o tempo estimado de duração do projeto, o custo, a alocação de recursos e o esforço gasto.  Este trabalho propõe o desenvolvimento de uma metodologia para o uso de indicadores na gerência de projetos de desenvolvimento de software, mediante a aplicação de métricas de software, indicadores da qualidade e produtividade e ferramentas gerenciais da qualidade. O objetivo principal é promover uma gerência eficaz de projetos de desenvolvimento de software com base em indicadores quantitativos para assegurar um melhor planejamento e controle de projetos. E ainda, fornecer subsídios para as organizações promoverem o aumento da produtividade e a melhora da qualidade de seu processo de desenvolvimento de software.  A implantação da Metodologia desenvolvida neste trabalho possui cinco fases: Implantação de um processo de medição de software; Definição dos Indicadores; Utilização de Dados Históricos para o planejamento e controle do projetos; Utilização de  ferramentas da Qualidade; Utilização dos Indicadores para promover a Melhoria do Processo. O estudo de caso foi desenvolvido na maior Unidade de Negócios do SERPRO - Serviço Federal de Processamento de Dados, a SUNAT- Superintendência de Negócios Administração Tributária. Foram destacados os seguintes projetos associados à implantação da metodologia: Implantação da técnica Análise por Pontos de Função, Sistemáticas de Pesquisa da Satisfação do Cliente e da Qualidade do Produto de Software e o Programa de Benchmarking desenvolvido para a área de desenvolvimento de sistemas da SUNAT, denominado PIT (Programa de Intercâmbio Técnico) - Desenvolvimento.', 
  resumo_en: '--'},

 {numero: 461, ano: 1999, dia: '01/05', autor: 'Mario Carlos Campanella', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Sistemas Dinâmicos em Inteligência Artificial e um Aplicativo para Linguagem Natural', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 460, ano: 1999, dia: '07/04', autor: 'Marcelo Adriano Perecim', orientador: [228], linha: 100, arquivo: '1999-Marcelo.pdf', 
  titulo: 'Visualização de Ambientes Virtuais Prediais', 
  resumo_pt: 'O problema de visualização interativa de cenas com quantidade elevada de polígonos tem motivado inúmeros esforços na sua solução. Artifícios como pouca iluminação, neblina e planos de projeção reduzido têm sido utilizados nesta direção. Uma solução precisa para visualização de ambientes reais com elevada quantidade de polígonos ainda se faz necessária. Ambientes prediais são tipicamente modelados através de Partição Binária do Espaço (BSP). Devido à sua estrutura arborescente, o método de BSP se presta a diversas otimizações que aceleram a exibição das cenas, comprometendo, porém, a uniformidade da velocidade de exibição. Neste trabalho utilizamos o método de BSP combinado com a Determinação dos Conjuntos de Polígonos Potencialmente Visíveis (PVS) para otimizar a visualização de ambientes prediais preservando a uniformidade de exibição.', 
  resumo_en: '--'},

 {numero: 459, ano: 1999, dia: '01/04', autor: 'Roberto Lehmann', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Melhoria de Qualidade na Recuperação de Informação na Web', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 458, ano: 1999, dia: '01/04', autor: 'Marcel Costa Almeida', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Arquitetura de Sistemas Multi-Agentes para Projeto de Linhas de Transmissão de Energia Elétrica', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 457, ano: 1999, dia: '01/03', autor: 'Lawrence Zordan Klein', orientador: [214], linha: 100, arquivo: '', 
  titulo: 'A Tecnologia Relacional-Objeto em um Ambiente de Data Warehouse', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 456, ano: 1999, dia: '01/03', autor: 'Eduardo Zapico Mouro', orientador: [229], linha: 100, arquivo: '', 
  titulo: 'Enfoque Participativo para Reengenharia de Processos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 455, ano: 1999, dia: '01/03', autor: 'Carlos Augusto Rolim', orientador: [226], linha: 100, arquivo: '', 
  titulo: 'Construção de Instrumento de Auto-Diagnóstico Organizacional: Aplicações do Serviço Público', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 454, ano: 1999, dia: '25/02', autor: 'Mariela Inés Cortés', orientador: [230], linha: 100, arquivo: '1999-Mariela.pdf', 
  titulo: 'Especificação Formal da Linguagem DDL usando Transformações', 
  resumo_pt: 'Neste trabalho é proposta uma metodologia para o desenvolvimento de um Front-End de uma linguagem de programação, responsável pela análise sintática e geração de uma representação intermediária para programas. O Front-End é desenvolvido através de transformações de programas, as quais rescrevem programas para uma representação intermediária simplificada, consistindo de um subconjunto da própria linguagem. No presente trabalho, é gerado um Front-End para a linguagem Design Description Language, desenvolvida academicamente na Pontifícia Universidade Católica do Rio de Janeiro. Esta linguagem apresenta algumas características dificilmente encontradas em outras linguagens de programação. Com o objetivo de facilitar a tradução da linguagem para outras linguagens de programação, tais características foram tratadas através de transformações de programas. As características consideradas neste trabalho são: tratadores de exceção a nível de classe, estados de objetos, classes genéricas, e constantes básicas Uma vez que as características mencionadas foram tratadas de forma adequada, o subconjunto da linguagem resultante é especificado formalmente utilizando semântica denotacional direta.', 
  resumo_en: '--'},

 {numero: 453, ano: 1999, dia: '22/02', autor: 'Gustavo Henrique Monteiro de Barros Carneiro', orientador: [224], linha: 100, arquivo: '1999-Gustavo.pdf', 
  titulo: 'Tratamento de Comandos Remotos e Processamento de Visão para Navegação de Veículos Autônomos em Tempo Real', 
  resumo_pt: 'A área de robótica vem apresentando um crescente interesse por parte da comunidade científica, principalmente na área de desenvolvimento de veículos autônomos. Um veículo autônomo apresenta um conjunto de sistemas mecânicos, elétricos e, especialmente, computacionais, capaz de realizar tarefas sem intervenção humana. Um de seus objetivos mais importantes é a realização de atividades em locais hostis à presença humana. Em aplicações deste tipo é necessário o monitoramento de suas ações e o envio de ordens de alto nível, remotamente, evitando que pessoas corram risco de vida. Um veículo autônomo deve ser capaz de desviar de obstáculos, achar trajetórias entre um ponto inicial e final, reconhecer certos tipos de objetos presentes em um ambiente, obedecer ordens enviadas remotamente, etc. Este trabalho apresenta o desenvolvimento de dois sub-sistemas no projeto do veículo autônomo CONTROLAB sendo desenvolvido no NCE-UFRJ (Núcleo de Computação Eletrônica da Universidade Federal do Rio de Janeiro). O primeiro é o desenvolvimento do sistema de comunicação e o segundo trata do sistema de visão computacional. O sistema de comunicação inclui a implementação de um sistema cliente/servidor para que operadores remotos possam enviar ordens e receber informações do veículo autônomo através da internet, e o sistema de rede sem fio possibilita a comunicação entre o veículo autônomo e o servidor. O sistema de visão é uma ferramenta de auxílio ao sistema de navegação para determinação da presença de obstáculos em tempo real. O veículo autônomo CONTROLAB necessita do desenvolvimento de vários outros sistemas, tais como o sistema de ultra-som, o navegador inteligente, o planejador de trajetória, etc., que deverão funcionar de forma integrada para que o objetivo final, um veículo capaz de agir autonomamente e de receber ordens remotamente, possa ser alcançado.', 
  resumo_en: '--'},

 {numero: 452, ano: 1999, dia: '01/02', autor: 'Simone de Souza Garcia', orientador: [206], linha: 100, arquivo: '1999-Simone.pdf', 
  titulo: 'Metadados para Documentação e Recuperação de Imagens', 
  resumo_pt: 'Este trabalho foi desenvolvido com o propósito de desenvolver uma modelagem conceitual que permita a descrição e recuperação de imagens, por meio do estudo e sistematização das características de imagens. As classes pertencentes ao esquema conceitual decorrente dessa modelagem viabilizam a documentação de uma imagem estática do tipo fotografia, pintura ou gravura qualquer, descrevendo suas propriedades, os componentes contidos na imagem e os relacionamentos existentes entre eles. Estes descritores permitem também identificar os objetos imagem que são versões de outro objeto imagem e os tipos de associações existentes entre os mesmos. O esquema proposto descreve tanto as informações técnicas quanto as informações relacionadas ao conteúdo semântico da imagem, contribuindo com uma especificação sistemática e detalhada dos elementos descritores de imagens estáticas digitais, podendo com isso serem utilizados em arquiteturas de metadados existentes. Este esquema serviu como base para a implementação de um protótipo de aplicação em um ambiente que possibilita a descrição e recuperação de imagens baseadas em seus descritores, possibilitando ainda que as imagens sejam consultadas no ambiente Internet.', 
  resumo_en: '--'},

 {numero: 451, ano: 1999, dia: '01/02', autor: 'Mutaleci de Góes Miranda', orientador: [225], linha: 100, arquivo: '', 
  titulo: 'Metodologia para Projeto de Bancos Distribuídos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 450, ano: 1999, dia: '01/02', autor: 'Cláudio Gomes de Mello', orientador: [225], linha: 100, arquivo: '', 
  titulo: 'SVS: um Sistema de Videoconferência Seguro', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 449, ano: 1999, dia: '01/01', autor: 'Luis Gustavo Varges Resende', orientador: [208], linha: 100, arquivo: '', 
  titulo: 'Reformulação Interativa da Consulta Utilizando um Meta-Mecanismo de Busca', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 448, ano: 1999, dia: '01/01', autor: 'Felipe Garcia Torrescano', orientador: [231], linha: 100, arquivo: '', 
  titulo: 'Metodologia para a Utilização do Desdobramento da Função Qualidade (QFD), na Manutenção Centrada em Confiabilidade (RCM)', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 447, ano: 1998, dia: '01/12', autor: 'Maurício José Viana Amorim', orientador: [214], linha: 100, arquivo: '', 
  titulo: 'Ambiente de Desenvolvimento de Aplicações de Banco de Dados Ativos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 446, ano: 1998, dia: '01/12', autor: 'Fernando Cesar Castaño Mariño', orientador: [232], linha: 100, arquivo: '', 
  titulo: 'Análise de Segurança das Cifras Rijndael e Serpent Contra as Criptoanálises Linear e Diferencial', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 445, ano: 1998, dia: '01/12', autor: 'Alexandre Almeida Lima', orientador: [232], linha: 100, arquivo: '', 
  titulo: 'Projeto e Implementação de uma Função de Condensação Resistente às Criptoanálises Diferencial e Linear', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 444, ano: 1998, dia: '01/05', autor: 'Carlos Mathias Mota Vargas', orientador: [222], linha: 100, arquivo: '', 
  titulo: 'Sistemas de Gestão para a Excelência: um Enfoque para a Indústria de Software', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 443, ano: 1998, dia: '01/04', autor: 'João Luiz Alves de Barros', orientador: [213], linha: 100, arquivo: '', 
  titulo: 'Ferramenta para Mineração de Dados com Interface Inteligente', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 442, ano: 1998, dia: '01/03', autor: 'Maria Candida Sotelino Torres', orientador: [222], linha: 100, arquivo: '', 
  titulo: 'O Uso da Simulação em uma das Perspectivas do Balanced Scorecard', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 441, ano: 1998, dia: '01/01', autor: 'Ronaldo Moreira Salles', orientador: [225], linha: 100, arquivo: '', 
  titulo: 'Protocolos de Múltiplo Acesso para Redes sem Fio', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 440, ano: 1998, dia: '01/01', autor: 'João Abdalla Ney da Silva', orientador: [225], linha: 100, arquivo: '', 
  titulo: 'Algoritmos de Solução para a Análise de Desempenho de Modelos de Sistemas de Computação e Comunicação', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 439, ano: 1997, dia: '22/12', autor: 'George Hamilton Andrade Costa', orientador: [214], linha: 100, arquivo: '', 
  titulo: 'Engenharia Reversa de Regras Ativas em Banco de Dados Relacionais', 
  resumo_pt: 'Uma das características dos sistemas de banco de dados é a habilidade de gerenciar eficientemente grande quantidade de informação. Quando regras são introduzidas nos sistemas de banco de dados, resultando nos chamados sistemas de banco de dados ativo, esta característica deve ser aplicada também às regras. O desenvolvimento de grandes sistemas com regras ativas requer o entendimento do comportamento das regras, especialmente porque a sua execução pode ser não determinística Oll conflitante. Existem várias ferramentas visuais para projeto de bancos de dados, baseadas em metodologias consagradas de modelagem de sistemas. Contudo, sente-se a necessidade de um tipo de ferramenta que possa suportar a visualização do comportamento ativo, para que o desenvolvimento e manutenção de regras ativas seja feito de forma mais fácil. De outro modo, seria muito difícil prever as interações complexas entre regras, eventos e as transações sobre entidades e relacionamentos do banco de dados. O presente trabalho descreve um ambiente de projeto de regras para bancos de dados ativos, baseado no modelo conceitual (ER)2 (Entidade, Relacionamento, Evento e Regra). Realiza uma análise comparativa de modelos, como também formula metamodelos, com a finalidade de verificar a sua capacidade de expressão de restrições de integridade, principalmente as representadas como regras ativas. Um protótipo de ferramenta para captura de regras ativas, especificadas como triggers em bancos de dados relacionais, é apresentado como prova de princípios da proposta.', 
  resumo_en: '--'},

 {numero: 438, ano: 1997, dia: '19/12', autor: 'Wladimir da Silva Meyer', orientador: [213], linha: 100, arquivo: '', 
  titulo: 'Metodologias para Classificação de Texturas e Consulta a Base de Imagens', 
  resumo_pt: 'Este trabalho teve por objetivo a pesquisa de técnicas de classificação de texturas, a formulação de algoritmos utilizando resultados de trabalhos, bem como a construção de um protótipo capaz de possibilitar uma avaliação eficaz dessas técnicas. Como objetivos específicos, incluem-se aspectos que envolvem a investigação de técnicas adequadas de realce (Processamento de Sinais), estudo da adeq uabilidade de modelos neurais na função de classificadores de texturas (Inteligência Artificial) bem como a utilização de descritores de textura na recuperação de imagens, por conteúdo, em bancos de dados relacionais (Banco de Dados).', 
  resumo_en: '--'},

 {numero: 437, ano: 1997, dia: '01/12', autor: 'Francisco Raul Aguirre Cabrera', orientador: [222], linha: 100, arquivo: '', 
  titulo: 'Alocação de Requisitos de Confiabilidade, Manutenibilidade e Disponibilidade (RAM) com Desdobramento da Função Qualidade (QFD)', 
  resumo_pt: 'Este presente trabalho visa a apresentar o desenvolvimento de uma metodologia para a alocação dos Requisitos de Confiabilidade, Disponibilidade e Manutenibilidade (Reliability, Availability and Maintainability - RAM) de um sistema, aplicando a metodologia de Desdobramento da Função Qualidade (QFD), de modo que, desde a fase de planejamento, seja considerada a voz do cliente e possa-se assegurar a qualidade do sistema no tempo e desta forma maximizar a satisfação do cliente. Para cumprir esse objetivo reali zou-se um estudo bibliográfico da Metodologia do Desdobramento da Função Qualidade (QFD), Requisitos de Confiabilidade, Manutenibilidade e Disponibilidade denominados Requisitos (RAM), diversos métodos de alocação e Análise de Modos de Falha (FMEA). A metodologia começa com a consideração dos Requisitos do Cliente e descrição geral do sistema, e através do uso do Desdobramento da Função Qualidade (QFD) são traduzidos esses Requisitos do Cliente em Requisitos de Projeto, os quais são alocados nos subsistemas e componentes do sistema, e desta forma assegurar a qualidade do sistema no tempo e conduzir a um nível maior de satisfação do cliente. Finalmente, é desenvolvida uma aplicação desta metodologia a uma adaptação de uma viatura do Exército Brasileiro, com o objetivo de mostrar a utilidade do mesmo, como ferramenta para a equipe de projetistas do sistema possa priorizar os componentes de maior importância para garantir a qualidade do produto.', 
  resumo_en: '--'},

 {numero: 436, ano: 1997, dia: '01/11', autor: 'Patrícia Alcântara Cardoso', orientador: [233], linha: 100, arquivo: '', 
  titulo: 'Uma Metodologia para o Desenvolvimento dos Indicadores Estratégicos em Logística', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 435, ano: 1997, dia: '01/11', autor: 'Claudio Azevedo Passos', orientador: [213], linha: 100, arquivo: '', 
  titulo: 'Ambiente para o Desenvolvimento de Sistemas Especialistas: Edição e Prototipação', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 434, ano: 1997, dia: '11/09', autor: 'Charles Sampaio Collyer Junior', orientador: [206], linha: 101, arquivo: '', 
  titulo: 'Linguagem de Consulta ao SIGO: um Sistema Gerenciador de Objetos', 
  resumo_pt: 'SIGO é um Sistema Gerenciador de Objetos em desenvolvimento no !ME (Instituto Militar de Engenharia), utilizado para o desenvolvimento de aplicações emergentes em Banco de Dados. Este trabalho tem como objetivo agregar novas funcionalidades à sintaxe da linguagem de consulta e manipulação LIMOS (Linguagem de Manipulação de Objetos no SIGO), implementando-a e integrando-a ao SIGO de duas formas: através de uma intelface de consulta interativa baseada em menus e ícones, dirigida principalmente a usuários finais, com processamento via ODBC (Open Database Conllectivity) e viabilizando um ambiente de programação para o desenvolvimento de aplicações homogêneo e uniforme, onde comandos da LIMOS podem ser embutidos no escopo da linguagem de programação C++, utilizada pelo sistema. Para atingir este objetivo alguns SGBDOOs (Sistemas Gerenciadores de Banco de Dados Orientados a Objetos) foram estudados, bem como as suas respectivas linguagens de consulta. O comando na linguagem de consulta LIMOS, antes de ser submetido ao ODBC, é traduzido para SQL utilizando-se o gerador de analisadores sintáticos LARS (Look-Ahead R*S).', 
  resumo_en: '--'},

 {numero: 433, ano: 1997, dia: '31/07', autor: 'José Mauro Marquez', orientador: [234], linha: 102, arquivo: '', 
  titulo: 'Procedimento para Análise de Oportunidade de Investimento para Aumentar a Capacidade de Movimentação em um Terminal de Conteineres', 
  resumo_pt: 'A evolução da conteinerização mundial e nacional, bem como a expansão dos terminais nestas últimas duas décadas, é investigada por meio de uma revisão bibliográfica sobre o tema. Os modais envolvidos no transporte de contêineres para o term.inal são analisados com a finalidade de identificar os fluxos internos de contêineres e os seus eventuais gargalos operacionais. Os fluxos foram divididos nos eventos: Recepção, Estocagem e DesembarquelEmbarque. A partir daí, os mesmos foram analisados e verificou-se que o evento de desembarque/embarque tem o maior potencial restritivo no que concerne às causas de atrasos na movimentação de contêineres. O passo seguinte do procedimento, é anal.isar as atividades do evento crítico, sendo que a atividade de descarga do navio foi identificada como a que movimenta o menor número de contêineres, pois a mesma além de ser complexa, depende dos portêineres, que são guindastes de cais próprios para a movimentação de contêineres. Este equipamento tem grandes proporções e só três podem ser alocados por navio, tornando esta atividade restritiva para efeito de aumento de capacidade. Dentre os equipamentos utilizados nesta atividade, o portêiner foi identificado como o que apresenta as maiores restrições operacionais, e a partir daí, propõe-se uma análise do problema a ser estudado para gerar alternativas que aumentem a capacidade de movimentação do terrn.inal, obedecendo a critérios de custo para sua seleção. Uma aplicação do procedimento, com a finalidade de mostrar sua utilidade para tomada de decisão gerencial, é feita com base em dados obtidos no term.inal de contêineres da Companhia Docas do Rio de Janeiro - TECONT/CDRJ. Uma das alternativas de investimento foi a modernização dos portêineres e a outra, foi a aquisição de mais dois com concepções modernas e de maiores proporções.', 
  resumo_en: '--'},

 {numero: 432, ano: 1997, dia: '21/07', autor: 'Helen Lucy da Silva', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Estudo da Qualidade nos Processos da Prestação de Serviços', 
  resumo_pt: 'É sumamente importante para uma empresa, que deseja continuar no mercado de trabalho, cada vez mais competitivo, começar a se preocupar com as reclamações dos clientes e procurar atingir suas expectativas, buscando com isso o aperfeiçoamento da qualidade na prestação dos seus serviços. Com ênfase nas empresas prestadoras de serviços, foram levantados alguns critérios para o aperfeiçoamento dos processos de melhoria que as empresas poderão utilizar para atingir as expectativas dos seus clientes, partindo para a conquista de novos mercados. Com base em leituras sobre o assunto em referencia, abordaram-se vários autores que definem o conceito da melhoria da qualidade nos processos da prestação de serviços. Ademais, adotando-se como uma empresa do ramo de refrigeração de porte médio, nela se aplicou uma metodologia de avaliação da qualidade dos serviços produzidos, onde se procura identificar como os clientes percebem os serviços prestados.', 
  resumo_en: '--'},

 {numero: 431, ano: 1997, dia: '15/07', autor: 'Josué Peter de Oliveira', orientador: [216], linha: 103, arquivo: '', 
  titulo: 'Metodologia de Implantação da AJ3NT ISO/IEC Guia 25 nos Laboratórios de Análises de Radionuclídeos em Amostras Ambientais da Divisão de Análises Ambientais/CNEN', 
  resumo_pt: 'A Norma ABNT ISO/IEC GUIA 25: 1993 (Requisitos gerais para capacitação de laboratórios de calibração e de ensaios) publicada pela Associação Brasileira de Normas Técnicas (ABNT) estabelece requisitos gerais que um laboratório deve demonstrar, para ser formalmente credenciado para realizar calibrações ou ensaios específicos. Desta forma, o laboratório passa a fazer parte da Rede Brasileira de Calibração (RBC) ou da Rede Brasileira de Laboratórios de Ensaio (RBLE) respectivamente. A Divisão de Análises Ambientais (DIAMB) do Departamento de Proteção Radiológica Ambiental (DEPRA) do Instituto de Radioproteção e Dosimetria (IRD) da Comissão Nacional de Energia Nuclear (CNEN) é um laboratório que realiza análises de radionuclídeos em amostras provenientes do Programa de Fiscalização, Pesquisa e Prestação de Serviços, quanto à possível contaminação por radionuclídeos, seja no meio ambiente, em alimentos e outros insumos para consumo humano, inclusive, na certificação destes para fins de importação e exportação. Por este motivo, a DIAMB tem necessidade de seu reconhecimento formal para a realização de análises de radionuclídeos em amostras ambientais. Este trabalho visa fornecer uma metodologia para orientar um laboratório que busque implementar um processo de credenciamento. Descreve também as políticas no cumprimento dos requisitos de que trata a Norma, as orientações necessárias ao detalhamento de determinadas fases e comenta alguns pontos da Norma, de forma a facilitar o entendimento do processo de credenciamento como um todo.', 
  resumo_en: '--'},

 {numero: 430, ano: 1997, dia: '07/07', autor: 'Lívia Cavalcanti Figueiredo', orientador: [235], linha: 100, arquivo: '', 
  titulo: 'Análise dos Fatores que Afetam a Confiabilidade de Questionários de Satisfação do Cliente', 
  resumo_pt: 'Uma organização precisa conhecer como os clientes julgam seus produtos ou serviços. Devido a isso, torna-se necessário desenvolver instrumentos de pesquisa que meçam a percepção do cliente em relação ao produto ou serviço oferecido. Um dos instrumentos mais utilizados é o questionário. Entretanto, na elaboração deste tipo de instrumento de pesquisa é necessário assegurar que os dados obtidos a partir dele possuam a confiabilidade e vaLidade da informação. A confiabilidade pode ser afetada por diversos fatores que diminuem a precisão do instrumento. Este trabalho apresenta uma aplicação da Abordagem dos Incidentes Críticos e Desenvolvimento de Dimensões da Qualidade na elaboração de questionário de medida de satisfação, bem como a análise dos fatores que afetam a confiabilidade deste instrumento de pesquisa.', 
  resumo_en: '--'},

 {numero: 429, ano: 1997, dia: '26/06', autor: 'Letícia Dexheimer', orientador: [236], linha: 102, arquivo: '', 
  titulo: 'Sistema para Gerenciamento Operacional em Terminais Intermodais de Carga', 
  resumo_pt: 'O presente trabalho tem como objetivo o da proposição de um procedimento para auxiliar na racionalização das operações realizadas nos terminais intermodais de carga, visando reduzir as interferências no fluxo de transporte. Este procedimento deverá contar com um módulo que possibilite a análise estatística dos tempos de realização das atividades para detecção dos possíveis pontos críticos e outro contendo a identificação das causas mais prováveis. Analisando-se as diferentes técnicas de transporte intermodal, optou-se por elaborar um sistema para terminais rodoferroviários especializados na movimentação de contêineres, buscando aproveitar as vantagens oriundas de sua utilização e seu notável crescimento no transporte internacional de mercadorias. Cabe notar que esta escolha não é restritiva, pois análises equivalentes podem ser desenvolvidas para outras situações, guardando as devidas características. Após o estudo das operações realizadas neste tipo de terminal, o procedimento foi elaborado com o auxílio de um modelo de simulação, o SIMUL, para obtenção das estatísticas que auxiliam na avaliação do desempenho operacional do terminal. A análise dos resultados da aplicação possibilita a adoção de políticas gerenciais, de forma a obter a racionalização das operações nesses terminais e a conseqüente redução dos tempos de permanência das cargas no mesmo. Essa simulação permite tratar vários cenários em curtos períodos de tempo, de forma a analisar várias estratégias operacionais antes da tomada de decisão.', 
  resumo_en: '--'},

 {numero: 428, ano: 1997, dia: '03/06', autor: 'Polyana Abrahão Menani', orientador: [208], linha: 101, arquivo: '', 
  titulo: 'Descoberta de Conhecimento via Provador de Teorema', 
  resumo_pt: 'O grande volume de dados armazenados, decorrentes da informatização dos diversos segmentos da sociedade, nos transporta a um novo contexto no tratamento da informação e sistemas de apoio a decisão. Neste novo contexto somente a disponibilização dos dados não bastará para que se desenvolvam estratégias de ação bem-sucedidas, será necessário que se possa interpretar, analisar e relacionar estes dados. Com isso, para atingir este novo requisito dos sistemas de informação que efetivamente apoiem decisões, surge uma busca intensa por ferramentas que tornem a análise de dados e a extração de conhecimento, implícito nos dados, a mais automática possível. Esta tese pesquisa técnicas que auxiliem no desenvolvimento destas ferramentas com enfoque na descoberta de conhecimento em banco de dados, conhecida como Knowledge Discovery) in Database (KDD). O trabalho concentra-se na revisão e adaptação de uma das técnicas de Programação Lógica Indutiva (ILP) denominada Generalização Menos Geral Relativa no contexto de Data Mining a partir de um banco relacional de dados. Este trabalho envolve basicamente a integração de duas linhas de pesquisa: Banco de Dados, especificamente banco relacional de dados, e ILP, com a abordagem da técnica Generalização Menos Geral Relativa.', 
  resumo_en: '--'},

 {numero: 427, ano: 1997, dia: '27/05', autor: 'Eugenio Rangel Marins', orientador: [218], linha: 101, arquivo: '', 
  titulo: 'Traçado Automático de Grafos Hierárquicos', 
  resumo_pt: 'Grafos hierárquicos são amplamente utilizados em muitas áreas do conhecimento, tais como economia, eletrônica e ciência da computação. Uma boa visualização da informação estrutural permite ao leitor focalizarse no conteúdo informativo do desenho. A necessidade de se obterem estes desenhos de forma rápida levou ao desenvolvimento de programas de computador para o traçado de tais grafos. Estes programas buscam a otimização de uma série de critérios que definem como deve ser um bom traçado. O principal critério para a obtenção de um bom traçado é a minimização do número de cruzamentos entre arestas, e este se constitui em um problema NP-completo. Neste trabalho são apresentados, analisados e comparados vários algoritmos heurísticos para a redução de cruzamentos de arestas no traçado automático de grafos hierárquicos. Analisam-se também algoritmos de reconhecimento de planaridade em grafos hierárquicos.', 
  resumo_en: '--'},

 {numero: 426, ano: 1997, dia: '15/05', autor: 'Renata Baltensberger Ferreira', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'Processo de Descobrimento de Conhecimento em Bases de Dados', 
  resumo_pt: 'Pesquisas têm sido feitas para que dados possam ser coletados mais rapidamente e com mais eficiência, resultando em bases de dados cada vez maiores. Com isto, a cada dia se torna mais fácil a obtenção de dados representativos dos fenômenos que nos cercam. Nos dias atuais estas mesmas bases de dados atingiram tamanhos tais que se tornaram um problema para os especialistas. Apesar do desenvolvimento de novas técnicas nos moldes tradicionais para o estudo destas bases, elas se mostram insuficientes. Com isto surge um novo ramo no estudo destas bases de dados, conhecido como extração de conhecimento em bases de dados (KDD - knowledge discovery in databases). O presente trabalho apresenta um estudo do processo de extração de conhecimento em bases de dados, composto das diversas técnicas para que este processo seja realizado do melhor modo possível, de forma que um analista de dados possa ter uma idéia geral do processo, das técnicas utilizadas e de suas possibilidades. O processo de extração de conhecimento em bases de dados é um processo complexo e, ainda hoje, muito dependente da experiência do especialista. Nosso trabalho visa, primordialmente, apresentar uma visão geral deste processo, sendo que ao final é apresentado um estudo de caso onde se tenta que este seja um processo mais automático possível. A pesquisa envolveu, portanto, um estudo da fase de preparação de dados, do processo de mineração de dados, das principais técnicas estatísticas e de redes neurais para reconhecimento de padrões, o estudo de um caso e todas as demais técnicas que podem ser usadas pelo processo de KDD.', 
  resumo_en: '--'},

 {numero: 425, ano: 1997, dia: '01/05', autor: 'Tânia Mara Potrich', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Qualidade de Vida na Empresa Holística', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 424, ano: 1997, dia: '28/04', autor: 'Andréa Teles da Silva', orientador: [206], linha: 101, arquivo: '', 
  titulo: 'Definição e Implementação de Regras Ativas em um Sistema Gerenciador de Objetos', 
  resumo_pt: 'Sistemas de Banco de Dados Ativos têm sido uma das áreas mais proeminentes desde a década de 1980. A extensão dos sistemas de banco de dados com funcionalidades ativas provê um mecanismo eficiente e uniforme para restrição de integridade, visões, monitoramento e alerta. Sistemas de Banco de Dados Ativos executam por si só certas operações automaticamente, em resposta à ocorrência de certos eventos e/ou condições satisfeitas. Sistemas ativos são centrados em torno da noção de regra. As regras E-C-A ou regras ativas são estruturas que permitem especificar o comportamento reativo desejado. Regras ativas definidas no contexto da Orientação a Objeto contribuem para monitorar o comportamento ativo de um objeto. Consequentemente, em um ambiente Orientado a Objeto ativo, o comportamento dos objetos pode ser especificado pelos métodos, aplicações e regras. Este trabalho apresenta a definição e implementação da linguagem de regras ativas do SIGO - Sistema Gerenciador de Objetos - bem como os componentes ativos responsáveis pela funcionalidade ativa. A implementação da arquitetura integrada e do tratamento das regras como objetos de primeira classe foi fundamental para tornar o SIGO um sistema ativo.', 
  resumo_en: '--'},

 {numero: 423, ano: 1997, dia: '23/04', autor: 'José de Carvalho Bustamante', orientador: [236], linha: 102, arquivo: '', 
  titulo: 'Alternativas Econômicas de Transporte Marítimo de Granéis Sólidos dos Países em Desenvolvimento', 
  resumo_pt: 'Os fluxos marítimos de granéis sólidos do comércio internacional experimentaram um crescimento vertiginoso a partir da década de 50. Considerando-se seus cinco componentes mais importantes - minério de ferro, carvão mineral, soja, bauxita e fosfatos - no período de 1970 a 1994, o total movimentado passou de 448 milhões de toneladas anuais para 1.015 milhões, com um incremento médio de 3,3% ao ano. Como conseqüência, para poder transportar com eficiência estes fluxos, o porte médio dos navios graneleiros novos teve que crescer continuadamente ao longo do período. Assim de 26 mil toneladas de porte bruto em 1965, chegou-se a 99 mil toneladas em 1995, com um incremento anual de 4,4%. Estes fatos, por sua vez, pressionaram os países exportadores e importadores a terem de adequar seus terminais graneleiros aos calados dos novos navios, fato sempre de elevado investimento e freqüentemente restrito pelas condições naturais dos litorais. O impacto deste problema de adequação porto-frota usuária na economia de produção e de exportação dos países fornecedores destas matérias-primas, em geral pertencentes ao Terceiro Mundo, forçou a busca de soluções alternativas mais econômicas, uma vez que não se podia excluir o uso de graneleiros de grande porte nestes fluxos, por seus custos unitários de transporte muito mais baratos, chegando a ser da ordem de 50%. Como o frete marítimo tem grande incidência no preço CIF destes produtos, que em sua quase totalidade são de baixo valor unitário, e o mercado é bastante competitivo, o problema passou a ser prioritário parei estes países. Duas alternativas despontaram como promissoras: o transshipment, ou seja a transferência ao largo de carga de navios de menor porte, com acesso aos terminais existentes de pouca profundidade, para os grandes graneleiros oceânicos, impedidos de atracar nestes terminais; ou o topping, em que se carrega no terminal apenas parcialmente o graneleiro de porte e se completa o carregamento em ponto onde permita a profundidade, pelo sistema que for mais adequado às circunstâncias. A tese examina conceitualmente a economicidade destas alternativas, centrando-se no transshipment, devido aos óbices comerciais que restringem o toppin.g, analisando os componentes das fases de carga, transporte e descarga dos produtos. Como resultado, montou-se um programa computacional, que a partir dos dados atuais e dos previstos no novo tipo de operação, estabelecesse para um dado fluxo a economicidade ou não de se executar o transshipment. Para avaliação de sua aplicabilidade prática, fez-se seu uso em fluxos de soja em grão de Brasil e Argentina para a Europa, com pleno êxito, justificando seu emprego.', 
  resumo_en: '--'},

 {numero: 422, ano: 1997, dia: '09/04', autor: 'Gilberto Vasques Cava', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Análise e Melhoria de Processos em Organizações de Saúde: um Estudo de Caso', 
  resumo_pt: 'As organizações de saúde necessitam aprimorar-se continuamente por meio da incorporação de novas tecnologias e da agilização de procedimentos para atender melhor seus clientes. A estruturação do hospital pela influência das ordens religiosas e da disciplina militar, a avaliação da qualidade dos serviços prestados, os Programas de Acreditação Hospitalar e a aplicação da Gestão pela Qualidade Total (GQT) na área da saúde são abordados a partir do conceito de vários autores. Foi realizado o Estudo de Caso no Instituto Estadual de Hematologia Arthur de Siqueira Cavalcanti - HEMORIo. A aplicação da Metodologia para Análise e Melhoria de Processos (MAMP), como parte da estratégia de implementação da GQT, proporcionou o aperfeiçoamento de processos-chaves e mais satisfação aos clientes da organização. As instituições de saúde devem monitorar a opinião dos usuários, pois pelo atendimento de suas necessidades é que a atenção consegue ser melhorada continuamente. O interesse crescente dos administradores hospitalares pelos programas de GQT ocorre porque se fazem necessárias ferramentas para atingir níveis de excelência e tornar as organizações competitivas. As formas de administrar e os sistemas de recompensa devem estar direcionados para o fortalecimento da horizontalização das estlUturas e das novas funções, como liderança de equipes. Devem ser estlUturados bons serviços de fornecimento de dados integrados com outras instituições através de redes de informações. A aplicação da MAMP permitiu observar a importância da liderança na facilitação da atuação e na motivação dos integrantes das equipes, que o elemento norteador nas decisões relativas às modificações dos processos foi a satisfação dos usuários, que a integração multiprofissional ocorreu de maneira bastante satisfatória pela troca de conhecimentos e diferentes experiências. Para atingir o êxito dos programas de qualidade, os gerentes têm que atuar conforme os propósitos da organização, treinar sua equipe e participar ativamente na execução das tarefas. Os profissionais de saúde devem capacitar-se a criar protocolos, padronizar procedimentos e valorizar o uso de medidas para avaliar o grau de melhoria dos processos. Os indicadores devem ser desenvolvidos pelas pessoas envolvidas diretamente nos processos e utilizados para acompanhamento, para se certificar de que as melhorias são continuadas. O nível de desempenho do hospital é um fator fundamental para sua imagem externa e os resultados devem estar documentados e ser divulgados. Os pacientes devem colaborar com idéias e participar na efetivação de soluções por meio de suas associações. Os princípios e ferramentas da qualidade devem ser ensinados nos cursos de graduação e pós-graduação dos profissionais de saúde para que possam desempenhar melhor suas tarefas. O alinhamento das aspirações dos funcionários com a estratégia organizacional é de grande importância para o êxito dos programas de Gestão pela Qualidade Total. A transformação da cultura organizacional pela incorporação de novas idéias e formas de trabalhar, como a atuação de equipes multi profissionais com a utilização de ferramentas da qualidade, a focalização no cliente e a possibilidade de utilização da Metodologia para Análise e Melhoria de Processos em outras unidades do Sistema de Saúde, mostra o benefício que pode trazer à melhoria da qualidade da atenção prestada nos hospitais brasileiros.', 
  resumo_en: '--'},

 {numero: 421, ano: 1997, dia: '07/04', autor: 'Eduardo Roberto Perez Deschamps', orientador: [206], linha: 101, arquivo: '', 
  titulo: 'Sistema Gerenciador de Objetos em uma Arquitetura Cliente-Servidor', 
  resumo_pt: 'A Arquitetura Cliente/Servidor é um importante requisito tecnológico para a integração de sistemas. Isto permite a conexão de diferentes plataformas, provendo um melhor uso dos sistemas computacionais relacionados com a recuperação e processamento da informação. Esta tecnologia está sendo amplamente utilizada em Sistemas Gerenciadores de Banco de Dados (SGBDs), diminuindo a distância e aumentando a qualidade nas informações das tarefas dos usuários finais. Este trabalho permite mostrar a transformação de um Sistema Gerenciador de Objetos (SIGO), que inicialmente se apresentava numa plataforma monousuária, para uma plataforma multiusuária baseada na Arquitetura Cliente/Servidor, utilizando o protocolo de comunicação TCPIIP e uma biblioteca de Sockets. Este trabalho descreve os importantes passos, lógicos e físicos, requisitados neste processo. Como o paradigma da Orientação a Objeto, a Arquitetura Cliente/Servidor se constitui em um ambiente distribuído flexível e de alto nível, facilitando, então, a implantação de sistemas complexos. SIGO é um sistema que visa prover o suporte necessário às aplicações emergentes em banco de dados, embora inicialmente seus módulos tenham sido desenvolvidos para plataformas monousuário.', 
  resumo_en: '--'},

 {numero: 420, ano: 1997, dia: '01/03', autor: 'Maria do Carmo Santos Neta', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Aplicação do Custeio Baseado em Atividades aos Serviços Administrativos Ligados a Produção: Estudo de Caso', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 419, ano: 1997, dia: '07/02', autor: 'Carlos Alberto dos Santos', orientador: [237], linha: 104, arquivo: '', 
  titulo: 'Suporte de Metadados na Recuperação de Imagens de Satélites Digitais Classificadas', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 418, ano: 1997, dia: '01/02', autor: 'Rubens Botelho da Silva', orientador: [235], linha: 100, arquivo: '', 
  titulo: 'Metodologia para Determinação de Requisitos de Projeto Visando o Cliente Interno Dedicado à Prestação de Serviços', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 417, ano: 1997, dia: '01/02', autor: 'Carlos Roberto Kenji Fuzita', orientador: [235], linha: 102, arquivo: '', 
  titulo: 'Procedimento para Análise da Eficácia de Veículos Rodoviários para Fins de Mobilização', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 416, ano: 1997, dia: '30/01', autor: 'Luiz Paulo Souto Fortes', orientador: [237], linha: 104, arquivo: '1997-Luiz.pdf', 
  titulo: 'Operacionalização da Rede Brasileira de Monitoramento Contínuo do Sistema GPS', 
  resumo_pt: 'A presente dissertação apresenta a Rede Brasileira de Monitoramento Contínuo do Sistema GPS (RBMC), que constitui uma rede geodésica de referência ativa. A RBMC é composta por um conjunto de estações de rastreamento permanente GPS, com abrangência nacional, cujos dados destinam-se aos usuários envolvidos com posicionamentos relativos baseados neste sistema. Este trabalho conceitua os aspectos relacionados à rede, descreve suas especificações e todas as etapas de sua operação, que incluem: gravação do arquivo de observações na memória do receptor GPS; sua descarga para o microcomputador que compõe a estação, transformação para o formato RINEX, crítica preliminar, compactação e transferência para o centro de controle da rede, localizado na sede do Departamento de Geodésia do IBGE, no Rio de Janeiro; e controle remoto das estações via linha telefônica discada ou pela Internet. São também descritas as rotinas computacionais desenvolvidas com o objetivo de automatizar o funcionamento da rede, uma vez que, segundo as especificações do projeto, a operação das estações deve se dar de forma desassistida. Por fim, são avaliados os procedimentos desenvolvidos no escopo deste trabalho, tomando por base a fase inicial de operação das duas primeiras estações implantadas segundo as especificações aqui apresentadas, situadas em Curitiba, Paraná e Presidente Prudente, São Paulo, bem como apresentadas conclusões e indicadas sugestões para futuras pesquisas.', 
  resumo_en: '--'},

 {numero: 415, ano: 1997, dia: '01/01', autor: 'Reginaldo da Silva Figueiredo', orientador: [230], linha: 101, arquivo: '1997-Reginaldo.pdf', 
  titulo: 'Sistemas Baseados em Conhecimento em Ambiente de Tempo Real', 
  resumo_pt: 'O principal objetivo deste trabalho é o estudo para a concepção de uma arquitetura apropriada à gerência de Agentes, em Sistemas Baseados em Conhecimento Em Ambiente de Tempo Real. Esta arquitetura foi nomeada MONITOR. Mais ainda: a idéia inicial para a concepção do modelo baseou-se no princípio de somente gerenciar Agentes de um mesmo tipo. Em outras palavras: a princípio a idealização do modelo visava apenas à administração, sob regime de tempo real, de Agentes cuja concepção tivesse sido projetada para operar Máquinas de Inferência que trabalhassem com Bases de Conhecimento, elaboradas segundo o paradigma de representação do conhecimento referenciado como Lógica de Primeira Ordem (LPO) ou Lógica de Predicados (LP).', 
  resumo_en: '--'},

 {numero: 414, ano: 1997, dia: '01/01', autor: 'Beniamin Achilles Bondarczuk', orientador: [235], linha: 103, arquivo: '', 
  titulo: 'Programação Linear Aplicada ao Desdobramento da Função Qualidade', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 413, ano: 1996, dia: '27/12', autor: 'Marcelo Nunes de Souza', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Metodologia de aplicação do Controle Estático da Qualidade para Melhoria de Processo', 
  resumo_pt: 'Este trabalho surgiu da necessidade de se estabelecer e desenvolver novas formas de controlar e melhorar a qualidade de processos, tendo como base o controle estatístico da qualidade. O trabalho apresenta uma metodologia de aplicação do controle estatístico, da qualidade sobre resultados gerados por um processo de modo que seu desempenho real e seu desempenho desejado possam ser comparados, ações corretivas possam ser propostas e realizadas quando necessário e oportunidades de melhoria possam ser aproveitadas. A abordagem proposta está baseada eu, fundamentos, conceitos e técnicas do controle e melhoria da qualidade e da probabilidade e estatística.', 
  resumo_en: '--'},

 {numero: 412, ano: 1996, dia: '23/12', autor: 'Nelsom Jose Veiga de Magalhães', orientador: [238], linha: 103, arquivo: '', 
  titulo: 'As Mudanças Metodológicas no Desenvolvimento de Novos Produtos', 
  resumo_pt: 'No escopo deste trabalho é desenvolvida uma organização para as árvores do produto, configurações do produto e sistemasdo produto com o intuito de fazer suporte ao sistema de desenvolvimento de novos produtos e sua otinúzação, dentro do conceito de produção modular estruturada. O sistema de construção modular é um método que permite projetar de maneira a se obter uma rápida adaptação a desejos especiais dos clientes, tomando a produção flexível e em quantidades crescentes com baixo custo. Um estudo sobre o gerenciamento de configurações e suas interfaces com processos computacionais é apresentado, bem como uma política de desenvolvimento de novos produtos, juntamente com conceitos para se realizar o start-up nos trabalhos de documentação e organização final do produto. Integramos neste trabalho dois conceitos importantes: o de autonomação e produtação, que fazem parte de todo um contexto de Percepção da Qualidade.', 
  resumo_en: '--'},

 {numero: 411, ano: 1996, dia: '18/12', autor: 'Manoel Gomes de Pinho', orientador: [239], linha: 101, arquivo: '', 
  titulo: 'Análise de Desempenho e Simulação da Plataforma Cliente/Servidor Orientada a Objeto', 
  resumo_pt: 'A utilização de Sistemas Distribuídos, em especial de Sistemas Cliente/Servidor, é uma realidade na Informática. A conjugação dos paradigmas Cliente/Servidor e de Orientação a Objeto, denominada Computação de Objetos Distribuídos, é a tendência atual, como indicam os modelos CORBAlOpenDoc e COM/OLE, propostos recentemente e ainda em fase de desenvolvimento e consolidação. A Arquitetura Cliente/Servidor Orientada a Objeto pode prover um ambiente distribuído, flexível e de alto nível, facilitando a implantação de sistemas complexos. Para que a distribuição de objetos pela rede não afete substancialmente o desempenho da Aplicação Cliente/Servidor Orientada a Objeto, fazem-se necessários, durante a fase de desenvolvimento, a modelagem do Sistema e um estudo aprofundado de tal modelo através de técnicas analíticas ou de simulação. A modelagem permite ainda o Planejamento da Capacidade futura do Sistema e o ajuste do desempenho atual. Pesquisando técnicas de modelagem e análise de desempenho, constmiu-se um protótipo em: linguagem C++, sob UNIX, em uma rede de estações de trabalho SUN e um modelo deste Sistema para inferêncía do desempenho, utilizando-se o Sistema de Simulação ARENA e fórmulas da Teoria das Filas e Análise Operacional.', 
  resumo_en: '--'},

 {numero: 410, ano: 1996, dia: '17/12', autor: 'Luiz Henrique da Costa Araujo', orientador: [218], linha: 101, arquivo: '', 
  titulo: 'Algoritmos Dinâmicos para Determinação de Caminhos em Digrafos', 
  resumo_pt: 'Algoritmos dinâmicos são aqueles que calculam a solução do problema par a par com a construção do digrafo que o modela. Para que seja possível o cálculo de uma nova solução a cada modificação do grafo, é necessário um grande suporte em termos de estruturas de dados. Neste trabalho faremos uma coletânea de algoritmos dinâmicos para a solução de problemas de caminho, estudando suas estruturas de dados, funcionamento e complexidade.', 
  resumo_en: '--'},

 {numero: 409, ano: 1996, dia: '16/12', autor: 'Marcelo Lima de Paiva', orientador: [239], linha: 101, arquivo: '', 
  titulo: 'Segurança das Mensagens na Arquitetura Cliente-Servidor Orientada a Objeto', 
  resumo_pt: 'A demanda pela distribuição dos sistemas tem provocado uma mudança nos paradigmas de desenvolvimento de software. A Arquitetura Cliente-Servidor Orientada, a Objeto é formada pela integração de duas tecnologias, Cliente-Servidor e orientação a objeto, e constitui uma forma de distribuir um sistema de maneira eficiente e flexível. Contudo, face à sua característica principal, que é a utilização de redes, a complexidade da segurança das mensagens trocadas pelos objetos através da rede cresce consideravelmente. Esta tese pesquisa meios para proteger as mensagens trocadas pelos objetos através da rede, assim como os aspectos de segurança em ambientes Cliente-Servidor e de objetos distribuídos. É proposto um modelo de protocolo para a comunicação entre, os objetos da aplicação Cliente-Servidor Orientada a Objeto e foi desenvolvido um protótipo em ambiente Sun para validar este modelo. As pesquisas permitiram concluir que é necessário portar os algoritmos criptográficos para utilização com estruturas de dados simples e complexas, assim como se deve buscar a independência entre os serviços de segurança e as aplicações.', 
  resumo_en: '--'},

 {numero: 408, ano: 1996, dia: '16/12', autor: 'Salvatore Di Giovanna Mazzone', orientador: [240], linha: 104, arquivo: '', 
  titulo: 'Uma Biblioteca de Classes Baseada no Catálogo de Objetos da Organização Hidrográfica Internacional (OHI)', 
  resumo_pt: 'O presente trabalho propõe um mecanismo para facilitar a definição, o armazenamento e a manipulação de dados não-convencionais em aplicações geográficas, Embora possa ser aplicada a uma variedade de outras situações espaciais, o mecanismo proposto foi elaborado no contexto das aplicações cartográficas, em particular, na área de cartografia náutica, permitindo ao usuário concentrar-se inteiramente nos aspectos relevantes dessa aplicação, sem quaisquer preocupações com a implementação dos objetos envolvidos na elaboração das cartas. Para isso, foi criada uma biblioteca de classes baseada no catálogo da Organização Hidrográfica Internacional (OHI), como um mecanismo de geração de objetos no desenvolvimento de aplicações na área de cartografia náutica. Esse mecanismo, além de gerar a estrutura dos objetos instanciados pelo usuário, também cria automaticamente os métodos que implementam sua funcionalidade, A biblioteca implementada contém um conjunto de classes de objetos envolvidas na definição de canais de navegação e dispositivos de separação de tráfego marítimo.', 
  resumo_en: '--'},

 {numero: 407, ano: 1996, dia: '03/12', autor: 'Roberto Mascia Moreira Amorim', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Indicadores de Qualidade do Software', 
  resumo_pt: 'As Métricas de Software são indicadores de qualidade do software. Estes indicadores identificam tendências a erros e falhas no software, que se identificados no princípio permitem que o código seja reescrito e/ou testado em detalhes. Contudo, a integração do uso das métricas em ambientes comerciais encontra resistências orçamentais, gerenciais e de prazo. Esta dissertação apresenta um programa métrico que introduz um método para a integração de métricas em um ambiente de desenvolvimento de software. Nesta tese será descrito um processo de identificação de medidas e a definição das métricas correspondentes que suportam a avaliação da qualidade do software. Este programa métrico fornece as diretrizes para a avaliação quantitativa da qualidade dos produtos de cada fase, a realimentação da informação para auxílio gerencial na tomada de decisões e critérios para o re-projeto ou re-codificação do sistema. Os paradigmas do GQM (Meta-Questão-Métrica) e do QFD (Desdobramento da Função Qualidade) foram usados para demonstrar como o programa métrico pode ser iniciado e implantado.', 
  resumo_en: '--'},

 {numero: 406, ano: 1996, dia: '01/11', autor: 'Julio Cesar Reguerín Pardo', orientador: [214], linha: 101, arquivo: '', 
  titulo: 'Modelagem de Sistemas de Tempo Real Baseados em Bancos de Dados Ativos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 405, ano: 1996, dia: '06/08', autor: 'Nelson Antonio Torracca', orientador: [236], linha: 102, arquivo: '', 
  titulo: 'Produtividade e Qualidade do Sistema de Transporte Ferroviário de Cargas', 
  resumo_pt: 'O objetivo deste trabalho de tese é o desenvolvimento de um procedimento que permita maximizar a Produtividade de um sistema ferroviário de cargas, aqui definida como sendo sua Rentabilidade, sujeita às restrições de Qualidade vistas pela ótica dos clientes. É realizada uma revisão bibliográfica que mostra a evolução e a importância do Sistema Ferroviário Brasileiro, bem como dos conceitos de Qualidade e Produtividade e seus inter-relacionamentos a fim de que pudessem ser extraídos desses indicadores as variáveis dependentes e independentes e os parâmetros de entrada que integram o procedimento proposto. A Qualidade é tratada através dos atributos disponibilidade, confiabilidade e segurança sendo que apenas o primeiro atributo, por possuir uma representação analítica através da demanda por unidade de tempo, foi incluído no procedimento supracitado. Finalmente, a Rentabilidade, diferença entre receitas e custos do transporte, é calculada em função da freqüência dos trens que é a variável de decisão do problema. Uma aplicação deste procedimento é desenvolvida com o objetivo de mostrar a potencialidade do mesmo como útil ferramenta gerencial.', 
  resumo_en: '--'},

 {numero: 404, ano: 1996, dia: '24/07', autor: 'Silvio Sodré Pacheco', orientador: [220], linha: 101, arquivo: '', 
  titulo: 'Algoritmos Híbridos Nepro-Genéticos', 
  resumo_pt: 'A presente tese proporciona o estudo de uma classe de algoritmos de treinamento de redes neurais que associam algoritmos convencionais do tipo backpropagation e algoritmos genéticos, denominada algoritmos híbridos neuro-genéticos. Vários aspectos destes algoritmos são analisados e o estado da arte, bem como a tendência para o desenvolvimento de novas pesquisas na área, é avaliado. É realizada empiricamente com ferramentas desenvolvidas durante a própria pesquisa, uma análise comparativa entre as performances dos algoritmos estudados no treinamento de uma rede neural na resolução de alguns problemas, tais como XOR, Encoder 4-2-4, Paridade 4 e o reconhecimento de dígitos alfanuméricos. É proposto ,um novo método denominado Shake, assim como algumas variações do mesmo, buscando otimizar o treinamento de redes neurais sob um vetor de critérios considerados desejados pelos pesquisadores conexionistas.', 
  resumo_en: '--'},

 {numero: 403, ano: 1996, dia: '04/07', autor: 'Ivo Rogério Reinhold', orientador: [222], linha: 102, arquivo: '', 
  titulo: 'Qualidade em Serviços de Transporte Coletivo Urbano por Ônibus', 
  resumo_pt: 'O presente trabalho tem como objetivo propor diretrizes gerenciais e operacionais de funcionalidade para melhoria da Qualidade, com base nos seus princípios e técnicas, relacionados com o conceito de nível de serviço que permita orientar o operador da empresa de TCUO a melhorar os seus serviços. Para cumprir este objetivo realizou-se um estudo teórico da literatura existente sobre a Gestão da Qualidade nos Transportes, e desenvolveu-se também pesquisa junto aos agentes dos Transportes por Ônibus (empresário e usuário), do Brasil, para determinar de acordo com a concepção de cada um deles os atributos mais importantes para a melhoria da prestação de serviços. Concluiu-se que os principais atributos são Conforto, Pontualidade, Rapidez, Segurança e Atendimento. Para cada atributo relacionaram-se indicadores como forma de representar o nível de serviços que a empresa deve operar e sugeriram-se recursos administrativos para melhorar a Qualidade dos Serviços oferecidos.', 
  resumo_en: '--'},

 {numero: 402, ano: 1996, dia: '24/05', autor: 'Gláucia Brito Brandão', orientador: [236], linha: 102, arquivo: '', 
  titulo: 'Transportes e o Meio Ambiente no Brasil', 
  resumo_pt: 'O objetivo deste trabalho é a análise da legislação ambiental brasileira em relação aos Transportes, focalizando-se sua evolução e as distintas técnicas utilizadas para avaliação dos impactos ambientais para estes projetos. Para tal, foram analisados os conceitos básicos e os principais documentos destinados à orientação do desenvolvimento dos Estudos de Impactos Ambientais e seus respectivos Relatórios de Impactos Ambientais: ElAs-RIMAs. A crescente conscientização da população quanto ao direito de reivindicar a garantia da qualidade de vida e a exigências de elaboração dos ElAs-RIMAs para fins de liberação de empréstimos internacionais, contribuíram para atualização da legislação ambiental brasileira. Entretanto, foram verificados alguns obstáculos que impedem a sua plenitude de ação quer seja na abordagem governamental na gestão do meio ambiente, quer sejam nas limitações técnicas das equipes encarregadas pela elaboração ou aprovação dos mesmos ou dos órgãos estaduais de controle e fiscalização.', 
  resumo_en: '--'},

 {numero: 401, ano: 1996, dia: '14/05', autor: 'Eliane Maracajá Porto', orientador: [202], linha: 104, arquivo: '', 
  titulo: 'Aplicação de Sistemas de Informação Geográfica na Análise de Dados Sócio-Econômicos', 
  resumo_pt: 'A crescente aplicação dos Sistemas de Informação Geográfica na análise de dados sócio-econômicos se deve, principalmente, a disponibilidade cada vez maior de dados espacialmente referenciados. A principal vantagem em se utilizar tais sistemas na análise de dados sócio-econômicos reside em sua característica mais importante, que os tornam mais do que simples ferramentas computacionais gráficas utilizadas na geração de mapas, que é a sua capacidade de modelar fenômenos do mundo real, de uma forma dinâmica. Neste sentido, os dados armazenados nesses sistemas, sob a forma digital, podem ser atualizados e visualizados simultaneamente, propiciando ao analista uma rápida e nova visão das mudanças sócio-econômicas que ocorrem em um determinado espaço geográfico. Entretanto, a representação de fenômenos sócio-econômicos, em um ambiente de Sistemas de Informação Geográfica, esbarra em questões que envolvem o problema da modelagem de dados uma vez que se espera que o modelo escolhido reflita de forma mais fiel possível a realidade geográfica que se deseja representar. Neste contexto, o principal objetivo desta pesquisa é a modelagem e estruturação de dados sócio-econômicos (não-gráficos) em um ambiente de Sistemas de Informação Geográfica, procurando permitir a análise dos relacionamentos espaciais de dados de natureza estatística, essencialmente não-gráficos, com dados de natureza gráfico-cartográfica, discutindo-se alguns modelos alternativos de representação.', 
  resumo_en: '--'},

 {numero: 400, ano: 1996, dia: '13/05', autor: 'Axel da Fonseca Keppke', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'Um Hipertexto com Sistema Especialista Médico Embutido', 
  resumo_pt: 'O rápido desenvolvimento tecnológico tem influenciado o ensino médico, requerendo mudanças contínuas dos meios de ensino e material didático (SINHA, 1992). Por outro lado, os leigos recorrem cada vez mais ao computador buscando informações médicas, preparando-se melhor para a consulta ou evitando visitas desnecessárias ao consultório. É comum hoje, principalmente nos EUA, médicos comparem hipertextos em CDROMs com históricos de doenças de sua especialidade para auxiliar nos seus diagnósticos. Neste trabalho, é apresentado um estudo de técnicas para a criação de Hipertextos e Sistemas Especialistas, modelos de funcionamento destas duas tecnologias, além da ligação entre elas visando o desenvolvimento de um sistema. Além disso, demonstra-se a viabilidade da aplicação na medicina de um sistema de Hipertexto com Sistema Especialista embutido. O resultado final é a apresentação de um protótipo de Sistema constituído por um Hipertexto com informações médicas úteis para o leigo e dois Sistemas Especialistas em Clínica Médica: um fornecendo ao leigo pré-diagnóstico provável a partir das características do sintoma e outro fornecendo ao médico conduta apropriada a partir das características da doença.', 
  resumo_en: '--'},

 {numero: 399, ano: 1996, dia: '09/05', autor: 'José Guilherme de Souza Pinto', orientador: [214], linha: 101, arquivo: '', 
  titulo: 'Arquiteturas para Interconexão de Computadores Móveis', 
  resumo_pt: 'Este trabalho trata dos aspectos ligados à mobilidade em redes de computadores baseada na família de protocolos TCP/IP, com particular ênfase para o nível de rede. Características desta família de protocolos são também apresentadas. Inicialmente, são descritas algumas arquiteturas, recentemente propostas, para a interconexão de computadores móveis. A seguir a nova versão para o IP, o IPNG (versão 6 ou IPV6), é descrita sucintamente, ressaltando-se o tratamento dado à mobilidade. Baseado em três destas arquiteturas, é aplicada uma metodologia baseada em eventos discretos, combinando o Método das Três Fases e o Diagrama de Ciclo de Atividade, com uma ferramenta de simulação desenvolvida no IME, o SIMUL. Estas arquiteturas são então comparadas com base nos resultados obtidos para esta simulação.', 
  resumo_en: '--'},

 {numero: 398, ano: 1996, dia: '03/05', autor: 'Ricardo Oliveira Barros', orientador: [206], linha: 101, arquivo: '1996-Ricardo.pdf', 
  titulo: 'Uso de Regras Ativas no Tratamento de Evolução de Esquemas em um Sistema de Gerenciador de Objetos', 
  resumo_pt: 'Evolução de esquemas é um importante requisito para aplicações OO (Orientadas a Objeto). Esta funcionalidade está associada à habilidade de executar dinamicamente uma larga variedade de modificações sobre o esquema do BD, minimizando a necessidade de uma reorganização em prol de um melhor desempenho do sistema. Modificações em esquemas de SGBDOOs tendem a ser mais freqüentes do que em SGBDs convencionais, devido ao fato de que um esquema de BDOO é extensível, ou seja, suporta a criação de novos tipos e operações. Portanto, a existência de muitos interrelacionamentos no esquema e a necessidade de alterações cada vez mais freqüentes, fazem com que o suporte à evolução de esquemas em SGBDOOs seja necessário e muito mais complexo que nos SGBDs convencionais. Muitos dos SGBDOOs comercialmente disponíveis tratam apenas da integridade estrutural do esquema, não permitindo modificações depois da base instanciada. Neste trabalho, são tratados os efeitos das modificações do esquema sobre os métodos e objetos instanciados no SIGO - Sistema Gerenciador de Objetos. O principal objetivo é garantir, além da integridade estrutural, a integridade comportamental e de instanciação do esquema da aplicação. Este resultado é obtido pelo armazenamento das efetivas modificações sobre o esquema numa hierarquia de classes e pela consequente propagação destas modificações aos objetos instanciados. A propagação é feita através de métodos definidos como regras ativas associados às classes. Estas regras são automaticamente ativadas por métodos que referenciam os objetos durante a manipulação do esquema.', 
  resumo_en: '--'},

 {numero: 397, ano: 1996, dia: '19/04', autor: 'João Roberto de Toledo Quadros', orientador: [206], linha: 101, arquivo: '', 
  titulo: 'Modelagem de Hiperdocumentos em um Sistema Gerenciador de Objetos', 
  resumo_pt: 'Multimídia e hipermídia são áreas cuja importância tem crescido muito no contexto de BDs. Porém essa interação requer o desenvolvimento de um modelo conceitual que permita o acoplamento de um documento em um Sistema Gerenciador de Banco de Dados. Esta tese propõe e implementa um modelo conceitual de documentos que utiliza características e padrões multimídia e hipermídia, incluindo também um mapeamento para a arquitetura geral de Banco de Dados através do mecanismo de visões para tratamento de consultas. O modelo de documentos, denominado DOCSIGO, foi definido para o ambiente de, Banco de Dados SIGO. Seu principal objetivo é demonstrar a possibilidade de se trabalhar com sistemas de hiperdocumentos em Banco de Dados Orientados a Objetos. Além do desenvolvimento do DOCSIGO, foi paralelamente construído um protótipo que contém as principais características do modelo. O DOCSIGO possui os elementos hipermídia (âncoras de componente inteiro, ligações unidirecionais, etc.) e de Banco de Dados multimídia (sincronismo, armazenamento de dados multimídia como objetos longos, etc.), podendo acessar componentes de um esquema conceitual do SIGO, ou elementos de sistemas externos. Cada uma das suas camadas (armazenamento, estrutura lógica, instanciação e apresentação) trabalham em conjunto com os módulos do SIGO, através de interfaces específicas, contidas no protótipo implementado.', 
  resumo_en: '--'},

 {numero: 396, ano: 1996, dia: '11/04', autor: 'Suely Pires de Oliveira', orientador: [241], linha: 103, arquivo: '', 
  titulo: 'O Problema dos k-servos com Janela de Tempo', 
  resumo_pt: 'Na busca de fornecer ao Sistema de Atendimento uma nova ferramenta para modelagem, controle e otimização sobre o ponto de vista da qualidade, no que diz respeito a confiabilidade, conformidade e a prontidão do atendimento, foi desenvolvida uma·nova classe dos K-Servos, que permitirá a inclusão da Janela de Tempo no modelo tradicional, classe essa denominada K-Servos com Janela de Tempo ou K-SJT. A validação e testes do modelo foram realizadas dentro do Sistema Flexível de Assistência ao Cliente (SFAC), com a finalidade de alcançar resultados práticos, permitindo avaliar o modelo dos K-SJT de maneira positiva. Em virtude dos resultados alcançados, os K-SJT demonstrou ser uma ferramenta com um grande potencial para o controle e otimização de Sistemas de Atendimento.', 
  resumo_en: '--'},

 {numero: 395, ano: 1996, dia: '09/04', autor: 'José Henrique Vilas Boas', orientador: [242], linha: 104, arquivo: '', 
  titulo: 'Mapeamento Ambiental com Suporte de Processamento Digital de Imagens e Banco de Dados Georreferenciados', 
  resumo_pt: 'Uso de bancos de dados georreferenciados às cartas resultantes de classificação digital. Estuda a incorporação dos resultados do processamento digital de imagens da superfície terrestre aos trabalhos de recursos naturais e meio-ambiente, em substituição aos procedimentos convencionais que utilizam os métodos visuais de interpretação. A adaptação do meio técnico aos recursos computacionais se dá em velocidade bem menor do que se verifica a evolução destes. Dessa forma, muito dos trabalhos nesta área utilizam apenas a digitalização e respectiva apresentação final das cartas temáticas encarando o computador como instrumento de trabalho em substituição aos meios convencionais de impressão de cartas. A utilização dos bancos de dados alfanuméricos como complemento aos mapas contidos em meio digital ou melhor, como esteio de um grande sistema de dados ambientais é ainda negligenciada pelas equipes que atuam nesta área. A concepção inerente aos sistemas de informações geográficas, que provêem o uso integrado de todas as ferramentas de geoprocessamento, deve ser experimentada e estimulada para que sejam avaliados os seus recursos e benefícios. Visando ampliar o horizonte útil deste estudo em direção aos técnicos potencialmente usuários, é utilizada uma linguagem simples tanto no embasamento teórico, que percorre os principais conceitos de interesse para o estudo, quanto na descrição da prática de um projeto experimental que tem por objetivo agregar os conhecimentos anteriormente expostos em base teórica. A área-teste escolhida corresponde ao setor sudeste da bacia hidrográfica de entorno à baía de Sepetiba, por conciliar disponibilidade de imagem a proximidade da área para checagem de campo. A base operacional foi desenvolvida no laboratório de Geomorfologia Fluvial Costeira e Submarina do Instituto de Geociências da UFRJ onde se utilizou o aplicativo desenvolvido pelo INPE, SITIM, versão 2.5, para o processamento da imagem LANDSAT-TM. O método de classificação supervisionado foi escolhido por se aproximar mais do estilo do técnico-intérprete, habituado ao método visual. Seus resultados foram transferidos para o SGI, versão 2.5, também do INPE, tendo sido acrescentados, via digitalização, os elementos de desenho cartográfico para composição da carta. Para melhor integração com o sistema de informações geográficas, foi utilizado o dBASE IV para o gerenciamento do banco de dados alfanuméricos, cuja estrutura foi baseada no Banco de Dados de Recursos Naturais e Meio-Ambiente da Divisão de Geociências do IBGE, em Salvador. Como produto final, o projeto experimental apresenta uma carta-imagem temática digital que fornece uma série de informações ambientais de forma gráfico-interativa, além daquelas que são inerentes ao próprio tema selecionado, a Vegetação. Além da modelagem conceitual dos dados ambientais e da preocupação com a sistematização da entrada de dados gráficos nos planos de informação, o presente estudo procura apresentar, de forma clara, como se procede exatamente o georreferenciamento dos dados. As tabelas de banco de dados, em anexo, documentam a base de informações.', 
  resumo_en: '--'},

 {numero: 394, ano: 1996, dia: '16/02', autor: 'Henrique Wilhelm da Silva Flink', orientador: [243], linha: 103, arquivo: '', 
  titulo: 'Sistema de Simulação em Ambiente Computacional Gráfico e Interativo', 
  resumo_pt: 'Este trabalho apresenta uma nova versão do SIMUL, programa computacional para Simulação a Eventos Discretos, que vem sendo utilizado por pesquisadores e alunos no IME/RJ, na COPPEAD/UFRJ e COPPE/UFRJ. Inicia-se pela explanação dos conceitos básicos de Programação Orientada a Objetos (abstração, encapsulamento, polimorfismo, herança), paradigma de programação no qual se baseou o desenvolvimento do sistema e de sua interface gráfica e interativa, fortemente baseada na programação para Microsoft Windows. São apresentados o novo projeto e arquitetura de sistema do SIMUL, principalmente pela opção do ambiente do Microsoft Windows e as várias melhorias na estrutura de dados interna que foram implementadas. Adicionalmente, o sistema permite integrar vários recursos disponíveis existentes nas versões anteriores. Enfatiza-se as novas características desta ferramenta, em especial a interface amigável e o ambiente completo de programação oferecido ao usuário. Vários exemplos são fornecidos, permitindo testar e obter resultados.', 
  resumo_en: '--'},

 {numero: 393, ano: 1996, dia: '12/02', autor: 'João Domingos Talon', orientador: [243], linha: 103, arquivo: '', 
  titulo: 'Simulador para o Estudo de Problemas de Tráfego em Sistemas de Telecomunicações', 
  resumo_pt: 'A aplicação da simulação, como técnica de PESQUISA OPERACIONAL, tem aumentado substancialmente nos últimos anos, em conseqüência da crescente complexidade dos problemas estudados da vida real, da popularização do microcomputador, das facilidades proporcionadas pela simulação e de sua maior divulgação. Esta tese apresenta um sistema computacional específico para o estudo de tráfego na área de telecomunicações. O desenvolvimento do sistema foi feito em linguagem Turbo Pascal, versão 7.0, tendo como base as rotinas pré-definidas do SIMUL. A este módulo foram acrescentadas novas rotinas desenvolvidas especificamente para aplicação na área de telecomunicações. Não se pretendeu desenvolver um sistema que fosse capaz de simular todos os tipos de problemas de tráfego da área de telecomunicações, mas sim os principais casos. Este trabalho apresenta, ainda, a descrição do Diagrama do Ciclo de Atividades padrão para uma chamada telefônica e exemplos de aplicação do sistema. Um dos exemplos é a aplicação a uma rede real de telecomunicações.', 
  resumo_en: '--'},

 {numero: 392, ano: 1996, dia: '05/02', autor: 'Ulf Bergmann', orientador: [244], linha: 101, arquivo: '', 
  titulo: 'Construção de um Domínio de Desenvolvimento de Software orientado a Objetos segundo o Paradigma Draco', 
  resumo_pt: 'A tese apresenta uma proposta de integração de um sistema transformacional, implementado pela máquina Draco-PUC, com a tecnologia de orientação a objetos, de maneira a obter um ambiente de desenvolvimento de software que favoreça a reutilização em altos níveis de abstração.', 
  resumo_en: '--'},

 {numero: 391, ano: 1996, dia: '26/01', autor: 'Ana Maria Laurentiz Pacífico', orientador: [245], linha: 101, arquivo: '', 
  titulo: 'Extensões de Banco de Dados Relacional aplicadas a Sistemas de Informação Geográfica', 
  resumo_pt: 'Esta tese aborda os aspectos de gerência de dados em Sistemas de Informação Geográfica (SIGs). Ela defende que nem os SIGs Convencionais e nem os BDs Convencionais têm uma solução adequada à gerência de grande volume de dados geográficos e ambientais, e aponta como solução do problema o uso de novas técnicas de Banco de Dados (Banco de Dados Não Convencionais) em SIGs. O trabalho propõe especificamente criar extensões em sistemas de banco de dados relacionais voltadas para SIG, e apresenta um estudo dessas extensões incorporadas no PostGres que permitem a criação de novos tipos de dados orientados a SIG. Esses novos tipos formam uma camada de tipos de dados geográficos que são utilizados nas definições e manipulações dos BDs dos SIGs. São também apresentados alguns exemplos de aplicação com as técnicas estudadas.', 
  resumo_en: '--'},

 {numero: 390, ano: 1996, dia: '22/01', autor: 'Arnalberto Jacques Nunes Seixas', orientador: [220], linha: 101, arquivo: '', 
  titulo: 'Sistema Híbrido Inteligente de Apoio à Tomada de Decisões (SHIATD)', 
  resumo_pt: 'Um protótipo de Sistema Híbrido de Apoio à Tomada de Decisões envolvendo os paradigmas de Redes Neurais e Sistemas Especialistas é proposto neste trabalho. O Sistema proposto tem por objetivo ser de aplicação livre em áreas as mais diversas, como a predição de séries temporais no mercado de ações; a análise de risco para empréstimo bancário; a análise de risco de cobertura de bens móveis ou imóveis; a seleção de recursos humanos e outras onde se possa contar com amostras de dados históricos de interesse e com um especialista do qual se possa extrair o conhecimento prático necessário ao módulo especialista do sistema. É feito um estudo de caso, que serve para a validação do protótipo, onde os dados de seleção de conscritos para o Serviço Militar Inicial são somados às alterações de ex-soldados do Exército Brasileiro com o objetivo de permitir ao sistema apresentar sugestões quanto a incorporar ou não determinado conscrito, cujos dados de seleção ele disponha. Os resultados alcançados e as conclusões obtidas, a partir dos estudos e treinamentos feitos, são reportados e ainda são feitas sugestões para trabalhos futuros , quer a nível de Sistema de Seleção de novos soldados, quer a nível de trabalhos de pesquisa dentro da linha proposta.', 
  resumo_en: '--'},

 {numero: 389, ano: 1996, dia: '19/01', autor: 'Claudia Constantina Saltarelli Saraiva', orientador: [246], linha: 104, arquivo: '', 
  titulo: 'Proposta de Automação de Mapas Municipais', 
  resumo_pt: 'Os mapas municipais são, segundo OLIVEIRA, 1987, a representação dos aspectos topográficos de um município como unidade administrativa. Os processos de construção podem variar de uma empresa para outra de acordo com a sua finalidade. Nesta dissertação, a técnica de construção estudada é a desenvolvida pelo Instituto de Geociências Aplicado da Fundação Centro Tecnológico de Minas Gerais (IGAlCETEC), ou seja, de construção analógica, da compilação das folhas topográficas do mapeamento sistemático do Brasil. Para esse estudo, é descrita esta técnica, levantando tempo, pessoal empregado e recursos necessários. São observadas, através de orçamentos, as fases que mais levam tempo, como também as que necessitam de técnicos treinados para executá-las além do consumo de material de custo elevado. A partir destas observações procurou-se automatizar o processo. Utilizando ambiente computacional, é desenvolvida uma metodologia para construção do mapa municipal. São descritos vários programas de CADs, SICs e SIGs existentes no mercado nacional e internacional, e dentre esses programas foi escolhido um deles para gerar um protótipo. Tabelas comparativas de tempo, materiais e equipamentos mostram as modificações ocorridas no processo. Um orçamento de mapa digital é elaborado para avaliar seu custo final. Um teste de impressão em plotter é realizado visando a sobreposição das pranchas desenhadas, por cores, através de registros.', 
  resumo_en: '--'},

 {numero: 388, ano: 1996, dia: '11/01', autor: 'Fábio Contarini Carneiro', orientador: [228], linha: 101, arquivo: '', 
  titulo: 'Projeto de um Compilador de Estelle para VHDL', 
  resumo_pt: 'Este trabalho apresenta o projeto de um compilador Estelle-VHDL segundo uma metodologia para a tradução de especificações de sistemas de comunicações descritos na linguagem Estelle para a linguagem VHDL. Esta metodologia define regras para a tradução de forma a especificar um compilador para a tradução automática de especificações, possibilitando a geração de máscaras de circuitos VLSI e a consequente implementação em hardware de tais sistemas. O enfoque principal da metodologia é a separação dos módulos definidos em Estelle em componentes que desempenhem a função destes módulos de forma concorrente, sincronizando suas ações através de trocas de primitivas. A separação do módulo em componentes concorrentes objetiva alcançar um desempenho maior não só pela execução do sistema em hardware, mas também pela execução paralela das ações deste módulo.', 
  resumo_en: '--'},

 {numero: 387, ano: 1996, dia: '10/01', autor: 'Luis Alejandro Ojeda Pérez', orientador: [247], linha: 104, arquivo: '', 
  titulo: 'Traçado de Curvas Isobatimétricas em Cartas Náuticas Mediante Processo Digital', 
  resumo_pt: 'Cada vez mais potentes, novos hardwares e softwares têm sido aproveitados com resultados satisfatórtos pela Engenharia Cartográfica na automação de processos de construção de cartas. A Cartografia Náutica também tem conseguido bons resultados. No entanto, ainda existem fases do processo de fabricação de uma carta náutica que são de realização exclusiva de um cartógrafo desenhista. Uma destas fases é conhecida como traçado das curvas isobatimétricas. Considerando que a Carta Náutica é projetada em função de representar o relevo submarino sob o ponto de vista de segurança à navegação, o presente trabalho tem por objetivo apresentar uma solução para o traçado de curvas isobatimétricas em meio digital mantendo as seguintes premissas: precisão suficiente para dar segurança durante a navegação de embarcações, suavidade no traçado, de maneira a facilitar sua compreensão e utilização por parte do usuário; e representar as variações do terreno com mais aproximação da realidade. A utilização de Modeladores Digitais de Terreno por instituições que trabalham com processos e geração de produtos cartográficos é um fato. Neste sentido, estudar-se-ão as características principais destas ferramentas assim como a conceituação que as envolve, com o fim de utilizar as mesmas no desenvolvimento de uma metodologia que atenda o objetivo proposto.', 
  resumo_en: '--'},

 {numero: 386, ano: 1996, dia: '10/01', autor: 'Marcelo Tuler de Oliveira', orientador: [247], linha: 104, arquivo: '', 
  titulo: 'Desenvolvimento de Modelo de Mapeamento para Determinação de Áreas Aptas em Função do Perfil do Solo e da Topografia, comparando a Classificação Fuzzy e Booleana', 
  resumo_pt: 'Do ponto de vista geográfico, os modelos de mapeamento são utilizados para retratar espacialmente fenômenos naturais. Alguns destes fenômenos comportam leis complexas em sua definição, e os Sistemas de Informações Geográficas tentam incrementar sua funcionalidade, provendo a combinação entre o domínio cartográfico e a análise espacial. Entretanto, a maioria destes Sistemas de Informações Geográficas armazenam os dados de meio ambiente em Banco de Dados Geográficos, e ao recuperá-los, utiliza-se de critérios classificatórios que determinam classes precisas para o atributo. Esta técnica, utilizando operações Booleanas para correlacionar os atributos, ignora a natureza contínua das variáveis e a incerteza na mensuração. Neste contexto, o objetivo deste trabalho é modelar um fenômeno natural (Excesso de Água), aplicando a técnica inferencial de krigagem e os métodos de classificação Booleano e Fuzzy sobre as variáveis diretamente envolvidas, e comparar os resultados obtidos. Elaborou-se um estudo de caso, utilizando dados reais de um campo experimental da PESAGRO-RIO e pertencentes ao CNPSolos-EMBRAPA. Observou-se que o método de classificação Fuzzy rejeitou menos informações que o método Booleano.', 
  resumo_en: '--'},

 {numero: 385, ano: 1996, dia: '05/01', autor: 'Moacyr Amaral Domingues Figueiredo', orientador: [248], linha: 103, arquivo: '', 
  titulo: 'Metodologia para o Desenvolvimento de Indicadores Estratégicos e Operacionais', 
  resumo_pt: 'Este trabalho surgiu da necessidade de se estabelecer e desenvolver novas formas de quantificar o desempenho organizacional, tendo como base a visão sistêmica e horizontal da organização. O trabalho apresenta uma metodologia para o desenvolvimento de indicadores estratégicos e operacionais relacionados com a estratégia, os processos-chave e os interesses dos clientes, acionistas, funcionários, sociedade e fornecedores da organização. A abordagem proposta usa como suporte a Metodologia QFD - Quality Function Deployment, aplicada no alinhamento da Gestão Estratégica e desenvolvimento do Produto, e o Método AHP - Analitical Hierarchy Process, aplicado na seleção de processos-chave.', 
  resumo_en: '--'},

 {numero: 384, ano: 1996, dia: '04/01', autor: 'Hildo Vieira Prado Filho', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Treinamento, Educação e Desenvolvimento em Qualidade', 
  resumo_pt: 'Uma Organização que busca implementar a filosofia da Qualidade Total em seus processos deve procurar identificar os desejos, necessidades e expectativas dos clientes, bem como buscar o desenvolvimento das pessoas que a compõem, visando com isso seu próprio crescimento. Faz-se necessário, portanto, um esforço em Treinamento, Educação e Desenvolvimento em Qualidade, direcionados a todos aqueles que integram essa Organização. Dentro desse enfoque, foi montada uma metodologia de Treinamento, Educação e Desenvolvimento em Qualidade que, baseada nas \'carências\' indicadas pela análise do diagnóstico levantado durante o planejamento estratégico da Organização, fundamentada na Visão Organizacional e na Política da Qualidade Total, e pela direção apontada pelos indicadores de desempenho, pudesse reduzir a diferença existente entre o nível de qualidade atual e o desejado. Para sua elaboração, foram utilizadas ferramentas e técnicas da Qualidade Total na elaboração dos currículos de cursos em Qualidade, para que fornecessem suporte técnico às decisões tomadas.', 
  resumo_en: '--'},

 {numero: 383, ano: 1995, dia: '28/12', autor: 'Cristiani Perrini Bodart', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'Sistema Híbrido: Rede Neural Integrada com Sistema Especialista para Diagnóstico de Falhas', 
  resumo_pt: 'Falha é uma perturbação ocorrida nas variáveis do processo e o efeito por esta gerado é propagado através do sistema. O diagnóstico de falha de um processo é de vital responsabilidade para o operador de uma planta industrial, em modernos processos químicos e nucleares. Quanto mais rápido um diagnóstico é fornecido, mais rapidamente uma ação de controle poderá ser tomada, evitando-se, assim, a ocorrência de problemas graves, tais como: a perda de toda uma produção ou, de acordo com o caso, a necessidade de se desligar equipamentos de muita importância na planta industrial. O processo industrial analisado foi o de separação óleo-gás numa planta de extração de petróleo. Para identificação e diagnóstico das falhas propostas para o processo, utilizou-se três metodologias, a saber: redes neurais, sistema especialista e sistema híbrido, os quais foram comparados à metodologia Diagnostic Model Processor - DMP. A rede neural hierárquica permite uma paralelização dos dados, por isso, um tempo menor é gasto pela mesma, quando se compara este com os das demais tecnologias estudadas, para a identificação de uma mesma falha. Para se trabalhar num regime não-contínuo de extração óleo-gás podem ser feitas adaptações nas metodologias. A rede neural é mais fácil de se adaptar, mas requer novo treinamento da rede. O SE precisaria reestruturar suas bases de conhecimento. A tecnologia DMP necessitaria de um aumento no número de equações residuais baseadas em modelos.', 
  resumo_en: '--'},

 {numero: 382, ano: 1995, dia: '22/12', autor: 'Tomás de Aquino Tinoco Botelho', orientador: [239], linha: 101, arquivo: '', 
  titulo: 'Análise de Desempenho da Arquitetura Cliente-Servidor Orientada a Objeto', 
  resumo_pt: 'A computação de objetos distribuídos em Arquitetura Cliente-Servidor é uma tendência atual, evidenciada nos recentes padrões internacionais consubstanciados nos modelos da ISO (RM-ODP) e da OMG (CORBA). A comunicação entre objetos remotos, por meio da troca de mensagens através da rede, aumenta o tráfego e afeta o desempenho da aplicação. Pesquisando os reflexos do desempenho na modelagem de uma aplicação orientada a objeto e executada sobre uma plataforma Cliente-Servidor, implementou-se uma biblioteca de classes genéricas para a comunicação remota entre objetos e estabeleceu-se um conjunto de equações que permitem projeções estatísticas sobre o comportamento dessa aplicação. As medidas permitiram concluir que o tráfego de mensagens através de uma rede de workstations SUN e a gerência dos objetos em memória afetam o desempenho da aplicação.', 
  resumo_en: '--'},

 {numero: 381, ano: 1995, dia: '21/12', autor: 'Sebastião Pereira de Miranda Filho', orientador: [245], linha: 101, arquivo: '', 
  titulo: 'O Desenvolvimento de Interfaces de Usuário Orientadas a Objeto para a Engenharia Química', 
  resumo_pt: 'Este trabalho apresenta um estudo sobre o desenvolvimento de Interfaces de Usuário Orientadas a Objetos para o domínio especializado da Engenharia de Processos Químicos. A orientação a objetos é examinada desde sua base cogntiva fundamental até sua manifestação em linguagens de programação, bancos de dados e interfaces de usuário. Ferramentas, técnicas e tecnologias são procuradas e a importância delas com respeito à construção de interfaces é examinada. Isso, conduz ao disseminado modelo MVC do Smalltalk, como a arquitetura apropriada para a interface aos padrões emergentes ODBC, VBX, OLE e OCX, como a tecnologia apropriada para a infraestrutura técnica relacionada à interface, incluindo serviços de arquivos, de banco de dados, de comunicação e de apresentação de dados. Algumas implementações do MVC em classes reutilizáveis são discutidas. O papel dos controles VBX e OCX, do ODBC e dos serviços OLE é examinado. Um protótipo com características de modernas interfaces de usuário é construído, ilustrando a aplicação dos princípios fundamentais desta dissertação.', 
  resumo_en: '--'},

 {numero: 380, ano: 1995, dia: '19/12', autor: 'Marta Ribeiro Barata', orientador: [249], linha: 101, arquivo: '', 
  titulo: 'Modelo Cooperativo para uma Estrutura Gráfica Multidimensional', 
  resumo_pt: 'O trabalho ora proposto apresenta o modelo CGMS (Cooperative Graphical Multidimensional Structure) para projeto de interface gráfica cooperativa, no intuito de formar bases hipermídia intercambiáveis. O enfoque baseia-se na crença de que o usuário de Sistemas Hipermídia seja capaz de integrar múltiplas bases hipermídia (através dos elementos de visualização definidos na modelagem), sob diferentes pontos de vista.', 
  resumo_en: '--'},

 {numero: 379, ano: 1995, dia: '18/12', autor: 'Alfredo Martins Muradas', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'Inteface em Linguagem Natural para Consulta a Bancos de Dados Relacionais', 
  resumo_pt: 'Este trabalho enfoca a pesquisa e desenvolvimento de uma Interface em Linguagem Naturai (ILN), cujo interpretador para o português tem como característica principal, um Sistema de Regras de Produção que reconhece fragmentos do português utilizado para consultar bancos de dados relacionais em linguagem natural, através da adaptação do paradigma das Gramáticas de Determinação para Regras de Produção. É implementada uma interface portátil para outros domínios, bem como possível de ser utilizada por sistemas que façam uso de programas de aplicação para SGBDs virtuais, pois uma de suas principais características é a independência entre o conhecimento linguístico e a estrutura do Banco de Dados.', 
  resumo_en: '--'},

 {numero: 378, ano: 1995, dia: '01/12', autor: 'César Bezerra Teixeira', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'GSEOO - Gerador de Sistemas Especialistas com Bases de Conhecimento que Suportam Herança', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 377, ano: 1995, dia: '01/11', autor: 'Fidel Leopoldo Castro de la Cruz', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'Técnica de Raciocínio Baseado em Casos para Sistemas Especialistas', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 376, ano: 1995, dia: '01/11', autor: 'Carlos Teiron Procel Silva', orientador: [230], linha: 101, arquivo: '', 
  titulo: 'Apresentação de Explicações em Ambiente para Processamento do Conhecimento Baseados em Resolução', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 375, ano: 1995, dia: '24/08', autor: 'Jesus Fernando Mansilla Baca', orientador: [214], linha: 104, arquivo: '', 
  titulo: 'Modelagem de um Banco de Dados Geodesicos', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 374, ano: 1995, dia: '04/07', autor: 'Romildo Gonçalves Valente', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'Predição de Séries Temporais Utilizando Redes Neurais', 
  resumo_pt: 'O presente trabalho propõe um novo algoritmo conexionista chamado RNV + P (Rede Neural dos Vizinhos mais Próximos) e demonstra seu uso na predição de séries temporais. O algoritmo constrói neurônios estimadores arranjados em camadas, empregando suavizamento kernel conjugado à reconstrução do espaço-estado de forma a realizar aproximação funcional. Neste trabalho ele é comparado a algoritmos tradicionais, como o método de retropropagação de erros, na predição da conhecida série das manchas solares e de uma série de um sinal laser, de comportamento quase caótico. Os resultados alcançados mostram que o algoritmo RNV + P pode ser empregado com sucesso como um estimado.', 
  resumo_en: '--'},

 {numero: 373, ano: 1995, dia: '29/06', autor: 'Abdias Fernandez Ramos', orientador: [250], linha: 102, arquivo: '', 
  titulo: 'Procedimento para Tomada de Decisão em Terminais Marítimos Petroleiros', 
  resumo_pt: 'Este trabalho tem como objetivo desenvolver um procedimento para decidir que alternativas ou ações mitigatórias, devem ser tomadas em Terminais Marítimos Petroleiros para melhorar o seu desempenho operacional, através da utilização de uma técnica da Tomada de Decisão por Objetivos Múltiplos, denominada Programação por Metas ou Goal Programming. Tomar decisões constitui uma tarefa básica da gestão. Decidir é escolher ou optar entre alternativas viáveis. A maioria das situações reais que exigem uma tomada de decisão são caracterizadas pela existência de vários objetivos ou desejos a serem atingidos. Esses objetivos são frequentemente conflitantes. A tomada de decisão em Terminais Marítimos Petroleiros passa pela definição de uma ferramenta que engloba as diversas variáveis envolvidas e que permite estabelecer sob o ponto de vista sistêmico as diversas metas, dentre as quais pode-se citar: minimização dos custos, diminuição dos impactos ambientais, minimização do tempo de estadia no Terminal, e melhoria dos métodos e programação. Como aplicação é formulada uma situação real para a qual são apresentadas as soluções de acordo com o procedimento descrito ao longo do trabalho. Como conclusão o procedimento auxilia ao tomador de decisão rever criticamente as prioridades da estrutura, e as metas desejadas, permitindo-lhe ainda prever o comportamento do sistema, realizar o tradeoff e os efeitos produzidos devido a modificações no sistema ou nos métodos de operação.', 
  resumo_en: '--'},

 {numero: 372, ano: 1995, dia: '21/06', autor: 'Eduardo Franco da Costa Fernandes', orientador: [213], linha: 101, arquivo: '', 
  titulo: 'Redes Neurais Aplicadas a Teoria do Combate', 
  resumo_pt: 'A presente tese apresenta de forma detalhada o algoritmo clássico da retropropagação do erro para redes neurais e as leis de Lanchester para a previsão de baixas em combates terrestres. Apresenta-se também a base de dados QJM, organizada pelo historiador-militar Trevor N. DuPuy, contendo dados reais de batalhas e previsões que, segundo a literatura disponível, são convergentes com as de Lanchester. Porém, análises iniciais mostram divergências entre as previsões QJM/Lanchester e as baixas observadas em combate. Para tentar-se achar um relacionamento funcional entre os valores teóricos e os observados, foram propostos e treinados modelos de redes neurais, baseados na retropropagação do erro. Na parte final do trabalho, mostra-se os resultados obtidos confrontando-se os dados reais de baixas, registrados na base QJM, com as previsões de QJM/Lanchester e as fornecidas pelas redes neurais.', 
  resumo_en: '--'},

 {numero: 371, ano: 1995, dia: '27/04', autor: 'Davidson Brown Ferreira Santos', orientador: [249], linha: 101, arquivo: '', 
  titulo: 'Uma Aplicação de Animação em Sistemas Hipermídia', 
  resumo_pt: 'Muitas vezes, a leitura (browsing) de bases hipermídia provoca uma sensação de desorientação para o leitor destas bases. A excessiva quantidade de informações disponíveis - muitas vezes acessadas de diversas formas e por vários caminhos - só será bem assimilada se estiver bem organizada e acessível a uma grande variedade de leitores. O enfoque usual se baseia na crença de que o leitor é capaz de memorizar uma grande quantidade de caminhos (hiperlinks) e associações a partir dos nós visitados. O enfoque ora proposto procura se utilizar da Animação como ferramenta de auxíl io visual na navegação de bases hipermídia (facilitando a busca das informações), através de um modelo para projeto de interface gráfica que oferece a possibilidade de autoria de aplicações - GMS. É apresntado o modelo ADMS (Allil1lated Graphical Multidil1lensional Structure), capaz de gerar informação de orientação a partir da manipulação interativa de fiwlIes de animação mediante recursos de visualização tridimensional.', 
  resumo_en: '--'},

 {numero: 370, ano: 1995, dia: '27/03', autor: 'Antonio Ricardo Nunes Guimarães', orientador: [206], linha: 101, arquivo: '', 
  titulo: 'Interface Orientada a Objetos para Criação e Manipulação de Esquemas Conceituais no SIGO', 
  resumo_pt: 'Este trabalho teve por objetivo construir uma interface para o Sistema Gerenciador de Objetos (SIGO), visando a tarefa de modelagem de esquemas. Para isso, partiu-se do estudo dos requisitos oriundos das aplicações não-convencionais de SGBDs, destacando os resultantes da dinamicidade da realidade - alvo destas aplicações. É abordada a solução proposta pela orientação a objetos e a perspectiva dos seus conceitos no SIGO. De maneira a garantir a correção do esquema obtido através do modelo previsto no sistema, são especificadas propriedades invariantes que estabelecem os aspectos de um esquema correto e regras de evolução que asseguram a manutenção da sua integridade diante das alterações. São obtidas diretivas para o projeto de interfaces que, em linhas gerais, indicam uma filosofia para a obtenção de aspectos desejáveis no processo interativo. Finalmente, é apresentada a Interface Orientada a Objetos para Criação e Manipulação de Esquemas Conceituais no SIGO, cuja finalidade precípua é oferecer meios de modelagem de esquemas, mantendo sua integridade através do processo interativo indicado pelas diretivas anteriormente obtidas.', 
  resumo_en: '--'},

 {numero: 369, ano: 1995, dia: '23/02', autor: 'David Enrique Jaramillo Veloz', orientador: [234], linha: 102, arquivo: '', 
  titulo: 'Planejamento de Sistemas: uma Visão Metodológica', 
  resumo_pt: 'O objetivo deste trabalho é desenvolver um procedimento para a seleção e hierarquização dos diferentes componentes de um Sistema Aeroviário Estadual. Inicialmente, faz-se uma análise das metodologias de elaboração de Planos Aeroviários Estaduais utilizados no Brasil, nos Estados Unidos e no Canadá, e uma apresentação de modelos de Programação Linear voltados para a seleção de sistemas aeroviários. A seguir, propõe-se o modelo denominado de Problema de Localização e Máxima Cobertura para os Planos Aeroviários Estaduais (PLMC-PAE), de visão multiobjetivo, que permite gerar alternativas de sistemas aeroviários minimizando os custos de investimento e maximizando os números de passageiros e de localidades atendidas . Finalmente, apresenta-se um Estudo de Caso com os dados do Plano Aeroviário Estadual de Santa Catarina (PAESC), no qual é feita uma avaliação comparativa entre a solução obtida pelo modelo proposto e a implementada pelo Instituto de Aviação Civil (IAC).', 
  resumo_en: '--'},

 {numero: 368, ano: 1995, dia: '01/02', autor: 'José Carlos da Silva', orientador: [214], linha: 101, arquivo: '', 
  titulo: 'Metodologia para Ouvir, Internalizar e Medir a Satisfação de Clientes: uma Aplicação para Currículo de Engenharia', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 367, ano: 1995, dia: '09/01', autor: 'Pedro Soares da Silva Neto', orientador: [251], linha: 103, arquivo: '', 
  titulo: 'Problema de Alocação de Canais e T-Coloração', 
  resumo_pt: 'O Problema de Alocação de Canais é um problema tradicional que visa a alocação de canais de rádio a transmissores, de forma tal que possíveis interferências entre os mesmos sejam minimizadas e o espectro de frequências não seja desperdiçado. O Problema de T-coloração consiste em determinar uma coloração do grafo G = (V, E, T), onde V é o conjunto de vértices, E é o conjunto de arestas (u, v) em que u, v E V e o módulo da diferença entre as cores alocadas a u, v não pertence a T. O conjunto T é um conjunto de restrições que representa as diferenças entre cores não permitidas para vértices adjacentes e assumimos que O sempre pertence a T. Para T = {O} o problema de T-coloração é equivalente ao problema de Coloração tradicional. Assim sendo, o problema de T-coloração é mais abrangente que o de Coloração e tem complexidade, no mínimo, igual à complexidade do Problema de Coloração. Em consequência, o problema de Tcoloração é NP-Completo. Apresentamos um modelo para o Problema de Alocação de Canal com vistas à aplicação da Tcoloração em T-Grafos. O produto final do modelo é um T-grafo que representa o problema original e suas restrições, o qual após receber uma T-coloração apropriada, resolve o problema ori gi nal. Falta-nos apenas os algoritmos para T-coloração de T-Grafos. Promovemos, também , uma investigação dos principais algoritmos de coloração de grafos com vistas à obtenção de técnicas e/ou estratégias para a obtenção de algoritmos para T-coloração que são quase inexistentes. O algoritmo Colorclique é um algoritmo proposto no presente trabalho, o qual apresenta uma performance comparável ao algoritmo Cosine e possui um desempenho computacional superior ao mesmo. Apresentamos o algoritmo TGuloso que já está bastante estudado mas que apresenta baixo desempenho para aplicações práticas. Propomos o algoritmo guloso T-LF que se baseiacno algoritmo guloso LF, com razão de performance média supe ri or ao a lgoritmo T-Guloso. Sugerimos o algoritmo T-Cosine e T-Colorclique, baseados nos algoritmos Cosine e Colorclique. De posse dos algoritmos propostos, podemos resolver o Problema de Coloração obtido e, por conseqüência, o Problema de Alocação de Canal correspondente.', 
  resumo_en: '--'},

 {numero: 366, ano: 1995, dia: '11/01', autor: 'Mauro Guedes Ferreira Mosqueira Gomes', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Gestão pela Qualidade Total dos Processos de Avaliação Operacional de Materiais de Emprego Militar', 
  resumo_pt: 'Este trabalho teve como objetivo propor uma nova metodologia de Avaliação Operacional de Materiais de Emprego Militar, baseada nos princípios da Qualidade Total, bem como definir bases para a implantação da Gestão pela Qualidade Total no Centro de Avaliações do Exército (CAEx), unidade do Exército Brasileiro utilizada como laboratório para pesquisa de campo. Realizou-se um estudo teórico da literatura existente, e algumas avaliações em andamento no ano de 1994 serviram como base para a introdução de algumas mudanças propostas . O conteúdo foi distribuído em 5 capítulos. No primeiro, Introdução, busca-se posicionar o leitor com relação aos objetivos da Tese e seu posicionamento no contexto do Exército. No capítulo 2 são apresentados os oito princípios da Qualidade considerados como bases para Implantação da Gestão pela Qualidade Total no CAEx. No capítulo 3 propõe-se uma Metodologia de Avaliação calcada nos princípios da Qualidade, dividida em 5 grandes fases - Definições e Pré-requisitos para Avaliação - Planejamento Voltado para a Qualidade - Planejamento Operacional - Execução - Análise e Apresentação dos Resultados. No capítulo 4 faz-se uma análise dos resultados de estudos específicos sobre a ferramenta QFD - Desdobramento da Função Qualidade e sobre a aplicação de medidas de desempenho ao trabalho de Avaliação Operacional. No capítulo 5 relata-se as experiências adquiridas, dificuldades no trabalho de implementação, recomendações visando a continuidade da pesquisa e as conclusões.', 
  resumo_en: '--'},

 {numero: 365, ano: 1995, dia: '11/01', autor: 'Heraldo Makrakis', orientador: [248], linha: 103, arquivo: '', 
  titulo: 'Princípios e Metodologias da Qualidade em Educação', 
  resumo_pt: 'Esta Tese tem o objetivo de apresentar princípios e metodologias da Qualidade em Educação, para serem utilizados nos estabelecimentos de ensino do Exército Brasileiro, tendo por base a filosofia da Qualidade Total e de uma Pedagogia de corte construtivista, resgatando a idéia da Educação através do PAIDÉIA. As metodologias apresentadas são: para o planejamento do ensino é proposto o QFD Quality Funetion Deployment; para a avaliação de ensino é proposta a medida de índice de desempenho; e para a gestão de ensino, a Gestão Estratégica. Os princípios se baseiam nos Princípios da Administração pela Qualidade Total para o Exército Brasileiro.', 
  resumo_en: '--'},

 {numero: 364, ano: 1995, dia: '04/01', autor: 'Stefani de Abreu Souza', orientador: [252], linha: 104, arquivo: '', 
  titulo: 'A Territorialidade do Potencial Turístico do Estado do Rio de Janeiro', 
  resumo_pt: 'A dificuldade de atualização e cruzamento de informações pelos diversos setores da sociedade vem acelerando a necessidade de uma tecnologia integradora. O Geoprocessamento surge com suas inúmeras aplicações, permitindo o avanço dos projetos interdisci plinares, Esta tese propõe uma ampliação no campo de atuação das tecnologias de Geoprocessamento, aplicando-as ao Planejamento Turístico, Este setor apresenta-se como um dos que mais cresce em todo o mundo, movimentando trilhões de dól ares e gerando milhões de empregos diretos e indiretos, justificando, portanto, investimentos prementes de forma a adequar os potenciais turísticos do Estado do Rio de Janeiro às necessidades do turista.', 
  resumo_en: '--'},

 {numero: 363, ano: 1995, dia: '04/01', autor: 'Wallace Oliveira de Figueiredo', orientador: [252], linha: 104, arquivo: '', 
  titulo: 'Catástrofes Ambientais e Defesa Civil no Município do Rio de Janeiro: uma Aplicação por Geoprocessamento', 
  resumo_pt: 'Esta tese propõe uma metodologia de investigação dos fenômenos ambientais presentes no município do Rio de Janeiro: desmoronamentos/deslizamentos, enchentes e incêndios florestais; decorrentes, principalmente, de condicionantes climáticos, buscando equacionar os fatores que contribuem para as suas ocorrências, partindo dos micro-sistemas em suas ambiências especificadas para a modelagem da problemática contextual, com a participação de equipes interdiscipli nares, compostas por profissionais e pesquisadores dos mais diversos órgãos e ciências envolvidos, preconizando a utilização de um Sistema de Informação Geográfica capaz de atender aos diversos questionamentos da Defesa Civil, otimizando o fluxo de informação entre os órgãos a ela agregados, possibilitando monitorar as causas destes fenômenos, de modo a evitá-los e minimizar suas consequências através de simulações e medidas preventivas.', 
  resumo_en: '--'},

 {numero: 362, ano: 1995, dia: '01/01', autor: 'Ubiratan de Salles', orientador: [214], linha: 102, arquivo: '', 
  titulo: 'Sistema de Informação para Mobilização dos Transportes - Modo Ferroviário', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'},

 {numero: 361, ano: 1995, dia: '01/01', autor: 'José Martins das Chagas', orientador: [222], linha: 103, arquivo: '', 
  titulo: 'Metodologia de Análise de Confiabilidade: uma aplicação prática ao foguete Chaff', 
  resumo_pt: 'RESUMO NÃO DISPONÍVEL', 
  resumo_en: 'ABSTRACT STILL NOT AVAILABLE'}
];
